{
  "hash": "8f9691fea53f39429261cf677619e79f",
  "result": {
    "markdown": "---\ntitle: \"Exercice 3\"\nauthor: \"Clément Poupelin\"\ndate: \"2025-02-17\"\nformat: \n  html:\n    embed-resources: false\n    toc: true\n    code-fold: false\n    code-summary: \"Show the code\"\n    code-tools: true\n    toc-location: right\n    page-layout: article\n    code-overflow: wrap\ntoc: true\nnumber-sections: false\neditor: visual\ncategories: [\"TP\"]\nimage: \"/img/validation.png\"\ndescription: \"Première essais de techniques de validations croisées sur des données générées manuellement\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n```\n:::\n\n\n\n# Question 1 --------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx = rnorm(1000)\ny = x - 2*(x^2) + rnorm(1000)\n# modele avec n = 1000 et p = 2. beta = t(1, -2 )\n```\n:::\n\n\nNous allons généré un modèle de régression linéaire classique : \n$$Y = X\\beta + \\mathcal{E}$$\n- $Y \\in \\mathbb{R}^{n}$ la variable réponse ou variable à expliquer\n\n- $X \\in \\mathbb{R}^{n\\times p}$ la matrice contenant nos variables explicatives \n\n- $\\beta \\in \\mathbb{R}^{n}$ le vecteur composée des coefficients de régression\n\n- $\\mathcal{E} \\in \\mathbb{R}^{n}$ le vecteur d'erreur suivant une loi $\\mathcal{N}(0, 1)$\n\nPour la génération de nos données, nous allons alors poser que $\\beta = (1, 2)'$ et $X = [x_1, x_1^2]$, $x_1 \\in \\mathbb{R}^n$ suivant une loi $\\mathcal{N}(0,1)$.\n\n\n\n# Question 2 --------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x,y)\n```\n\n::: {.cell-output-display}\n![](Exercice_3_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(x,y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2908668\n```\n:::\n\n```{.r .cell-code}\n# correlation faible, pourtant le nuage de point montre bien un effet de x sur y \n```\n:::\n\n\n\n\n\n\n# Question 3 --------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = as.data.frame(cbind(y, x, x^2, x^3, x^4))\nmod1 = lm(y~x, data = df)\nmod2 = lm(y~ x + V3, data=df)\nmod3 = lm(y~ x + V3 + V4, data=df)\nmod4 = lm(y~ x + V3 + V4 + V5, data=df)\n\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.8594  -0.9890   0.7507   1.8526   5.0428 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.92130    0.09229 -20.818   <2e-16 ***\nx            0.89410    0.09310   9.604   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.918 on 998 degrees of freedom\nMultiple R-squared:  0.0846,\tAdjusted R-squared:  0.08369 \nF-statistic: 92.24 on 1 and 998 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\ncor(x,y)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08460352\n```\n:::\n\n```{.r .cell-code}\n#le r2 du mod1 doit correspondre à la corr^2 : ok\n```\n:::\n\n\n\n# Question 4 --------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.08460352\n```\n:::\n\n```{.r .cell-code}\nBIC(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4998.42\n```\n:::\n\n```{.r .cell-code}\nAIC(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4983.697\n```\n:::\n\n```{.r .cell-code}\nBIC(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2876.101\n```\n:::\n\n```{.r .cell-code}\nAIC(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2856.47\n```\n:::\n\n```{.r .cell-code}\nBIC(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2881.088\n```\n:::\n\n```{.r .cell-code}\nAIC(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2856.549\n```\n:::\n\n```{.r .cell-code}\nBIC(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2885.15\n```\n:::\n\n```{.r .cell-code}\nAIC(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2855.704\n```\n:::\n\n```{.r .cell-code}\n# mod2 est le meilleur modèle \n\n## FAIRE UN TABLEAU POUR LES RESULTATS\n# R2, R2aj, Cp, AIC, BIC, coeff, etc...\n```\n:::\n\n\n\n# Question 5 --------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfmla2 = y~ x + V3\nfmla3 = y~ x + V3 + V4\nfmla1 = y~ x\nfmla4 = y~ x + V3 + V4 + V5\n####\n\n# ##### TRASH ####\n# \n# FMLA = c(fmla1, fmla2, fmla3, fmla4)\n# \n# LOO_1 = function(y, x, fmla, j){\n#   y_res = rep(NA, 1000)\n#   for (i in 1:1000){\n#     \n#     df[i] = as.data.frame(cbind(y[-i], x[-i], x[-i]^2, x[-i]^3, x[-i]^4))\n#     mod = lm(fmla[[j]], data = df[i])\n#     \n#     \n#     df2[i] = as.data.frame(cbind(x[-i], x[-i]^2, x[-i]^3, x[-i]^4))\n#     yi_hat = predict(mod, df2[i])\n#     yi = y[i]\n#     y_res[i] = yi - yi_hat \n#   }\n#   loo = mean(y_res^2)\n#     return(loo)\n# }\n# LOO_1(y,x,FMLA, 2)\n# \n# \n# \n# LOO_2 =function(y, x, fmla, i){\n#   r = lm(fmla[[i]], data = df)$residuals\n#   h = hatvalues(lm(fmla[[i]], data = df)) #diag de hatmatrix\n#   loo = mean((r/(1-h))^2)\n#   return(loo)\n# }\n# LOO_2(y,x,FMLA, 1)\n# LOO_2(y,x,FMLA, 2)\n# LOO_2(y,x,FMLA, 3)\n# LOO_2(y,x,FMLA, 4)\n# \n# #####\n\n# Fonction loo qui utilise en entree le modele (ne fonctionne pas si la formule utilise la fonction poly)\nloo=function(mod){\n  n = nrow(mod$model)\n  Call = mod$call # mod1$call --> lm(formula = y ~ x, data = df)\n  erreur=1:n\n  for(i in 1:n){\n    Call$data = mod$model[-i, ] # mod1$call$data = df\n    fit = eval.parent(Call)\n    pred = predict(fit, mod$model[i,])\n    erreur[i] = (pred - mod$model[i,1])^2\n  }\n  return(mean(erreur))\n}\n\n\n\n#Fonction loo qui utilise la formule du cours\nloo2=function(mod){\n  mean((residuals(mod)/(1-hatvalues(mod)))^2)\n}\n```\n:::\n\n\n\n\n\n# Question 6 --------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.581687\n```\n:::\n\n```{.r .cell-code}\nloo(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.016685\n```\n:::\n\n```{.r .cell-code}\nloo(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.017154\n```\n:::\n\n```{.r .cell-code}\nloo(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.01563\n```\n:::\n\n```{.r .cell-code}\nloo2(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.581687\n```\n:::\n\n```{.r .cell-code}\nloo2(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.016685\n```\n:::\n\n```{.r .cell-code}\nloo2(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.017154\n```\n:::\n\n```{.r .cell-code}\nloo2(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.01563\n```\n:::\n\n```{.r .cell-code}\n# les resultats coincident\n# loo2 est plus rapide pour calculer \n# le plus haut loo est pour mod1 et le plus bas pour mod4 \n# (même si resultats prochent entre mod2, mod3 et mod4)\n\n# modèle le plus parcimonieux est le mod2\n```\n:::\n\n\n\n\n\n# Question 7 --------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(boot)\n# Loo c'est K=1\n\nmod1_glm = glm(formula = fmla1 , family = gaussian, data = df)\nmod2_glm = glm(formula = fmla2 , family = gaussian, data = df)\nmod3_glm = glm(formula = fmla3 , family = gaussian, data = df)\nmod4_glm = glm(formula = fmla4 , family = gaussian, data = df)\n\n\ncvmod1 = cv.glm(data = df, glmfit = mod1_glm, K = 10) \ncvmod2 = cv.glm(data = df, glmfit = mod2_glm, K = 10) \ncvmod3 = cv.glm(data = df, glmfit = mod3_glm, K = 10) \ncvmod4 = cv.glm(data = df, glmfit = mod4_glm, K = 10) \n\nsummary(cvmod1$delta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  8.566   8.567   8.567   8.567   8.568   8.569 \n```\n:::\n\n```{.r .cell-code}\nloo2(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8.581687\n```\n:::\n\n```{.r .cell-code}\nsummary(cvmod2$delta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.015   1.015   1.016   1.016   1.016   1.016 \n```\n:::\n\n```{.r .cell-code}\nloo2(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.016685\n```\n:::\n\n```{.r .cell-code}\nsummary(cvmod3$delta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.014   1.014   1.014   1.014   1.015   1.015 \n```\n:::\n\n```{.r .cell-code}\nloo2(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.017154\n```\n:::\n\n```{.r .cell-code}\nsummary(cvmod4$delta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.017   1.017   1.017   1.017   1.017   1.017 \n```\n:::\n\n```{.r .cell-code}\nloo2(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.01563\n```\n:::\n\n```{.r .cell-code}\n# les valeurs ont la même ordre de grandeur\n```\n:::\n\n\n\n\n# Question 8 --------------------------------------------------------------\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# on regarde les 3 mod où loo faible \nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0319 -0.6942  0.0049  0.7116  3.2855 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.05006    0.03924   1.276    0.202    \nx            1.08894    0.03220  33.817   <2e-16 ***\nV3          -2.00919    0.02338 -85.943   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.007 on 997 degrees of freedom\nMultiple R-squared:  0.8911,\tAdjusted R-squared:  0.8909 \nF-statistic:  4080 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(mod3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3 + V4, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0691 -0.6821  0.0060  0.7023  3.3186 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.05425    0.03934   1.379    0.168    \nx            1.02684    0.05522  18.594   <2e-16 ***\nV3          -2.01489    0.02373 -84.917   <2e-16 ***\nV4           0.02177    0.01573   1.384    0.167    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 996 degrees of freedom\nMultiple R-squared:  0.8913,\tAdjusted R-squared:  0.891 \nF-statistic:  2723 on 3 and 996 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(mod4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3 + V4 + V5, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0765 -0.6854 -0.0009  0.7140  3.3274 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.018703   0.044616   0.419   0.6752    \nx            1.014166   0.055684  18.213   <2e-16 ***\nV3          -1.931446   0.054935 -35.158   <2e-16 ***\nV4           0.028192   0.016171   1.743   0.0816 .  \nV5          -0.016599   0.009858  -1.684   0.0925 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.005 on 995 degrees of freedom\nMultiple R-squared:  0.8917,\tAdjusted R-squared:  0.8912 \nF-statistic:  2047 on 4 and 995 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# les r2 sont tous bon mais le mod2 à toutes ses variables significatives (sauf intercept)\n# on va donc préférer le mod2\n\n# On enlève la cst non significative \nmod_final = lm(y ~ x + V3-1, data=df)\nmod_final_glm = glm(y ~ x + V3-1, data=df) # par defaut family = gaussian\nsummary(mod2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0319 -0.6942  0.0049  0.7116  3.2855 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.05006    0.03924   1.276    0.202    \nx            1.08894    0.03220  33.817   <2e-16 ***\nV3          -2.00919    0.02338 -85.943   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.007 on 997 degrees of freedom\nMultiple R-squared:  0.8911,\tAdjusted R-squared:  0.8909 \nF-statistic:  4080 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(mod_final)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3 - 1, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9910 -0.6570  0.0328  0.7467  3.3288 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|)    \nx   1.08779    0.03220   33.78   <2e-16 ***\nV3 -1.99176    0.01898 -104.97   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.007 on 998 degrees of freedom\nMultiple R-squared:  0.9216,\tAdjusted R-squared:  0.9215 \nF-statistic:  5870 on 2 and 998 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(mod_final_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ x + V3 - 1, data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.9910  -0.6570   0.0328   0.7467   3.3288  \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|)    \nx   1.08779    0.03220   33.78   <2e-16 ***\nV3 -1.99176    0.01898 -104.97   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1.014328)\n\n    Null deviance: 12919.5  on 1000  degrees of freedom\nResidual deviance:  1012.3  on  998  degrees of freedom\nAIC: 2856.1\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n\n```{.r .cell-code}\n# x = rnorm(1000)\n# y = x - 2*(x^2) + rnorm(1000)\n \n# REMARQUE :\n# quand on enlève l'intercept, les valeurs des coeffs bougent légèrement \n# alors que théoriquement identique \n# pareil, on a une différence sur le r2\n# Pourquoi ??\n\n# regardons le modèle plus simple\nmod1_sansIntercept = lm(y ~ x -1, data=df)\nsummary(mod1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.8594  -0.9890   0.7507   1.8526   5.0428 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.92130    0.09229 -20.818   <2e-16 ***\nx            0.89410    0.09310   9.604   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.918 on 998 degrees of freedom\nMultiple R-squared:  0.0846,\tAdjusted R-squared:  0.08369 \nF-statistic: 92.24 on 1 and 998 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(mod1_sansIntercept)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x - 1, data = df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.6785  -2.8966  -1.1766  -0.0688   3.1315 \n\nCoefficients:\n  Estimate Std. Error t value Pr(>|t|)    \nx   0.8626     0.1114   7.741  2.4e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.493 on 999 degrees of freedom\nMultiple R-squared:  0.05659,\tAdjusted R-squared:  0.05565 \nF-statistic: 59.93 on 1 and 999 DF,  p-value: 2.402e-14\n```\n:::\n\n```{.r .cell-code}\n# on constante le même soucis\n\nmean(x) # x n'est pas parfaitement centré\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01612787\n```\n:::\n\n```{.r .cell-code}\nvar(x) # var pas = 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9834589\n```\n:::\n\n```{.r .cell-code}\n# Don même si on a demandé à r de générer un x centré reduit,\n# enfait les données ne le sont pas parfaitement !!!!\n\n# on test en juste centrer le jeu de données\ndf_center = as.data.frame(scale(df, center=TRUE, scale=FALSE))\n\nmod2_center= lm(y ~ x + V3, data=df_center)\nmod_final_center = lm(y ~ x + V3-1, data=df_center)\nsummary(mod2_center)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3, data = df_center)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0319 -0.6942  0.0049  0.7116  3.2855 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.894e-17  3.184e-02    0.00        1    \nx            1.089e+00  3.220e-02   33.82   <2e-16 ***\nV3          -2.009e+00  2.338e-02  -85.94   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.007 on 997 degrees of freedom\nMultiple R-squared:  0.8911,\tAdjusted R-squared:  0.8909 \nF-statistic:  4080 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(mod_final_center)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3 - 1, data = df_center)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0319 -0.6942  0.0049  0.7116  3.2855 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|)    \nx   1.08894    0.03218   33.83   <2e-16 ***\nV3 -2.00919    0.02337  -85.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.006 on 998 degrees of freedom\nMultiple R-squared:  0.8911,\tAdjusted R-squared:  0.8909 \nF-statistic:  4085 on 2 and 998 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# quasi plus de différence pour coeffs et r2\n\n\n# on test en scale le jeu de données\ndf_scale = as.data.frame(scale(df))\n\nmod2_scale= lm(y ~ x + V3, data=df_scale)\nmod_final_scale = lm(y ~ x + V3-1, data=df_scale)\nsummary(mod2_scale)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3, data = df_scale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.99458 -0.22772  0.00162  0.23344  1.07780 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.739e-17  1.044e-02    0.00        1    \nx            3.543e-01  1.048e-02   33.82   <2e-16 ***\nV3          -9.003e-01  1.048e-02  -85.94   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3303 on 997 degrees of freedom\nMultiple R-squared:  0.8911,\tAdjusted R-squared:  0.8909 \nF-statistic:  4080 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(mod_final_scale)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x + V3 - 1, data = df_scale)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.99458 -0.22772  0.00162  0.23344  1.07780 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|)    \nx   0.35425    0.01047   33.83   <2e-16 ***\nV3 -0.90030    0.01047  -85.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3301 on 998 degrees of freedom\nMultiple R-squared:  0.8911,\tAdjusted R-squared:  0.8909 \nF-statistic:  4085 on 2 and 998 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\n# quasi plus de différence pour coeffs et r2\n\n\n## remarque : \n# le r2 et r2aj est le meilleur pour mod_final qui correspond\n# au mod 2 sans intercept mais où l'on a pas scale le df\n```\n:::\n",
    "supporting": [
      "Exercice_3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "604741ef1b1b2cdbe2dd2b0e60be882d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercice 07\"\nauthor: \"Clément Poupelin\"\ndate: \"2025-02-25\"\ndate-modified: \"2025-02-25\"\nformat: \n  html:\n    embed-resources: false\n    toc: true\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: true\n    toc-location: right\n    page-layout: article\n    code-overflow: wrap\ntoc: true\nnumber-sections: false\neditor: visual\ncategories: [\"Régression linéaire\", \"Sélection automatique\", \"Régression sur composantes principales\", \"Régression des moindres carrés partiels\", \"Régression Ridge\", \"Régression Lasso\", \"Validation croisée\"]\nimage: \"/img/baseball.png\"\ndescription: \"On continu sur les données de baseball avec les techniques de régression **Ridge** et **Lasso**. Puis on compare les résultats avec ceux obtenus précédemment\"\n---\n\n\n\n# Intervenant.e.s\n\n### Rédaction\n\n-   **Clément Poupelin**, [clementjc.poupelin\\@gmail.com](mailto:clementjc.poupelin@gmail.com){.email}\\\n\n### Relecture\n\n-   \n\n# Setup\n\n:::: panel-tabset\n## Packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Données\nlibrary(ISLR)         # Hitters data \nlibrary(dplyr)        # manipulation des données\n\n# Infrence\nlibrary(pls) ## PCR et PLSR\nlibrary(glmnet) ## regression pénalisée\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n```\n:::\n\n\n\n## Fonctions\n\n::: panel-tabset\n\n### Intercative Boxplot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_interactive_boxplot <- function(data) {\n  data_long <- reshape2::melt(data)\n  \n  p <- ggplot(data_long,\n              aes(\n                x = variable,\n                y = value,\n                fill = variable,\n                stat = \"identity\"\n              )) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    labs(title = \"Distribution des Variables (Intercative Boxplot)\", x = \"Variables\", y = \"Valeurs\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  return(plotly::ggplotly(p))\n}\n```\n:::\n\n\n\n\n\n\n:::\n\n## Seed\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(140400)\n```\n:::\n\n\n::::\n\n# Données\n\nCette exercice est la suite direct de l'[Exercice 06](../posts/Exercice_06.qmd) où l'on a pu utiliser les méthodes de regressions avec reduction de dimension **PCR** et **PLSR**.\n\nOn va donc reprendre les mêmes données avec le même découpage en *train* et *test*.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHitters_Without_NA <- Hitters %>% na.omit()\npercent_to_draw <- 0.75\nindex_train <- sample(nrow(Hitters_Without_NA), size = floor(percent_to_draw * nrow(Hitters_Without_NA)))\n\nHitters_train <- Hitters_Without_NA[index_train, ]\n\nHitters_test <- Hitters_Without_NA[-index_train, ]\n```\n:::\n\n\n\nNotre objectif ici sera donc de compléter l'analyse de l'[Exercice 06](../posts/Exercice_06.qmd) en utilisant cette fois ci les méthodes de regression pénalisée **Ridge** et **Lasso** (méthodes détaillées dans l'[Exercice 06 Bonus : Ridge vs Lasso](../posts/Exercice_06Bonus.qmd)).\n\n# Analyse inférentielle\n\nContrairement à la plupart des autres package *`R`* qui permettent de faire de l’apprentissage, le package *`glmnet`* n’autorise pas l’utilisation de formules.\\\nIl faut donc spécifier explicitement la matrice $X$ et le vecteur $y$.\\\n\nOn peut obtenir la matrice $X$ et notamment le codage des variables qualitatives avec la fonction *`model.matrix`*.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX_train <- model.matrix(Salary ~ ., data = Hitters_train)[, -1]\nY_train <- Hitters_train$Salary\n\nX_test <- model.matrix(Salary ~ ., data = Hitters_test)[, -1]\nY_test <- Hitters_test$Salary\n```\n:::\n\n\n\nEt ce n'est qu'après que l'on peut mettre en place la modélisation.\n\n::::::::: panel-tabset\n## Ridge\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.R <- glmnet(X_train, Y_train, alpha = 0) \n```\n:::\n\n\n\nOn peut également visualiser les chemins de régularisation des estimateurs **Ridge**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod.R, xvar = \"lambda\", label = TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercice_07_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\nIci on voit l'évolution de nos coefficients $\\beta$ en fonction des diffrentes valeurs de $\\lambda$.\\\nAinsi, sur la gauche on se retrouve dans la situation où il n'y a pas de pénalisation et donc nos coefficients sont les $\\beta$ de l'estimation par moindres carrés. Et donc plus $\\lambda$ va augmenter, plus on se retrouvera dans une situation où les coefficients vont tendrent vers 0.\n\n\n## Lasso\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.L <- glmnet(X_train, Y_train, alpha = 1) \n```\n:::\n\n\n\nOn peut également visualiser les chemins de régularisation des estimateurs **Lasso**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mod.L, xvar = \"lambda\", label = TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercice_07_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\nIci on voit l'évolution de nos coefficients $\\beta$ en fonction des diffrentes valeurs de $\\lambda$.\\\nAinsi, sur la gauche on se retrouve dans la situation où il n'y a pas de pénalisation et donc nos coefficients sont les $\\beta$ de l'estimation par moindres carrés. Et donc plus $\\lambda$ va augmenter, plus on se retrouvera dans une situation où les coefficients vont tendrent vers 0.\n\n:::::::::\n\n# Selection des paramètres de régularisation\n\nMaintenant que les modèles sont estimés avec plusieurs valeurs de $\\lambda$ possibles, il se pose la question du choix du bon paramètre.\\\nPour cela, on utilise la fonction *`cv.glmnet`* qui, comme son nom le laisse suggérer, permet d'effectuer une validation croisée pour notre modèle avec par défaut *`nfolds=10`* (le nombre de pli pour le découpage de sous ensembles). Puis on peut faire un *`plot`* de l’objet.\n\n::: panel-tabset\n## Ridge\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridgeCV <- cv.glmnet(X_train, Y_train, alpha = 0) \nplot(ridgeCV)\n```\n\n::: {.cell-output-display}\n![](Exercice_07_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\nOn visualise ici les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de $\\lambda$ (échelle logarithmique). Deux traits verticaux sont représentés :\n\n-   celui de gauche correspond à la valeur de $\\lambda$ qui minimise l’erreur quadratique\n\n-   celui de droite correspond à la plus grande valeur de $\\lambda$ telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur\n\nD’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux.\\\nA savoir que si l'on veut diminuer la complexité du modèle on choisira la valeur de droite.\n\n::: callout-warning\nAttention, on peut remarquer ici que l'axe verticale de gauche semble toucher le bord du plot. Dans ces cas là, il convient de parametrer les $\\lambda$ de telle sorte à \"explorer\" des valeurs de $\\lambda$ plus petite.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nridgeCV2 <- cv.glmnet(X_train, Y_train, alpha = 0, lambda = seq(exp(-1), exp(9), by = 1)) \nplot(ridgeCV2)\n```\n\n::: {.cell-output-display}\n![](Exercice_07_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nCette fois ci, nos deux axes verticaux sont éloignés du bord du graphe et on voit bien qu'on a pu baisser ma valeur minimal de $\\lambda$.\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\nIci on on obtient $\\lambda_{min} =$ 1.368 et $\\lambda_{1se} =$ 1468.368 \n:::\n\n## Lasso\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlassoCV <- cv.glmnet(X_train, Y_train, alpha = 1)\nplot(lassoCV)\n```\n\n::: {.cell-output-display}\n![](Exercice_07_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nOn visualise ici les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de $\\lambda$ (échelle logarithmique). Deux traits verticaux sont représentés :\n\n-   celui de gauche correspond à la valeur de $\\lambda$ qui minimise l’erreur quadratique\n\n-   celui de droite correspond à la plus grande valeur de $\\lambda$ telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur\n\nD’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux.\\\nA savoir que si l'on veut diminuer la complexité du modèle on choisira la valeur de droite.\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\nIci on on obtient $\\lambda_{min} =$ 1.167 et $\\lambda_{1se} =$ 84.295 \n:::\n\n:::\n\n\n# Prédiction et comparaison\n\nOn souhaite maintenant prédiction pour le jeu de données *test*.\n\nUne première approche pourrait consister à réajuster le modèle sur toutes les données pour la valeur de $\\lambda$ sélectionnée.\\\nCette étape est en réalité déjà effectuée par la fonction *`cv.glmnet`*. Il suffit par conséquent d’appliquer la fonction *`predict`* à l’objet obtenu avec *`cv.glmnet`* en spécifiant la valeur de $\\lambda$ souhaitée puis on calcul l'erreur de prediction pour les modèles **Ridge** et **Lasso**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred.ridge_min <- predict(ridgeCV, newx = X_test, s = \"lambda.min\")\nrmsep_ridge <- sqrt(mean((pred.ridge_min - Y_test)^2, na.rm=T))\n\npred.lasso_min <- predict(lassoCV, newx = X_test, s = \"lambda.min\") \nrmsep_lasso <- sqrt(mean((pred.lasso_min - Y_test)^2, na.rm=T))\n```\n:::\n\n\n\n\nAinsi on peut obtenir l'erreur de prédiction via le RMSEP pour les 2 modèles et les comparer avec les valeurs obtenues pour les méthodes précédemment testées : **Sélection automatique both** ([Exercice 01](../posts/Exercice_01.qmd)), **PCR** ([Exercice 06](../posts/Exercice_06.qmd)), **PLSR** ([Exercice 06](../posts/Exercice_06.qmd).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Sélection automatique\n########################\nmod0 <- lm(Salary ~ 0, data = Hitters_train)\nmod1 <- lm(Salary ~ ., data = Hitters_train)\nmod_step <- step(\n  mod0,\n  scope = formula(mod1),\n  trace = FALSE,\n  direction = \"both\",\n  k = log(nrow(Hitters_train))\n)\n\nhat_Hitters_test_mod_step <- predict(mod_step, Hitters_test)\nrmsep_mod_step <- sqrt(mean((\n  hat_Hitters_test_mod_step - Hitters_test$Salary\n) ** 2))\n\n\n## PCR\n########################\nmod_pcr <- pcr(\n  Salary ~ .,\n  scale = TRUE,\n  data = Hitters_train,\n  validation = \"CV\",\n  segments = 10\n)\nncomp.rmsep_pcr <- which.min(RMSEP(mod_pcr, estimate = c(\"CV\"))$val[\"CV\", , ]) -\n  1\nhat_Hitters_test_mod_pcr <- predict(mod_pcr, Hitters_test, ncomp = (which.min(RMSEP(\n  mod_pcr, estimate = c(\"CV\")\n)$val[\"CV\", , ]) - 1))\nrmsep_mod_pcr <- sqrt(mean((\n  hat_Hitters_test_mod_pcr - Hitters_test$Salary\n) ** 2))\n\n## PLS\n########################\nmod_pls <- plsr(\n  Salary ~ .,\n  scale = TRUE,\n  data = Hitters_train,\n  validation = \"CV\",\n  segments = 10\n)\nncomp.rmsep_pls <- which.min(RMSEP(mod_pls, estimate = c(\"CV\"))$val[\"CV\", , ]) -\n  1\nhat_df_test_salary.pls <- predict(mod_pls, Hitters_test, ncomp = (which.min(RMSEP(\n  mod_pls, estimate = c(\"CV\")\n)$val[\"CV\", , ]) - 1))\nrmsep_mod_pls <- sqrt(mean((\n  hat_df_test_salary.pls - Hitters_test$Salary\n) ** 2))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmethods <- c(\"Err_mod_step\",\n             \"Err_PCR\",\n             \"Err_PLS\",\n             \"Err_Ridge\",\n             \"Err_Lasso_glmnet\")\nerrors <- c(rmsep_mod_step,\n            rmsep_mod_pcr,\n            rmsep_mod_pls,\n            rmsep_ridge,\n            rmsep_lasso)\nrmsep_pred_df <- data.frame(Method = factor(methods, levels = methods), Error = round(errors, 3))\n\np <- ggplot(rmsep_pred_df, aes(x = Method, y = Error, fill = Method)) +\n  geom_bar(stat = \"identity\",\n           width = 0.6,\n           color = \"black\") +\n  geom_text(aes(label = round(Error, 3)), fontface = \"bold\", vjust = -1.5, size = 6) +\n  scale_fill_viridis_d() +\n  ylim(0, 450) +\n  theme_minimal() +\n  labs(title = \"Erreur pour les différentes méthodes\", x = \"Méthode\", y = \"Erreur\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\np\n```\n\n::: {.cell-output-display}\n![](Exercice_07_files/figure-html/unnamed-chunk-14-1.png){width=768}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrmsep_pred_df %>% DT::datatable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-05bfd1e9cf194e6e25b2\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-05bfd1e9cf194e6e25b2\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\"],[\"Err_mod_step\",\"Err_PCR\",\"Err_PLS\",\"Err_Ridge\",\"Err_Lasso_glmnet\"],[378.84,374.732,375.126,343.754,368.222]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Method<\\/th>\\n      <th>Error<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Method\",\"targets\":1},{\"name\":\"Error\",\"targets\":2}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\nOn peut voir ici que c'est la méthode **Ridge** qui minimise l'erreur de prédiction.\n:::\n\n\n\\\n\nPour confirmer ou contredire nos résultats, il convient maintenant de tester les méthodes $K$ fois pour btenir $K$ erreurs de prédictions et ainsi regarder la méthode qui, en moyenne, va minimiser cette erreur tout en prenant en compte la variance des erreurs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK <- 10\nn <- nrow(Hitters_Without_NA)\nseg <- pls::cvsegments(n, K)\nerr_step <- NULL\nerr_pcr <- NULL\nerr_pls <- NULL\nerr_ridge <- NULL\nerr_lasso <- NULL\nerr_lars <- NULL\nfor (i in 1:K) {\n  test <- seg[[i]]\n  mod0 <- lm(Salary ~ 0, data = Hitters_Without_NA, subset = -test)\n  mod1 <- lm(Salary ~ ., data = Hitters_Without_NA, subset = -test)\n  mod_step <- step(\n    mod0,\n    scope = formula(mod1),\n    direction = \"both\",\n    k = log(n),\n    trace = 0\n  )\n  mod_step_pred <- predict(mod_step, Hitters_Without_NA[test, ])\n  err_step[i] <- sqrt(mean((mod_step_pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                            T))\n  \n  pcr.fit <- pcr(\n    Salary ~ .,\n    data = Hitters_Without_NA,\n    scale = TRUE,\n    subset = -test,\n    validation = \"CV\",\n    segments = 10\n  )\n  nb_comp <- which.min(RMSEP(pcr.fit, 'CV')$val[, , 1:10])\n  pcr.pred <- predict(pcr.fit, Hitters_Without_NA[test, ], ncomp = nb_comp)\n  err_pcr[i] <- sqrt(mean((pcr.pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                            T))\n  \n  pls.fit <- plsr(\n    Salary ~ .,\n    data = Hitters_Without_NA,\n    subset = -test,\n    scale = TRUE,\n    validation = \"CV\"\n  )\n  nb_comp <- which.min(RMSEP(pls.fit, 'CV')$val[, , 1:10])\n  pls.pred <- predict(pls.fit, Hitters_Without_NA[test, ], ncomp = nb_comp)\n  err_pls[i] <- sqrt(mean((pls.pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                            T))\n  \n  train.mat <- model.matrix(Salary ~ ., data = Hitters_Without_NA[-test, ])\n  train.mat <- train.mat[, -1]\n  y <- Hitters_Without_NA[-test, 19]\n  test.mat <- model.matrix(Salary ~ ., data = Hitters_Without_NA[test, ])\n  test.mat <- test.mat[, -1]\n  ytest <- Hitters_Without_NA[test, 19]\n  \n  \n  ridge.cv <- cv.glmnet(train.mat, y, alpha = 0, lambda = seq(1, 5000))\n  lambdachoisi <- ridge.cv$lambda.min\n  ridge.pred <- predict(ridge.cv, test.mat, s = lambdachoisi)\n  err_ridge[i] <- sqrt(mean((ridge.pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                              T))\n  \n  \n  lasso.cv <- cv.glmnet(train.mat, y, alpha = 1)\n  lasso.pred <- predict(lasso.cv, test.mat, s = lasso.cv$lambda.min)\n  err_lasso[i] <- sqrt(mean((lasso.pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                              T))\n  \n  # lars.cv <- cv.lars(train.mat, y)\n  # choix <- lars.cv$index[which.min(lars.cv$cv)]\n  # temp <- lars(train.mat, y)\n  # lars.pred <- predict(temp, train.mat, s=choix, mode='fraction')$fit\n  # err_lars[i] <- sqrt(mean((lars.pred - Hitters_Without_NA[test,19])^2, na.rm=T))\n  \n}\n\nerr_pred_df <- cbind(err_step, err_pcr, err_pls, err_ridge, err_lasso) %>% as.data.frame()\ncolnames(err_pred_df) <- c(\"Err_mod_step\", \"Err_PCR\", \"Err_PLS\", \"Err_Ridge\", \"Err_Lasso_glmnet\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_interactive_boxplot(err_pred_df)\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-e8eedaf1ee809842645b\" style=\"width:100%;height:557px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-e8eedaf1ee809842645b\">{\"x\":{\"data\":[{\"x\":[1,1,1,1,1,1,1,1,1,1],\"y\":[367.65600620350796,276.36621460102225,275.83889720214432,508.41627482140848,398.46377843377809,407.28955439894133,254.34548929221359,289.64406371627547,360.73647212382463,324.14746640403348],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(68,1,84,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.6692913385826778},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.8897637795275593},\"name\":\"Err_mod_step\",\"legendgroup\":\"Err_mod_step\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[2,2,2,2,2,2,2,2,2,2],\"y\":[335.22242521589794,240.64012863297881,288.79332475454027,513.73250483761433,390.89991776235127,410.28668888318606,250.42075385681321,272.54004459256424,335.9931750466115,289.10333626758813],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(59,82,139,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.6692913385826778},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.8897637795275593},\"name\":\"Err_PCR\",\"legendgroup\":\"Err_PCR\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[3,3,3,3,3,3,3,3,3,3],\"y\":[359.88945732827841,266.06109247658827,273.9204346498463,482.16986444884134,382.27190049379197,434.04538798993661,263.89182877371968,303.28446483068302,353.81171640388339,278.75167279731346],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(33,144,140,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.6692913385826778},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.8897637795275593},\"name\":\"Err_PLS\",\"legendgroup\":\"Err_PLS\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[4,4,4,4,4,4,4,4,4,4],\"y\":[306.61352372911068,253.7901179140062,266.88677808584811,487.07446311548961,337.52280018381845,445.92641880990482,254.59551803910779,314.37559798213834,359.41692577012452,282.16161568457738],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(93,200,99,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.6692913385826778},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.8897637795275593},\"name\":\"Err_Ridge\",\"legendgroup\":\"Err_Ridge\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[5,5,5,5,5,5,5,5,5,5],\"y\":[336.36911776157797,259.05428890905142,278.86828928161651,505.7755490467332,342.86032762273049,432.42880047295768,253.93626678077888,315.11168943525541,347.41584553905665,283.08998370918061],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(253,231,37,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.8897637795275593,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.6692913385826778},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.8897637795275593},\"name\":\"Err_Lasso_glmnet\",\"legendgroup\":\"Err_Lasso_glmnet\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":42.057838660578383,\"r\":7.3059360730593621,\"b\":76.296743058867008,\"l\":43.105022831050235},\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724},\"title\":{\"text\":\"Distribution des Variables (Intercative Boxplot)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.534246575342465},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.40000000000000002,5.5999999999999996],\"tickmode\":\"array\",\"ticktext\":[\"Err_mod_step\",\"Err_PCR\",\"Err_PLS\",\"Err_Ridge\",\"Err_Lasso_glmnet\"],\"tickvals\":[1,2,3,4,5],\"categoryorder\":\"array\",\"categoryarray\":[\"Err_mod_step\",\"Err_PCR\",\"Err_PLS\",\"Err_Ridge\",\"Err_Lasso_glmnet\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-45,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"Variables\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[226.98550982274705,527.38712364784612],\"tickmode\":\"array\",\"ticktext\":[\"300\",\"400\",\"500\"],\"tickvals\":[300,400,500],\"categoryorder\":\"array\",\"categoryarray\":[\"300\",\"400\",\"500\"],\"nticks\":null,\"ticks\":\"\",\"tickcolor\":null,\"ticklen\":3.6529680365296811,\"tickwidth\":0,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.68949771689498},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176002,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"Valeurs\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":true,\"legend\":{\"bgcolor\":null,\"bordercolor\":null,\"borderwidth\":0,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.68949771689498},\"title\":{\"text\":\"variable\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.611872146118724}}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"1a0023a07653e\":{\"x\":{},\"y\":{},\"fill\":{},\"stat\":{},\"type\":\"box\"}},\"cur_data\":\"1a0023a07653e\",\"visdat\":{\"1a0023a07653e\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmethods <- c(\"Err_mod_step\",\n             \"Err_PCR\",\n             \"Err_PLS\",\n             \"Err_Ridge\",\n             \"Err_Lasso_glmnet\")\nerrors.mean <- sapply(list(err_step, err_pcr, err_pls, err_ridge, err_lasso), function(x) mean(x))\nerrors.median <- sapply(list(err_step, err_pcr, err_pls, err_ridge, err_lasso), function(x) median(x))\nq1_values <- sapply(list(err_step, err_pcr, err_pls, err_ridge, err_lasso), function(x) quantile(x, 0.25))\nq3_values <- sapply(list(err_step, err_pcr, err_pls, err_ridge, err_lasso), function(x) quantile(x, 0.75))\niq_values <- q3_values - q1_values\nerr.mean_pred_df <- data.frame(\n  Method = methods,\n  Mean_Error = round(errors.mean, 3),\n  Median_Error = round(errors.median, 3),\n  InterQartile_Error = round(iq_values, 3)\n)\nerr.mean_pred_df %>% DT::datatable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-d349f2838270cc6b823b\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-d349f2838270cc6b823b\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\"],[\"Err_mod_step\",\"Err_PCR\",\"Err_PLS\",\"Err_Ridge\",\"Err_Lasso_glmnet\"],[346.29,332.763,339.81,330.836,335.491],[342.442,312.163,328.548,310.495,325.74],[111.076,100.57,101.548,83.238,66.35299999999999]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Method<\\/th>\\n      <th>Mean_Error<\\/th>\\n      <th>Median_Error<\\/th>\\n      <th>InterQartile_Error<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Method\",\"targets\":1},{\"name\":\"Mean_Error\",\"targets\":2},{\"name\":\"Median_Error\",\"targets\":3},{\"name\":\"InterQartile_Error\",\"targets\":4}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\nEn analysant les résultats, on constate que la méthode **Ridge** minimise à la fois l'erreur moyenne et l'erreur médiane, suivie de près par la méthode **PCR**, puis **Lasso**. En revanche, les valeurs d'erreurs sont moins variables pour la méthode **Lasso**, suivie de **Ridge**, et enfin **PLSR**.\n\nCela suggère que, pour choisir la méthode la plus efficace dans notre contexte, il convient de trouver un bon compromis entre la réduction de l'erreur moyenne et la faible variabilité des erreurs. Dans ce cas, les méthodes **Ridge** et **Lasso** semblent offrir le meilleur équilibre. Le choix final pourrait alors dépendre de la préférence accordée à une méthode qui minimise les fluctuations des erreurs.\n:::\n\n::: callout-note \nPour les curieux, il pourra être intéressant de faire varier la valeur de $K$ et ainsi remarquer que les méthodes les plus performantes ne sont pas toujours les mêmes.\n:::\n\n# Conclusion\n\nEn conclusion, nous avons pu observer l'efficacité des méthodes **Ridge** et **Lasso** par rapport à **PCR**, **PLSR** et la **sélection forward**. Ces deux dernières méthodes se distinguent par une capacité à minimiser les erreurs moyennes tout en offrant une stabilité dans les prédictions.\\\n\nCependant, il est important de noter que **PCR**, **PLSR** et la **sélection forward** présentent aussi de bonnes performances et restent donc des méthodes pertinentes qui peuvent être particulièrement utiles dans certains contextes, en fonction des caractéristiques spécifiques des données et des objectifs de modélisation.\\\nAinsi, le choix de la méthode dépendra du compromis souhaité entre précision et variabilité des résultats.\n\n# Session info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(pkgs = \"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-02-25\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n glmnet    * 4.1-8   2023-08-22 [1] CRAN (R 4.4.2)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.4.2)\n ISLR      * 1.4     2021-09-15 [1] CRAN (R 4.4.2)\n Matrix    * 1.6-5   2024-01-11 [4] CRAN (R 4.3.2)\n pls       * 2.8-5   2024-09-15 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n",
    "supporting": [
      "Exercice_07_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/datatables-binding-0.33/datatables.js\"></script>\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<script src=\"../site_libs/plotly-binding-4.10.4/plotly.js\"></script>\n<script src=\"../site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<link href=\"../site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
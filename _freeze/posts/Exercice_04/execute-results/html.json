{
  "hash": "4db5df45fe89ad015d811c546655c204",
  "result": {
    "markdown": "---\ntitle: \"Exercice 4\"\nauthor: \"Clément Poupelin\"\ndate: \"2025-02-19\"\nformat: \n  html:\n    embed-resources: false\n    toc: true\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: true\n    toc-location: right\n    page-layout: article\n    code-overflow: wrap\ntoc: true\nnumber-sections: false\neditor: visual\ncategories: [\"TP\"]\nimage: \"/img/baseball.png\"\ndescription: \"\"\n---\n\n\n\n\n# Setup\n\n::: panel-tabset\n## packages\n\n::: {.cell}\n\n```{.r .cell-code}\n# Données\nlibrary(ISLR)         # Caravan data \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'ISLR' a été compilé avec la version R 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)        # manipulation des données\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'dplyr' a été compilé avec la version R 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(car)          # pour VIF\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'car' a été compilé avec la version R 4.2.3\n```\n:::\n\n```{.r .cell-code}\n# PCA\nlibrary(FactoMineR)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'FactoMineR' a été compilé avec la version R 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(factoextra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'ggplot2' a été compilé avec la version R 4.2.3\n```\n:::\n\n```{.r .cell-code}\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(reshape2)     # transformer les données en format long\nlibrary(gridExtra)\n\n## for pairs panel\nlibrary(psych)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'psych' a été compilé avec la version R 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(RColorBrewer)\n```\n:::\n\n\n## fonctions\n\n::: panel-tabset\n### boxplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_boxplot <- function(data) {\n  # Transformer les données en format long pour ggplot\n  data_long <- melt(data)\n  \n  ggplot(data_long, aes(x = variable, y = value, fill = variable)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +  # Palette de couleurs harmonieuse\n    labs(title = \"Distribution des Variables (Boxplot)\", x = \"Variables\", y = \"Valeurs\") +\n    theme_minimal() +  # Thème épuré\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotation des étiquettes\n}\n```\n:::\n\n\n### pairs.panels\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_pairs.panels <- function(data){\n  pairs.panels(\n    data,\n    method = \"pearson\",      # Méthode de corrélation \n    hist.col = brewer.pal(9, \"Set3\"), # Couleurs des histogrammes\n    density = TRUE,          # Ajout des courbes de densité\n    ellipses = TRUE,         # Ajout d'ellipses \n    smooth = TRUE,           # Ajout de régressions lissées\n    lm = TRUE,               # Ajout des droites de régression\n    col = \"#69b3a2\",         # Couleur des points\n    alpha = 0.5              # Transparence \n    )\n}\n```\n:::\n\n:::\n::::\n\n\n# Données\n\nOn considère le jeu de données Caravan de la librairie ISLR de R. Ce jeu de données contient, pour 5822 clients d’une assurance, 86 variables décrivant leur profil. La dernière variable Purchase indique si le client a souscrit\nune assurance pour caravane ou non.\n\nAu vu des dimensions, on se passera de regarder le summary.\n\n::: {.cell}\n\n```{.r .cell-code}\nCaravan %>% dim()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5822   86\n```\n:::\n:::\n\nVérifions juste s'il y a des valeurs manquantes \n\n::: {.cell}\n\n```{.r .cell-code}\nanyNA(Caravan)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\nOn s'intéressera à la variable purchase. Celle ci peut déjà nous indiquer que le pourcentage de clients ayant souscrit à une assurance Caravan est de 5.98%\n\n\n# Analyse descriptive\n\n::::::::::::::::::::: panel-tabset\n## Boxplot\n\nOn peut regarder un peu la distribution de nos différents variables quantitatives via des boxplots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_boxplot(Caravan)\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-6-1.png){width=1536}\n:::\n:::\n\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\n\n\n:::\n\nPour confirmer cela, on peut faire des boxplot pour uniquement une varibale et ses différents points de temps.\n\n\n## Correlation panel\n\nOn regarde ici la corrélation calculée entre chacune de nos variables.\n\nComme nos données sont de grande dimension, on se propose de réduire le nombre d'individus et de prendre seulement quelques variables. Cela ne permettra donc pas d'avoir une étude précise des corrélation mais juste voir si sur un sous échantillon on peut déjà déceler des particularité\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_pairs.panels(Caravan[1:500, 1:20])\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-7-1.png){width=2688}\n:::\n:::\n\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\n\n:::\n\n## PCA\n\nAvec une Analyse en Composantes Principales (PCA) on peut regarder un peu le comportement de nos données.\n\nEn effet, Cette méthode respose sur la transformation des variables d'origine en nouvelles variables non corrélées, appelées composantes principales, qui capturent successivement la plus grande variance possible des données.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_pca <- PCA(Caravan, \n               quali.sup = c(which(colnames(Caravan) %in% c(\"Purchase\"))),\n               graph = FALSE)\n```\n:::\n\n\nIci, on spécifi notre variable qualitative en variable supplémentaire, ce qui veut d'ire qu'elles ne seront pas considérés pour la formation de nos composantes principales (variable que l'on cherchera à estimer plus tard).\n\n::::::::::::: panel-tabset\n#### Barplot des variances\n\nTout d'abord, on peut commencer par regarder le pourcentage de variance expliqué par nos différentes composantes principales.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_eig(res_pca, \n         ncp = 15,\n         addlabels = TRUE, \n         barfill = \"coral\",\n         barcolor = \"coral\",\n         ylim = c(0, 15),\n         main = \"Percentage of variance of the 15 first components\")\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\nOn voit ainsi que la variance expliqué par nos deux premiers axes est d'environ 16%. Ce qui est une situation que l'on peut facilement retrouver dans des cas de grandes dimensions avec beaucoup de variables.\n:::\n\n#### Individus\n\nLe plan des individus est une projection des observations sur les axes principaux de la PCA. Cette visualisation permet d’identifier des regroupements, tendances et anomalies au sein des données.\n\nAinsi, des individus proches sur le graphique ont des caractéristiques similaires par rapport aux variables utilisées.\n\nPuis, le placement d'un individu en fonction des axes peut permettre de savoir comment le jouer se caractérise par rapport aux variables qui contribuent le plus à ces axes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_pca_ind(res_pca,\n             label=\"none\", \n             pointsize = 2,\n             habillage=as.factor(Caravan$Purchase),\n             addEllipses=TRUE,\n             ellipse.level=0.95)\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-10-1.png){width=768}\n:::\n:::\n\n\n\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\nIci on voit une repartition plutot uniforme sur le plan qui ne semble pas permettre de distinguer une séparation forte correspodant à notre variable qualitative.\n:::\n\n#### Variables\n\nLe cercle des variables est une représentation graphique qui permet d’analyser les relations entre les variables initiales et les composantes principales qui forment nos axes. Il est basé sur les corrélations entre les variables et les axes principaux.\n\nAinsi, plus une variable est proche du bord du cercle, plus elle est bien représentée sur le plan factoriel et contribue fortement à la formation des axes. Ici, on utilise le cos2 pour le gradient de couleur qui va aider à l'indentifictation de ces différentes qualitées de représentation.\n\nDe plus, selon l'angle entre deux varibles, on peut faire des suppositions sur leur corrélation :\n\n-   Si deux variables ont des vecteurs proches (petit angle), elles sont fortement corrélées positivement\n\n-   Si deux variables ont des vecteurs opposés (angle proche de 180°), elles sont corrélées négativement\n\n-   Si l’angle est proche de 90°, alors les variables ne sont pas corrélées\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_pca_var(res_pca, \n             col.var = \"cos2\",\n             gradient.cols = rainbow(n = 8, start = .6, end = .9),\n             repel = TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-11-1.png){width=1152}\n:::\n:::\n\n\n:::: success-header\n::: success-icon\n:::\n\nRésultats\n::::\n\n::: success\nIci aussi, du fait du grand nombre de variable il est difficile de dicerner quelque chose de pertinent.\n\nMaintenant, certaines variables sont tout de même bien représenter sur nos premiers axes et sont assez proche, ce qui témoigne d'une corrélation entre elles.\n:::\n:::::::::::::\n:::::::::::::::::::::\n\n\n\n\n\n# Analyse inférentielle\n\n## Modèle brut\n\nAjustons un modèle de régression logistique modélisant la probabilité de souscrire une assurance caravane en fonction de toutes les autres variables à disposition\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- glm(Caravan$Purchase~.,\n                family = binomial,\n                Caravan)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n```\n:::\n\n```{.r .cell-code}\nmod1 %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Caravan$Purchase ~ ., family = binomial, data = Caravan)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7047  -0.3711  -0.2450  -0.1588   3.2916  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.542e+02  1.116e+04   0.023  0.98183    \nMOSTYPE      6.580e-02  4.624e-02   1.423  0.15468    \nMAANTHUI    -1.832e-01  1.927e-01  -0.951  0.34157    \nMGEMOMV     -2.696e-02  1.399e-01  -0.193  0.84723    \nMGEMLEEF     2.096e-01  1.016e-01   2.063  0.03911 *  \nMOSHOOFD    -2.767e-01  2.076e-01  -1.333  0.18247    \nMGODRK      -1.142e-01  1.069e-01  -1.068  0.28535    \nMGODPR      -1.910e-02  1.177e-01  -0.162  0.87112    \nMGODOV      -1.618e-02  1.055e-01  -0.153  0.87818    \nMGODGE      -6.817e-02  1.113e-01  -0.612  0.54024    \nMRELGE       2.310e-01  1.566e-01   1.475  0.14031    \nMRELSA       8.509e-02  1.466e-01   0.580  0.56169    \nMRELOV       1.467e-01  1.562e-01   0.939  0.34759    \nMFALLEEN    -8.291e-02  1.311e-01  -0.633  0.52702    \nMFGEKIND    -1.154e-01  1.337e-01  -0.863  0.38813    \nMFWEKIND    -8.140e-02  1.417e-01  -0.575  0.56561    \nMOPLHOOG     9.717e-04  1.311e-01   0.007  0.99408    \nMOPLMIDD    -9.077e-02  1.365e-01  -0.665  0.50605    \nMOPLLAAG    -1.994e-01  1.376e-01  -1.449  0.14740    \nMBERHOOG     8.883e-02  9.349e-02   0.950  0.34204    \nMBERZELF     3.918e-02  9.897e-02   0.396  0.69219    \nMBERBOER    -1.169e-01  1.104e-01  -1.059  0.28951    \nMBERMIDD     1.353e-01  9.191e-02   1.472  0.14106    \nMBERARBG     3.976e-02  9.067e-02   0.438  0.66104    \nMBERARBO     9.954e-02  9.143e-02   1.089  0.27628    \nMSKA         2.690e-02  1.035e-01   0.260  0.79502    \nMSKB1       -8.801e-03  1.011e-01  -0.087  0.93064    \nMSKB2        1.200e-02  9.081e-02   0.132  0.89485    \nMSKC         9.016e-02  9.958e-02   0.905  0.36527    \nMSKD        -2.468e-02  9.724e-02  -0.254  0.79967    \nMHHUUR      -1.472e+01  8.140e+02  -0.018  0.98557    \nMHKOOP      -1.469e+01  8.140e+02  -0.018  0.98561    \nMAUT1        1.819e-01  1.514e-01   1.202  0.22953    \nMAUT2        1.507e-01  1.371e-01   1.099  0.27162    \nMAUT0        9.325e-02  1.436e-01   0.649  0.51603    \nMZFONDS     -1.445e+01  9.359e+02  -0.015  0.98768    \nMZPART      -1.451e+01  9.359e+02  -0.016  0.98763    \nMINKM30      1.181e-01  1.006e-01   1.174  0.24039    \nMINK3045     1.366e-01  9.650e-02   1.415  0.15694    \nMINK4575     1.009e-01  9.667e-02   1.043  0.29678    \nMINK7512     1.144e-01  1.027e-01   1.114  0.26513    \nMINK123M    -1.607e-01  1.449e-01  -1.109  0.26738    \nMINKGEM      9.214e-02  9.945e-02   0.927  0.35417    \nMKOOPKLA     6.856e-02  4.642e-02   1.477  0.13966    \nPWAPART      5.954e-01  3.901e-01   1.526  0.12693    \nPWABEDR     -2.757e-01  4.635e-01  -0.595  0.55196    \nPWALAND     -4.405e-01  1.035e+00  -0.425  0.67052    \nPPERSAUT     2.306e-01  4.199e-02   5.491 4.01e-08 ***\nPBESAUT      1.215e+01  4.029e+02   0.030  0.97595    \nPMOTSCO     -8.101e-02  1.147e-01  -0.706  0.48006    \nPVRAAUT     -2.106e+00  2.557e+03  -0.001  0.99934    \nPAANHANG     1.014e+00  9.371e-01   1.082  0.27917    \nPTRACTOR     7.229e-01  4.278e-01   1.690  0.09107 .  \nPWERKT      -5.525e+00  4.805e+03  -0.001  0.99908    \nPBROM        2.170e-01  4.865e-01   0.446  0.65559    \nPLEVEN      -2.382e-01  1.170e-01  -2.036  0.04173 *  \nPPERSONG    -4.523e-01  2.094e+00  -0.216  0.82901    \nPGEZONG      1.444e+00  1.029e+00   1.404  0.16033    \nPWAOREG      8.239e-01  5.943e-01   1.386  0.16565    \nPBRAND       2.401e-01  7.714e-02   3.113  0.00185 ** \nPZEILPL     -8.658e+00  3.261e+03  -0.003  0.99788    \nPPLEZIER    -1.886e-01  3.259e-01  -0.579  0.56289    \nPFIETS       3.664e-01  8.325e-01   0.440  0.65985    \nPINBOED     -1.068e+00  8.764e-01  -1.219  0.22301    \nPBYSTAND    -1.676e-01  3.321e-01  -0.505  0.61373    \nAWAPART     -9.293e-01  7.802e-01  -1.191  0.23364    \nAWABEDR      4.197e-01  1.082e+00   0.388  0.69824    \nAWALAND      2.762e-01  3.528e+00   0.078  0.93758    \nAPERSAUT    -3.902e-02  1.772e-01  -0.220  0.82566    \nABESAUT     -7.298e+01  2.417e+03  -0.030  0.97591    \nAMOTSCO      2.418e-01  3.772e-01   0.641  0.52142    \nAVRAAUT     -4.490e+00  1.078e+04   0.000  0.99967    \nAAANHANG    -1.351e+00  1.687e+00  -0.801  0.42322    \nATRACTOR    -2.376e+00  1.524e+00  -1.559  0.11899    \nAWERKT      -8.749e-01  9.682e+03   0.000  0.99993    \nABROM       -1.060e+00  1.549e+00  -0.684  0.49367    \nALEVEN       4.789e-01  2.245e-01   2.133  0.03291 *  \nAPERSONG     3.997e-01  4.329e+00   0.092  0.92644    \nAGEZONG     -3.163e+00  2.706e+00  -1.169  0.24247    \nAWAOREG     -3.212e+00  3.433e+00  -0.936  0.34939    \nABRAND      -4.118e-01  2.787e-01  -1.477  0.13956    \nAZEILPL      1.047e+01  3.261e+03   0.003  0.99744    \nAPLEZIER     2.516e+00  1.010e+00   2.490  0.01276 *  \nAFIETS       2.318e-01  5.699e-01   0.407  0.68420    \nAINBOED      1.947e+00  1.412e+00   1.378  0.16812    \nABYSTAND     1.078e+00  1.103e+00   0.977  0.32870    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2635.5  on 5821  degrees of freedom\nResidual deviance: 2243.5  on 5736  degrees of freedom\nAIC: 2415.5\n\nNumber of Fisher Scoring iterations: 17\n```\n:::\n:::\n\n\non a ici un modèle avec beaucoup de variable. Mais si on analyse le summary, on constate que seulement 6 varaibales sont significative.\n\nRegardons un peu le VIF pour toutes les variables.\n\n::: {.cell}\n\n```{.r .cell-code}\nvif_values <- vif(mod1)\n\n# Transformer en data frame pour ggplot\nvif_df <- data.frame(\n  Variable = names(vif_values),\n  VIF = vif_values\n)\nggplot(vif_df, aes(x = reorder(Variable, VIF), y = pmin(VIF, 15), fill = VIF > 10)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = ifelse(VIF > 15, round(VIF, 1), \"\")),  \n            hjust = -0.2, size = 5) +  # Affiche seulement si VIF > 15\n  coord_flip() +  \n  scale_fill_manual(values = c(\"FALSE\" = \"#0072B2\", \"TRUE\" = \"#D55E00\")) +\n  labs(title = \"Variance Inflation Factor (VIF)\",\n       x = \"Variables\",\n       y = \"VIF (limité à 15)\") +\n  theme_minimal() +\n  theme(axis.text = element_text(size = 12),\n        axis.title = element_text(size = 14, face = \"bold\"),\n        plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-13-1.png){width=2688}\n:::\n:::\n\nOn constate la présence de beaucoup de valeurs avec un VIF très élevé et donc une forte colinéarité.\n\n\n## Sélecion automatique \n\n::: panel-tabset\n\n### AIC\n\n::: panel-tabset\n\n#### Forward\n\n#### Backward\n\n#### Both\n\n\n:::\n\n### BIC\n\n::: panel-tabset\n\n#### Forward\n\n#### Backward\n\n#### Both\n\n\n:::\n\n:::\n\n\nAprès toute ces modélisations, rappelons nous tout de même l'objectif de l’assureur est de démarcher des clients de manière ciblée pour\nleurs faire souscrire une assurance caravane. \nOn pourrait alors de demander : s’il démarchait les clients de façon aléatoire, sans tenir compte de leurs caractéristiques, quel serait environ son taux de réussite ?\n\nPour cela il suffit juste de ce rappeler du pourcentage donné précédemment qui nous disait la proportion de oui actuellement.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(table(Caravan$Purchase)*100/nrow(Caravan), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    No    Yes \n94.023  5.977 \n```\n:::\n:::\n\n\nLe pourcentage étant très bas, on va souhaiter utiliser l’un des 3 modèles estimés ci-dessus (le global, un de ceux sélectionnés par AIC et un de ceux sélectionnés par BIC) pour cibler les clients à démarcher.\n\nAinsi on regardera \n\nSi l’on choisissait de démarcher tous les clients ayant une probabilité de souscrire l’assurance supérieure à 0.5, quel pourcentage de clients cela représenterait il pour chacun des 3 modèles estimés ? Quel seuil faudrait-il choisir à la place de 0.5 pour que ce pourcentage corresponde à environ 6% des clients ? On décide dans la suite de fixer ce seuil à 0.2 et on cherche à sélectionner le meilleur modèle parmi les 3 précédents.\n\n\n# Conclusion\n\n\n\n\n# Session info\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(pkgs = \"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.1 (2022-06-23 ucrt)\n os       Windows 10 x64 (build 22631)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  French_France.utf8\n ctype    French_France.utf8\n tz       Europe/Paris\n date     2025-02-19\n pandoc   3.2 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package      * version date (UTC) lib source\n car          * 3.1-2   2023-03-30 [1] CRAN (R 4.2.3)\n carData      * 3.0-5   2022-01-06 [1] CRAN (R 4.2.1)\n dplyr        * 1.1.4   2023-11-17 [1] CRAN (R 4.2.3)\n factoextra   * 1.0.7   2020-04-01 [1] CRAN (R 4.2.1)\n FactoMineR   * 2.9     2023-10-12 [1] CRAN (R 4.2.3)\n ggplot2      * 3.5.1   2024-04-23 [1] CRAN (R 4.2.3)\n gridExtra    * 2.3     2017-09-09 [1] CRAN (R 4.2.1)\n ISLR         * 1.4     2021-09-15 [1] CRAN (R 4.2.3)\n psych        * 2.4.1   2024-01-18 [1] CRAN (R 4.2.3)\n RColorBrewer * 1.1-3   2022-04-03 [1] CRAN (R 4.2.0)\n reshape2     * 1.4.4   2020-04-09 [1] CRAN (R 4.2.1)\n\n [1] C:/Users/cleme/AppData/Local/R/win-library/4.2\n [2] C:/Program Files/R/R-4.2.1/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncar <- Caravan\n# Question 3 --------------------------------------------------------------\n\nmod_full = glm(car$Purchase~., family = binomial, car)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n```\n:::\n\n```{.r .cell-code}\nsummary(mod_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = car$Purchase ~ ., family = binomial, data = car)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7047  -0.3711  -0.2450  -0.1588   3.2916  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.542e+02  1.116e+04   0.023  0.98183    \nMOSTYPE      6.580e-02  4.624e-02   1.423  0.15468    \nMAANTHUI    -1.832e-01  1.927e-01  -0.951  0.34157    \nMGEMOMV     -2.696e-02  1.399e-01  -0.193  0.84723    \nMGEMLEEF     2.096e-01  1.016e-01   2.063  0.03911 *  \nMOSHOOFD    -2.767e-01  2.076e-01  -1.333  0.18247    \nMGODRK      -1.142e-01  1.069e-01  -1.068  0.28535    \nMGODPR      -1.910e-02  1.177e-01  -0.162  0.87112    \nMGODOV      -1.618e-02  1.055e-01  -0.153  0.87818    \nMGODGE      -6.817e-02  1.113e-01  -0.612  0.54024    \nMRELGE       2.310e-01  1.566e-01   1.475  0.14031    \nMRELSA       8.509e-02  1.466e-01   0.580  0.56169    \nMRELOV       1.467e-01  1.562e-01   0.939  0.34759    \nMFALLEEN    -8.291e-02  1.311e-01  -0.633  0.52702    \nMFGEKIND    -1.154e-01  1.337e-01  -0.863  0.38813    \nMFWEKIND    -8.140e-02  1.417e-01  -0.575  0.56561    \nMOPLHOOG     9.717e-04  1.311e-01   0.007  0.99408    \nMOPLMIDD    -9.077e-02  1.365e-01  -0.665  0.50605    \nMOPLLAAG    -1.994e-01  1.376e-01  -1.449  0.14740    \nMBERHOOG     8.883e-02  9.349e-02   0.950  0.34204    \nMBERZELF     3.918e-02  9.897e-02   0.396  0.69219    \nMBERBOER    -1.169e-01  1.104e-01  -1.059  0.28951    \nMBERMIDD     1.353e-01  9.191e-02   1.472  0.14106    \nMBERARBG     3.976e-02  9.067e-02   0.438  0.66104    \nMBERARBO     9.954e-02  9.143e-02   1.089  0.27628    \nMSKA         2.690e-02  1.035e-01   0.260  0.79502    \nMSKB1       -8.801e-03  1.011e-01  -0.087  0.93064    \nMSKB2        1.200e-02  9.081e-02   0.132  0.89485    \nMSKC         9.016e-02  9.958e-02   0.905  0.36527    \nMSKD        -2.468e-02  9.724e-02  -0.254  0.79967    \nMHHUUR      -1.472e+01  8.140e+02  -0.018  0.98557    \nMHKOOP      -1.469e+01  8.140e+02  -0.018  0.98561    \nMAUT1        1.819e-01  1.514e-01   1.202  0.22953    \nMAUT2        1.507e-01  1.371e-01   1.099  0.27162    \nMAUT0        9.325e-02  1.436e-01   0.649  0.51603    \nMZFONDS     -1.445e+01  9.359e+02  -0.015  0.98768    \nMZPART      -1.451e+01  9.359e+02  -0.016  0.98763    \nMINKM30      1.181e-01  1.006e-01   1.174  0.24039    \nMINK3045     1.366e-01  9.650e-02   1.415  0.15694    \nMINK4575     1.009e-01  9.667e-02   1.043  0.29678    \nMINK7512     1.144e-01  1.027e-01   1.114  0.26513    \nMINK123M    -1.607e-01  1.449e-01  -1.109  0.26738    \nMINKGEM      9.214e-02  9.945e-02   0.927  0.35417    \nMKOOPKLA     6.856e-02  4.642e-02   1.477  0.13966    \nPWAPART      5.954e-01  3.901e-01   1.526  0.12693    \nPWABEDR     -2.757e-01  4.635e-01  -0.595  0.55196    \nPWALAND     -4.405e-01  1.035e+00  -0.425  0.67052    \nPPERSAUT     2.306e-01  4.199e-02   5.491 4.01e-08 ***\nPBESAUT      1.215e+01  4.029e+02   0.030  0.97595    \nPMOTSCO     -8.101e-02  1.147e-01  -0.706  0.48006    \nPVRAAUT     -2.106e+00  2.557e+03  -0.001  0.99934    \nPAANHANG     1.014e+00  9.371e-01   1.082  0.27917    \nPTRACTOR     7.229e-01  4.278e-01   1.690  0.09107 .  \nPWERKT      -5.525e+00  4.805e+03  -0.001  0.99908    \nPBROM        2.170e-01  4.865e-01   0.446  0.65559    \nPLEVEN      -2.382e-01  1.170e-01  -2.036  0.04173 *  \nPPERSONG    -4.523e-01  2.094e+00  -0.216  0.82901    \nPGEZONG      1.444e+00  1.029e+00   1.404  0.16033    \nPWAOREG      8.239e-01  5.943e-01   1.386  0.16565    \nPBRAND       2.401e-01  7.714e-02   3.113  0.00185 ** \nPZEILPL     -8.658e+00  3.261e+03  -0.003  0.99788    \nPPLEZIER    -1.886e-01  3.259e-01  -0.579  0.56289    \nPFIETS       3.664e-01  8.325e-01   0.440  0.65985    \nPINBOED     -1.068e+00  8.764e-01  -1.219  0.22301    \nPBYSTAND    -1.676e-01  3.321e-01  -0.505  0.61373    \nAWAPART     -9.293e-01  7.802e-01  -1.191  0.23364    \nAWABEDR      4.197e-01  1.082e+00   0.388  0.69824    \nAWALAND      2.762e-01  3.528e+00   0.078  0.93758    \nAPERSAUT    -3.902e-02  1.772e-01  -0.220  0.82566    \nABESAUT     -7.298e+01  2.417e+03  -0.030  0.97591    \nAMOTSCO      2.418e-01  3.772e-01   0.641  0.52142    \nAVRAAUT     -4.490e+00  1.078e+04   0.000  0.99967    \nAAANHANG    -1.351e+00  1.687e+00  -0.801  0.42322    \nATRACTOR    -2.376e+00  1.524e+00  -1.559  0.11899    \nAWERKT      -8.749e-01  9.682e+03   0.000  0.99993    \nABROM       -1.060e+00  1.549e+00  -0.684  0.49367    \nALEVEN       4.789e-01  2.245e-01   2.133  0.03291 *  \nAPERSONG     3.997e-01  4.329e+00   0.092  0.92644    \nAGEZONG     -3.163e+00  2.706e+00  -1.169  0.24247    \nAWAOREG     -3.212e+00  3.433e+00  -0.936  0.34939    \nABRAND      -4.118e-01  2.787e-01  -1.477  0.13956    \nAZEILPL      1.047e+01  3.261e+03   0.003  0.99744    \nAPLEZIER     2.516e+00  1.010e+00   2.490  0.01276 *  \nAFIETS       2.318e-01  5.699e-01   0.407  0.68420    \nAINBOED      1.947e+00  1.412e+00   1.378  0.16812    \nABYSTAND     1.078e+00  1.103e+00   0.977  0.32870    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2635.5  on 5821  degrees of freedom\nResidual deviance: 2243.5  on 5736  degrees of freedom\nAIC: 2415.5\n\nNumber of Fisher Scoring iterations: 17\n```\n:::\n\n```{.r .cell-code}\nlibrary(car)\nvif(mod_full) # quelques de vif elevé \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     MOSTYPE     MAANTHUI      MGEMOMV     MGEMLEEF     MOSHOOFD       MGODRK \n1.176524e+02 1.129402e+00 3.502955e+00 1.843805e+00 1.159758e+02 3.254979e+00 \n      MGODPR       MGODOV       MGODGE       MRELGE       MRELSA       MRELOV \n1.149989e+01 3.403490e+00 9.654767e+00 2.092478e+01 4.738546e+00 1.755829e+01 \n    MFALLEEN     MFGEKIND     MFWEKIND     MOPLHOOG     MOPLMIDD     MOPLLAAG \n1.396957e+01 1.484774e+01 2.401911e+01 1.697423e+01 1.707883e+01 2.881042e+01 \n    MBERHOOG     MBERZELF     MBERBOER     MBERMIDD     MBERARBG     MBERARBO \n1.005928e+01 2.005447e+00 2.032453e+00 9.340155e+00 6.998429e+00 6.263887e+00 \n        MSKA        MSKB1        MSKB2         MSKC         MSKD       MHHUUR \n1.143795e+01 5.439487e+00 5.554208e+00 1.173787e+01 3.231964e+00 1.742646e+09 \n      MHKOOP        MAUT1        MAUT2        MAUT0      MZFONDS       MZPART \n1.742646e+09 1.434527e+01 8.048254e+00 1.242104e+01 1.102939e+09 1.102939e+09 \n     MINKM30     MINK3045     MINK4575     MINK7512     MINK123M      MINKGEM \n1.022040e+01 1.002438e+01 9.896903e+00 5.441997e+00 1.649801e+00 5.338352e+00 \n    MKOOPKLA      PWAPART      PWABEDR      PWALAND     PPERSAUT      PBESAUT \n2.649650e+00 4.429193e+01 6.632572e+00 3.321309e+01 3.113930e+00 9.398932e+06 \n     PMOTSCO      PVRAAUT     PAANHANG     PTRACTOR       PWERKT        PBROM \n3.450522e+00 5.470096e+01 1.397225e+01 1.333457e+01 1.707055e+02 1.778348e+01 \n      PLEVEN     PPERSONG      PGEZONG      PWAOREG       PBRAND      PZEILPL \n4.287040e+00 1.688445e+01 2.897539e+01 3.110765e+01 6.246956e+00 2.891237e+06 \n    PPLEZIER       PFIETS      PINBOED     PBYSTAND      AWAPART      AWABEDR \n6.860219e+00 7.914642e+00 6.966391e+00 1.333206e+01 4.493341e+01 5.984836e+00 \n     AWALAND     APERSAUT      ABESAUT      AMOTSCO      AVRAAUT     AAANHANG \n3.329385e+01 3.236914e+00 9.398929e+06 3.397310e+00 5.470096e+01 1.391263e+01 \n    ATRACTOR       AWERKT        ABROM       ALEVEN     APERSONG      AGEZONG \n1.327892e+01 1.707055e+02 1.775902e+01 4.383663e+00 1.691406e+01 2.912335e+01 \n     AWAOREG       ABRAND      AZEILPL     APLEZIER       AFIETS      AINBOED \n3.115261e+01 5.907867e+00 2.891237e+06 6.611109e+00 7.924550e+00 6.980250e+00 \n    ABYSTAND \n1.320138e+01 \n```\n:::\n\n```{.r .cell-code}\n## courbe ROC\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'pROC' a été compilé avec la version R 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType 'citation(\"pROC\")' for a citation.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttachement du package : 'pROC'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLes objets suivants sont masqués depuis 'package:stats':\n\n    cov, smooth, var\n```\n:::\n\n```{.r .cell-code}\nlibrary(PresenceAbsence) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: le package 'PresenceAbsence' a été compilé avec la version R 4.2.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttachement du package : 'PresenceAbsence'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nL'objet suivant est masqué depuis 'package:pROC':\n\n    auc\n```\n:::\n\n```{.r .cell-code}\ndf_rocr_mod = matrix(0, nrow=nrow(as.matrix(car$Purchase)), ncol = 3)\ndf_rocr_mod[,1] = 1:nrow(as.matrix(car$Purchase))\ndf_rocr_mod[,2] = as.numeric(mod_full$y)\ndf_rocr_mod[,3] = mod_full$fitted\ndf_rocr_mod = as.data.frame(df_rocr_mod)\ndimnames(df_rocr_mod)[[2]] = c('ID', \"Observed\", \"Predicted\")\ndimnames(df_rocr_mod)[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ID\"        \"Observed\"  \"Predicted\"\n```\n:::\n\n```{.r .cell-code}\n# matrice de confusion \ncmx(df_rocr_mod, threshold = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         observed\npredicted    1    0\n        1    7    8\n        0  341 5466\n```\n:::\n\n```{.r .cell-code}\n# Calcul de la specificite et de la sensibilite\nsensitivity(cmx(df_rocr_mod,threshold=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  sensitivity sensitivity.sd\n1  0.02011494    0.007536717\n```\n:::\n\n```{.r .cell-code}\nspecificity(cmx(df_rocr_mod,threshold=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  specificity specificity.sd\n1   0.9985385   0.0005163715\n```\n:::\n\n```{.r .cell-code}\n# Courbe ROC pour le modele logistique CHD\nroc.plot.calculate(df_rocr_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    threshold        PCC sensitivity specificity       Kappa\n1        0.00 0.05977327 1.000000000   0.0000000 0.000000000\n2        0.01 0.19958777 0.994252874   0.1490683 0.019729331\n3        0.02 0.37117829 0.948275862   0.3344903 0.048112212\n4        0.03 0.49793885 0.882183908   0.4735111 0.073758972\n5        0.04 0.58381999 0.818965517   0.5688710 0.094814495\n6        0.05 0.64943318 0.767241379   0.6419437 0.115979442\n7        0.06 0.70611474 0.718390805   0.7053343 0.139459289\n8        0.07 0.74544830 0.678160920   0.7497260 0.158914265\n9        0.08 0.78461010 0.640804598   0.7937523 0.184853124\n10       0.09 0.80951563 0.609195402   0.8222506 0.202925152\n11       0.10 0.83081415 0.557471264   0.8481915 0.212292894\n12       0.11 0.84455514 0.508620690   0.8659116 0.213107179\n13       0.12 0.85915493 0.459770115   0.8845451 0.215550569\n14       0.13 0.87478530 0.428160920   0.9031787 0.229246669\n15       0.14 0.88302989 0.396551724   0.9139569 0.229808908\n16       0.15 0.89350739 0.382183908   0.9260139 0.245451984\n17       0.16 0.90243902 0.367816092   0.9364267 0.259544938\n18       0.17 0.91016833 0.341954023   0.9462916 0.265064708\n19       0.18 0.91429062 0.293103448   0.9537815 0.244581472\n20       0.19 0.91738234 0.267241379   0.9587139 0.235129532\n21       0.20 0.92133288 0.250000000   0.9640117 0.234176183\n22       0.21 0.92476812 0.229885057   0.9689441 0.229027112\n23       0.22 0.92751632 0.209770115   0.9731458 0.220970671\n24       0.23 0.92923394 0.181034483   0.9767994 0.200444267\n25       0.24 0.93026451 0.155172414   0.9795396 0.178395897\n26       0.25 0.93198214 0.146551724   0.9819145 0.175116889\n27       0.26 0.93318447 0.143678161   0.9833760 0.176098707\n28       0.27 0.93421505 0.126436782   0.9855681 0.160632157\n29       0.28 0.93507386 0.120689655   0.9868469 0.156949456\n30       0.29 0.93490210 0.094827586   0.9883084 0.125527191\n31       0.30 0.93644796 0.091954023   0.9901352 0.126779770\n32       0.31 0.93576091 0.077586207   0.9903179 0.106196042\n33       0.32 0.93541738 0.063218391   0.9908659 0.086031310\n34       0.33 0.93610443 0.060344828   0.9917793 0.083991859\n35       0.34 0.93661972 0.048850575   0.9930581 0.069180662\n36       0.35 0.93713501 0.045977011   0.9937888 0.066380530\n37       0.36 0.93730677 0.040229885   0.9943369 0.058357107\n38       0.37 0.93799382 0.040229885   0.9950676 0.060137142\n39       0.38 0.93782205 0.037356322   0.9950676 0.055368303\n40       0.39 0.93799382 0.031609195   0.9956156 0.047030424\n41       0.40 0.93850910 0.031609195   0.9961637 0.048314040\n42       0.41 0.93868087 0.028735632   0.9965290 0.044261534\n43       0.42 0.93868087 0.028735632   0.9965290 0.044261534\n44       0.43 0.93919615 0.028735632   0.9970771 0.045543714\n45       0.44 0.93936791 0.028735632   0.9972598 0.045975176\n46       0.45 0.93953968 0.028735632   0.9974425 0.046408696\n47       0.46 0.93971144 0.028735632   0.9976251 0.046844289\n48       0.47 0.93953968 0.025862069   0.9976251 0.041837866\n49       0.48 0.93953968 0.022988506   0.9978078 0.037223007\n50       0.49 0.93988320 0.020114943   0.9983559 0.033382162\n51       0.50 0.94005496 0.020114943   0.9985385 0.033794496\n52       0.51 0.93988320 0.017241379   0.9985385 0.028658321\n53       0.52 0.93988320 0.014367816   0.9987212 0.023888084\n54       0.53 0.94005496 0.014367816   0.9989039 0.024281108\n55       0.54 0.94022673 0.011494253   0.9992693 0.019838766\n56       0.55 0.94022673 0.011494253   0.9992693 0.019838766\n57       0.56 0.94022673 0.011494253   0.9992693 0.019838766\n58       0.57 0.94022673 0.011494253   0.9992693 0.019838766\n59       0.58 0.94022673 0.011494253   0.9992693 0.019838766\n60       0.59 0.94039849 0.011494253   0.9994520 0.020225632\n61       0.60 0.94039849 0.011494253   0.9994520 0.020225632\n62       0.61 0.94039849 0.011494253   0.9994520 0.020225632\n63       0.62 0.94039849 0.011494253   0.9994520 0.020225632\n64       0.63 0.94039849 0.011494253   0.9994520 0.020225632\n65       0.64 0.94039849 0.011494253   0.9994520 0.020225632\n66       0.65 0.94057025 0.011494253   0.9996346 0.020614427\n67       0.66 0.94039849 0.008620690   0.9996346 0.015329831\n68       0.67 0.94022673 0.005747126   0.9996346 0.010018763\n69       0.68 0.94022673 0.005747126   0.9996346 0.010018763\n70       0.69 0.94022673 0.005747126   0.9996346 0.010018763\n71       0.70 0.94022673 0.005747126   0.9996346 0.010018763\n72       0.71 0.94022673 0.005747126   0.9996346 0.010018763\n73       0.72 0.94022673 0.005747126   0.9996346 0.010018763\n74       0.73 0.94022673 0.005747126   0.9996346 0.010018763\n75       0.74 0.94022673 0.005747126   0.9996346 0.010018763\n76       0.75 0.94022673 0.005747126   0.9996346 0.010018763\n77       0.76 0.94039849 0.005747126   0.9998173 0.010384857\n78       0.77 0.94057025 0.005747126   1.0000000 0.010752794\n79       0.78 0.94057025 0.005747126   1.0000000 0.010752794\n80       0.79 0.94057025 0.005747126   1.0000000 0.010752794\n81       0.80 0.94057025 0.005747126   1.0000000 0.010752794\n82       0.81 0.94057025 0.005747126   1.0000000 0.010752794\n83       0.82 0.94057025 0.005747126   1.0000000 0.010752794\n84       0.83 0.94057025 0.005747126   1.0000000 0.010752794\n85       0.84 0.94039849 0.002873563   1.0000000 0.005389965\n86       0.85 0.94039849 0.002873563   1.0000000 0.005389965\n87       0.86 0.94039849 0.002873563   1.0000000 0.005389965\n88       0.87 0.94039849 0.002873563   1.0000000 0.005389965\n89       0.88 0.94039849 0.002873563   1.0000000 0.005389965\n90       0.89 0.94039849 0.002873563   1.0000000 0.005389965\n91       0.90 0.94039849 0.002873563   1.0000000 0.005389965\n92       0.91 0.94039849 0.002873563   1.0000000 0.005389965\n93       0.92 0.94039849 0.002873563   1.0000000 0.005389965\n94       0.93 0.94039849 0.002873563   1.0000000 0.005389965\n95       0.94 0.94039849 0.002873563   1.0000000 0.005389965\n96       0.95 0.94039849 0.002873563   1.0000000 0.005389965\n97       0.96 0.94039849 0.002873563   1.0000000 0.005389965\n98       0.97 0.94022673 0.000000000   1.0000000 0.000000000\n99       0.98 0.94022673 0.000000000   1.0000000 0.000000000\n100      0.99 0.94022673 0.000000000   1.0000000 0.000000000\n101      1.00 0.94022673 0.000000000   1.0000000 0.000000000\n```\n:::\n\n```{.r .cell-code}\nauc.roc.plot(df_rocr_mod) # graphe courbe ROC\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nauc(df_rocr_mod) # calcul AUC \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        AUC     AUC.sd\n1 0.7902737 0.01208314\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Question 4 --------------------------------------------------------------\n\nmod_start = glm(car$Purchase~1, family = binomial, car)\n\n# AIC k = 2 \nmod_both_AIC = step(mod_start, scope = formula(mod_full), trace = FALSE, direction = \"both\", k=2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n```\n:::\n\n```{.r .cell-code}\n# BIC k = log(n)\nn = dim(car)[1]\nmod_both_BIC = step(mod_start, scope = formula(mod_full), trace = FALSE, direction = \"both\", k = log(n))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n```\n:::\n\n```{.r .cell-code}\nsummary(mod_both_AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = car$Purchase ~ PPERSAUT + MKOOPKLA + PBRAND + APLEZIER + \n    MOPLLAAG + MBERBOER + MRELGE + PWALAND + AFIETS + MINK123M + \n    MINKGEM + PWAOREG + MGEMLEEF + PWAPART + ABYSTAND + ABRAND + \n    AWERKT + MGODPR + AWAOREG + MSKC + MOPLHOOG + MBERMIDD, family = binomial, \n    data = car)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.8826  -0.3744  -0.2565  -0.1725   3.1647  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -6.19224    0.52494 -11.796  < 2e-16 ***\nPPERSAUT      0.22355    0.02431   9.196  < 2e-16 ***\nMKOOPKLA      0.05593    0.03559   1.572 0.116048    \nPBRAND        0.24533    0.07137   3.437 0.000587 ***\nAPLEZIER      2.04258    0.38299   5.333 9.65e-08 ***\nMOPLLAAG     -0.09275    0.04084  -2.271 0.023141 *  \nMBERBOER     -0.16145    0.08213  -1.966 0.049312 *  \nMRELGE        0.08609    0.03637   2.367 0.017941 *  \nPWALAND      -0.35244    0.18342  -1.922 0.054667 .  \nAFIETS        0.49134    0.20112   2.443 0.014565 *  \nMINK123M     -0.26512    0.12196  -2.174 0.029718 *  \nMINKGEM       0.09974    0.05618   1.775 0.075849 .  \nPWAOREG       0.90496    0.58036   1.559 0.118920    \nMGEMLEEF      0.16131    0.07647   2.109 0.034907 *  \nPWAPART       0.15607    0.07597   2.054 0.039943 *  \nABYSTAND      0.51565    0.30400   1.696 0.089846 .  \nABRAND       -0.45738    0.26499  -1.726 0.084345 .  \nAWERKT      -12.56588  306.81856  -0.041 0.967331    \nMGODPR        0.05639    0.03474   1.623 0.104613    \nAWAOREG      -3.63288    3.31767  -1.095 0.273512    \nMSKC          0.08675    0.04353   1.993 0.046261 *  \nMOPLHOOG      0.09673    0.04644   2.083 0.037263 *  \nMBERMIDD      0.06828    0.03260   2.094 0.036234 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2635.5  on 5821  degrees of freedom\nResidual deviance: 2295.4  on 5799  degrees of freedom\nAIC: 2341.4\n\nNumber of Fisher Scoring iterations: 15\n```\n:::\n\n```{.r .cell-code}\nsummary(mod_both_BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = car$Purchase ~ PPERSAUT + PBRAND + APLEZIER + MOPLLAAG + \n    MBERBOER + MRELGE, family = binomial, data = car)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5324  -0.3869  -0.2681  -0.1813   3.1846  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.25590    0.28160 -15.113  < 2e-16 ***\nPPERSAUT     0.23346    0.02360   9.894  < 2e-16 ***\nPBRAND       0.17672    0.03042   5.810 6.26e-09 ***\nAPLEZIER     2.00086    0.36984   5.410 6.30e-08 ***\nMOPLLAAG    -0.13801    0.02539  -5.436 5.46e-08 ***\nMBERBOER    -0.26479    0.07468  -3.546 0.000392 ***\nMRELGE       0.13598    0.03256   4.176 2.97e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2635.5  on 5821  degrees of freedom\nResidual deviance: 2353.0  on 5815  degrees of freedom\nAIC: 2367\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n\n```{.r .cell-code}\nvif(mod_both_AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n PPERSAUT  MKOOPKLA    PBRAND  APLEZIER  MOPLLAAG  MBERBOER    MRELGE   PWALAND \n 1.061129  1.612138  5.438441  1.008809  2.642005  1.202952  1.227286  1.096876 \n   AFIETS  MINK123M   MINKGEM   PWAOREG  MGEMLEEF   PWAPART  ABYSTAND    ABRAND \n 1.009692  1.211645  1.793174 28.081012  1.097419  1.687378  1.021142  5.460960 \n   AWERKT    MGODPR   AWAOREG      MSKC  MOPLHOOG  MBERMIDD \n 1.000002  1.075606 28.057222  2.309295  2.212204  1.235918 \n```\n:::\n\n```{.r .cell-code}\nvif(mod_both_BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPPERSAUT   PBRAND APLEZIER MOPLLAAG MBERBOER   MRELGE \n1.009196 1.026969 1.001332 1.039257 1.041977 1.018104 \n```\n:::\n\n```{.r .cell-code}\n# on a encore un peu de colinearité pour des var du modèle selec avec AIC\n# plus de colinearité pour le model selec avec BIC\n\n# Il serait preferable de garder le mod_both_BIC\n#mais on peut aussi enlever les variables au vif elevé \n\nmod_both_AIC_2.0 = glm(formula = car$Purchase ~ PPERSAUT + MKOOPKLA + PBRAND + APLEZIER + \n                         MOPLLAAG + MBERBOER + MRELGE + PWALAND + AFIETS + MINK123M + \n                         MINKGEM + MGEMLEEF + PWAPART + ABYSTAND + ABRAND + \n                         AWERKT + MGODPR + MSKC + MOPLHOOG + MBERMIDD, family = binomial, \n                       data = car)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n```\n:::\n\n```{.r .cell-code}\nsummary(mod_both_AIC_2.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = car$Purchase ~ PPERSAUT + MKOOPKLA + PBRAND + APLEZIER + \n    MOPLLAAG + MBERBOER + MRELGE + PWALAND + AFIETS + MINK123M + \n    MINKGEM + MGEMLEEF + PWAPART + ABYSTAND + ABRAND + AWERKT + \n    MGODPR + MSKC + MOPLHOOG + MBERMIDD, family = binomial, data = car)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5608  -0.3748  -0.2578  -0.1733   3.1609  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -6.13191    0.52316 -11.721  < 2e-16 ***\nPPERSAUT      0.22142    0.02424   9.134  < 2e-16 ***\nMKOOPKLA      0.05561    0.03553   1.565 0.117578    \nPBRAND        0.25594    0.07078   3.616 0.000299 ***\nAPLEZIER      2.03348    0.38274   5.313 1.08e-07 ***\nMOPLLAAG     -0.09327    0.04093  -2.279 0.022689 *  \nMBERBOER     -0.16619    0.08210  -2.024 0.042939 *  \nMRELGE        0.08763    0.03631   2.413 0.015803 *  \nPWALAND      -0.35071    0.18353  -1.911 0.056017 .  \nAFIETS        0.48693    0.20098   2.423 0.015402 *  \nMINK123M     -0.27387    0.12204  -2.244 0.024830 *  \nMINKGEM       0.09891    0.05605   1.765 0.077632 .  \nMGEMLEEF      0.15985    0.07635   2.094 0.036287 *  \nPWAPART       0.14818    0.07594   1.951 0.051032 .  \nABYSTAND      0.50720    0.30385   1.669 0.095074 .  \nABRAND       -0.47863    0.26420  -1.812 0.070048 .  \nAWERKT      -12.06366  312.17287  -0.039 0.969174    \nMGODPR        0.05282    0.03470   1.522 0.127952    \nMSKC          0.08254    0.04356   1.895 0.058073 .  \nMOPLHOOG      0.09360    0.04634   2.020 0.043391 *  \nMBERMIDD      0.06768    0.03255   2.079 0.037600 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2635.5  on 5821  degrees of freedom\nResidual deviance: 2301.9  on 5801  degrees of freedom\nAIC: 2343.9\n\nNumber of Fisher Scoring iterations: 15\n```\n:::\n\n```{.r .cell-code}\nvif(mod_both_AIC_2.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPPERSAUT MKOOPKLA   PBRAND APLEZIER MOPLLAAG MBERBOER   MRELGE  PWALAND \n1.058123 1.612705 5.387839 1.008466 2.656919 1.206316 1.227004 1.095281 \n  AFIETS MINK123M  MINKGEM MGEMLEEF  PWAPART ABYSTAND   ABRAND   AWERKT \n1.009595 1.209562 1.790558 1.096694 1.689480 1.020739 5.435687 1.000000 \n  MGODPR     MSKC MOPLHOOG MBERMIDD \n1.074299 2.317082 2.210706 1.235177 \n```\n:::\n\n```{.r .cell-code}\n# on analyse tout de même tout les modèle \n\n#####\n# df_rocr_mod_both_AIC\ndf_rocr_mod_both_AIC = matrix(0, nrow=nrow(as.matrix(car$Purchase)), ncol = 3)\ndf_rocr_mod_both_AIC[,1] = 1:nrow(as.matrix(car$Purchase))\ndf_rocr_mod_both_AIC[,2] = as.numeric(mod_both_AIC$y)\ndf_rocr_mod_both_AIC[,3] = mod_both_AIC$fitted\ndf_rocr_mod_both_AIC = as.data.frame(df_rocr_mod_both_AIC)\ndimnames(df_rocr_mod_both_AIC)[[2]] = c('ID', \"Observed\", \"Predicted\")\ndimnames(df_rocr_mod_both_AIC)[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ID\"        \"Observed\"  \"Predicted\"\n```\n:::\n\n```{.r .cell-code}\n# matrice de confusion \ncmx(df_rocr_mod_both_AIC, threshold = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         observed\npredicted    1    0\n        1    3   13\n        0  345 5461\n```\n:::\n\n```{.r .cell-code}\n# Calcul de la specificite et de la sensibilite\nsensitivity(cmx(df_rocr_mod_both_AIC,threshold=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  sensitivity sensitivity.sd\n1  0.00862069    0.004962793\n```\n:::\n\n```{.r .cell-code}\nspecificity(cmx(df_rocr_mod_both_AIC,threshold=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  specificity specificity.sd\n1   0.9976251    0.000657946\n```\n:::\n\n```{.r .cell-code}\n# Courbe ROC pour le modele logistique CHD\nroc.plot.calculate(df_rocr_mod_both_AIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    threshold        PCC sensitivity specificity       Kappa\n1        0.00 0.05977327 1.000000000   0.0000000 0.000000000\n2        0.01 0.14496737 0.991379310   0.0911582 0.010733751\n3        0.02 0.34111989 0.945402299   0.3027037 0.040606652\n4        0.03 0.46444521 0.876436782   0.4382536 0.061954525\n5        0.04 0.56338028 0.827586207   0.5465839 0.087861037\n6        0.05 0.63672278 0.775862069   0.6278772 0.111048014\n7        0.06 0.69958777 0.729885057   0.6976617 0.137907936\n8        0.07 0.74081072 0.683908046   0.7444282 0.156654525\n9        0.08 0.78392305 0.643678161   0.7928389 0.185051480\n10       0.09 0.80745448 0.580459770   0.8218853 0.190200758\n11       0.10 0.82892477 0.517241379   0.8487395 0.193846739\n12       0.11 0.84421161 0.465517241   0.8682864 0.194093262\n13       0.12 0.86241841 0.416666667   0.8907563 0.200739921\n14       0.13 0.87633116 0.387931034   0.9073803 0.211607862\n15       0.14 0.88509103 0.373563218   0.9176105 0.221679840\n16       0.15 0.89436620 0.344827586   0.9293022 0.225820973\n17       0.16 0.90243902 0.310344828   0.9400804 0.223914056\n18       0.17 0.90999656 0.287356322   0.9495798 0.228334268\n19       0.18 0.91463415 0.264367816   0.9559737 0.224879950\n20       0.19 0.91875644 0.238505747   0.9620022 0.217162129\n21       0.20 0.92339402 0.221264368   0.9680307 0.217371228\n22       0.21 0.92579869 0.192528736   0.9724151 0.199909401\n23       0.22 0.92837513 0.163793103   0.9769821 0.180944458\n24       0.23 0.93129509 0.143678161   0.9813665 0.169829831\n25       0.24 0.93249742 0.123563218   0.9839240 0.151809671\n26       0.25 0.93421505 0.112068966   0.9864815 0.144117324\n27       0.26 0.93507386 0.100574713   0.9881257 0.133117681\n28       0.27 0.93524562 0.080459770   0.9895871 0.108406729\n29       0.28 0.93644796 0.068965517   0.9915966 0.096749684\n30       0.29 0.93696324 0.063218391   0.9925100 0.090387630\n31       0.30 0.93679148 0.045977011   0.9934235 0.065476523\n32       0.31 0.93679148 0.045977011   0.9934235 0.065476523\n33       0.32 0.93679148 0.037356322   0.9939715 0.052769147\n34       0.33 0.93713501 0.037356322   0.9943369 0.053627643\n35       0.34 0.93747853 0.028735632   0.9952503 0.041339153\n36       0.35 0.93696324 0.020114943   0.9952503 0.026667104\n37       0.36 0.93696324 0.020114943   0.9952503 0.026667104\n38       0.37 0.93696324 0.020114943   0.9952503 0.026667104\n39       0.38 0.93765029 0.020114943   0.9959810 0.028198702\n40       0.39 0.93765029 0.020114943   0.9959810 0.028198702\n41       0.40 0.93747853 0.014367816   0.9961637 0.018581672\n42       0.41 0.93747853 0.011494253   0.9963464 0.013899901\n43       0.42 0.93765029 0.011494253   0.9965290 0.014257769\n44       0.43 0.93782205 0.011494253   0.9967117 0.014617351\n45       0.44 0.93799382 0.011494253   0.9968944 0.014978662\n46       0.45 0.93816558 0.011494253   0.9970771 0.015341712\n47       0.46 0.93816558 0.011494253   0.9970771 0.015341712\n48       0.47 0.93833734 0.011494253   0.9972598 0.015706515\n49       0.48 0.93850910 0.011494253   0.9974425 0.016073084\n50       0.49 0.93850910 0.011494253   0.9974425 0.016073084\n51       0.50 0.93850910 0.008620690   0.9976251 0.011288039\n52       0.51 0.93919615 0.008620690   0.9983559 0.012732519\n53       0.52 0.93936791 0.008620690   0.9985385 0.013098084\n54       0.53 0.93936791 0.008620690   0.9985385 0.013098084\n55       0.54 0.93919615 0.005747126   0.9985385 0.007860143\n56       0.55 0.93919615 0.005747126   0.9985385 0.007860143\n57       0.56 0.93936791 0.005747126   0.9987212 0.008215463\n58       0.57 0.93953968 0.005747126   0.9989039 0.008572545\n59       0.58 0.93953968 0.005747126   0.9989039 0.008572545\n60       0.59 0.93971144 0.005747126   0.9990866 0.008931403\n61       0.60 0.93988320 0.005747126   0.9992693 0.009292050\n62       0.61 0.93988320 0.005747126   0.9992693 0.009292050\n63       0.62 0.94005496 0.005747126   0.9994520 0.009654499\n64       0.63 0.94005496 0.005747126   0.9994520 0.009654499\n65       0.64 0.94005496 0.005747126   0.9994520 0.009654499\n66       0.65 0.94005496 0.005747126   0.9994520 0.009654499\n67       0.66 0.94005496 0.005747126   0.9994520 0.009654499\n68       0.67 0.94005496 0.005747126   0.9994520 0.009654499\n69       0.68 0.94022673 0.005747126   0.9996346 0.010018763\n70       0.69 0.94022673 0.005747126   0.9996346 0.010018763\n71       0.70 0.94022673 0.005747126   0.9996346 0.010018763\n72       0.71 0.94039849 0.005747126   0.9998173 0.010384857\n73       0.72 0.94022673 0.002873563   0.9998173 0.005034602\n74       0.73 0.94022673 0.002873563   0.9998173 0.005034602\n75       0.74 0.94022673 0.002873563   0.9998173 0.005034602\n76       0.75 0.94022673 0.002873563   0.9998173 0.005034602\n77       0.76 0.94022673 0.002873563   0.9998173 0.005034602\n78       0.77 0.94022673 0.002873563   0.9998173 0.005034602\n79       0.78 0.94022673 0.002873563   0.9998173 0.005034602\n80       0.79 0.94022673 0.002873563   0.9998173 0.005034602\n81       0.80 0.94022673 0.002873563   0.9998173 0.005034602\n82       0.81 0.94022673 0.002873563   0.9998173 0.005034602\n83       0.82 0.94022673 0.002873563   0.9998173 0.005034602\n84       0.83 0.94022673 0.002873563   0.9998173 0.005034602\n85       0.84 0.94039849 0.002873563   1.0000000 0.005389965\n86       0.85 0.94039849 0.002873563   1.0000000 0.005389965\n87       0.86 0.94039849 0.002873563   1.0000000 0.005389965\n88       0.87 0.94039849 0.002873563   1.0000000 0.005389965\n89       0.88 0.94039849 0.002873563   1.0000000 0.005389965\n90       0.89 0.94039849 0.002873563   1.0000000 0.005389965\n91       0.90 0.94039849 0.002873563   1.0000000 0.005389965\n92       0.91 0.94039849 0.002873563   1.0000000 0.005389965\n93       0.92 0.94039849 0.002873563   1.0000000 0.005389965\n94       0.93 0.94039849 0.002873563   1.0000000 0.005389965\n95       0.94 0.94039849 0.002873563   1.0000000 0.005389965\n96       0.95 0.94039849 0.002873563   1.0000000 0.005389965\n97       0.96 0.94022673 0.000000000   1.0000000 0.000000000\n98       0.97 0.94022673 0.000000000   1.0000000 0.000000000\n99       0.98 0.94022673 0.000000000   1.0000000 0.000000000\n100      0.99 0.94022673 0.000000000   1.0000000 0.000000000\n101      1.00 0.94022673 0.000000000   1.0000000 0.000000000\n```\n:::\n\n```{.r .cell-code}\n# df_rocr_mod_both_AIC_2.0\ndf_rocr_mod_both_AIC_2.0 = matrix(0, nrow=nrow(as.matrix(car$Purchase)), ncol = 3)\ndf_rocr_mod_both_AIC_2.0[,1] = 1:nrow(as.matrix(car$Purchase))\ndf_rocr_mod_both_AIC_2.0[,2] = as.numeric(mod_both_AIC_2.0$y)\ndf_rocr_mod_both_AIC_2.0[,3] = mod_both_AIC_2.0$fitted\ndf_rocr_mod_both_AIC_2.0 = as.data.frame(df_rocr_mod_both_AIC_2.0)\ndimnames(df_rocr_mod_both_AIC_2.0)[[2]] = c('ID', \"Observed\", \"Predicted\")\ndimnames(df_rocr_mod_both_AIC_2.0)[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ID\"        \"Observed\"  \"Predicted\"\n```\n:::\n\n```{.r .cell-code}\n# matrice de confusion \ncmx(df_rocr_mod_both_AIC_2.0, threshold = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         observed\npredicted    1    0\n        1    3   10\n        0  345 5464\n```\n:::\n\n```{.r .cell-code}\n# Calcul de la specificite et de la sensibilite\nsensitivity(cmx(df_rocr_mod_both_AIC_2.0,threshold=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  sensitivity sensitivity.sd\n1  0.00862069    0.004962793\n```\n:::\n\n```{.r .cell-code}\nspecificity(cmx(df_rocr_mod_both_AIC_2.0,threshold=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  specificity specificity.sd\n1   0.9981732   0.0005772153\n```\n:::\n\n```{.r .cell-code}\n# Courbe ROC pour le modele logistique CHD\nroc.plot.calculate(df_rocr_mod_both_AIC_2.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    threshold        PCC sensitivity specificity       Kappa\n1        0.00 0.05977327 1.000000000  0.00000000 0.000000000\n2        0.01 0.14256269 0.991379310  0.08860066 0.010375734\n3        0.02 0.33699760 0.945402299  0.29831933 0.039679377\n4        0.03 0.46238406 0.876436782  0.43606138 0.061328003\n5        0.04 0.56080385 0.824712644  0.54402631 0.086231565\n6        0.05 0.63637925 0.775862069  0.62751187 0.110865467\n7        0.06 0.69941601 0.724137931  0.69784436 0.136290531\n8        0.07 0.74098248 0.672413793  0.74534161 0.153464445\n9        0.08 0.78237719 0.629310345  0.79210815 0.178752751\n10       0.09 0.80522157 0.568965517  0.82024114 0.183406580\n11       0.10 0.82789419 0.511494253  0.84800877 0.190144650\n12       0.11 0.84180694 0.454022989  0.86645963 0.185476989\n13       0.12 0.86087255 0.410919540  0.88947753 0.195294306\n14       0.13 0.87495706 0.385057471  0.90610157 0.207432573\n15       0.14 0.88543456 0.370689655  0.91815857 0.220814092\n16       0.15 0.89333562 0.330459770  0.92911947 0.214786877\n17       0.16 0.90123669 0.298850575  0.93953233 0.213403336\n18       0.17 0.90896599 0.281609195  0.94884910 0.221517154\n19       0.18 0.91394710 0.261494253  0.95542565 0.220781531\n20       0.19 0.91824115 0.232758621  0.96181951 0.211047328\n21       0.20 0.92201992 0.215517241  0.96693460 0.208226193\n22       0.21 0.92493988 0.186781609  0.97186701 0.191966716\n23       0.22 0.92785984 0.152298851  0.97716478 0.167856623\n24       0.23 0.93060804 0.137931034  0.98100110 0.161528129\n25       0.24 0.93249742 0.112068966  0.98465473 0.138716548\n26       0.25 0.93404328 0.100574713  0.98702959 0.129898981\n27       0.26 0.93473033 0.091954023  0.98830837 0.121435084\n28       0.27 0.93558914 0.074712644  0.99031787 0.101916487\n29       0.28 0.93661972 0.068965517  0.99177932 0.097250552\n30       0.29 0.93730677 0.060344828  0.99305809 0.087378516\n31       0.30 0.93730677 0.045977011  0.99397150 0.066835585\n32       0.31 0.93730677 0.045977011  0.99397150 0.066835585\n33       0.32 0.93730677 0.037356322  0.99451955 0.054059829\n34       0.33 0.93747853 0.037356322  0.99470223 0.054493992\n35       0.34 0.93782205 0.028735632  0.99561564 0.042164393\n36       0.35 0.93747853 0.020114943  0.99579832 0.027813102\n37       0.36 0.93747853 0.020114943  0.99579832 0.027813102\n38       0.37 0.93747853 0.020114943  0.99579832 0.027813102\n39       0.38 0.93799382 0.020114943  0.99634636 0.028975383\n40       0.39 0.93799382 0.020114943  0.99634636 0.028975383\n41       0.40 0.93765029 0.014367816  0.99634636 0.018948977\n42       0.41 0.93782205 0.011494253  0.99671173 0.014617351\n43       0.42 0.93816558 0.011494253  0.99707709 0.015341712\n44       0.43 0.93816558 0.011494253  0.99707709 0.015341712\n45       0.44 0.93850910 0.011494253  0.99744246 0.016073084\n46       0.45 0.93850910 0.011494253  0.99744246 0.016073084\n47       0.46 0.93850910 0.011494253  0.99744246 0.016073084\n48       0.47 0.93868087 0.011494253  0.99762514 0.016441431\n49       0.48 0.93868087 0.011494253  0.99762514 0.016441431\n50       0.49 0.93868087 0.008620690  0.99780782 0.011646519\n51       0.50 0.93902439 0.008620690  0.99817318 0.012368746\n52       0.51 0.93936791 0.008620690  0.99853855 0.013098084\n53       0.52 0.93971144 0.008620690  0.99890391 0.013834639\n54       0.53 0.93971144 0.008620690  0.99890391 0.013834639\n55       0.54 0.93971144 0.008620690  0.99890391 0.013834639\n56       0.55 0.93953968 0.005747126  0.99890391 0.008572545\n57       0.56 0.93971144 0.005747126  0.99908659 0.008931403\n58       0.57 0.93971144 0.005747126  0.99908659 0.008931403\n59       0.58 0.93971144 0.005747126  0.99908659 0.008931403\n60       0.59 0.93988320 0.005747126  0.99926927 0.009292050\n61       0.60 0.94005496 0.005747126  0.99945195 0.009654499\n62       0.61 0.94005496 0.005747126  0.99945195 0.009654499\n63       0.62 0.94022673 0.005747126  0.99963464 0.010018763\n64       0.63 0.94022673 0.005747126  0.99963464 0.010018763\n65       0.64 0.94022673 0.005747126  0.99963464 0.010018763\n66       0.65 0.94022673 0.005747126  0.99963464 0.010018763\n67       0.66 0.94022673 0.005747126  0.99963464 0.010018763\n68       0.67 0.94022673 0.005747126  0.99963464 0.010018763\n69       0.68 0.94022673 0.005747126  0.99963464 0.010018763\n70       0.69 0.94039849 0.005747126  0.99981732 0.010384857\n71       0.70 0.94039849 0.005747126  0.99981732 0.010384857\n72       0.71 0.94039849 0.002873563  1.00000000 0.005389965\n73       0.72 0.94039849 0.002873563  1.00000000 0.005389965\n74       0.73 0.94039849 0.002873563  1.00000000 0.005389965\n75       0.74 0.94039849 0.002873563  1.00000000 0.005389965\n76       0.75 0.94039849 0.002873563  1.00000000 0.005389965\n77       0.76 0.94039849 0.002873563  1.00000000 0.005389965\n78       0.77 0.94039849 0.002873563  1.00000000 0.005389965\n79       0.78 0.94039849 0.002873563  1.00000000 0.005389965\n80       0.79 0.94039849 0.002873563  1.00000000 0.005389965\n81       0.80 0.94039849 0.002873563  1.00000000 0.005389965\n82       0.81 0.94039849 0.002873563  1.00000000 0.005389965\n83       0.82 0.94039849 0.002873563  1.00000000 0.005389965\n84       0.83 0.94039849 0.002873563  1.00000000 0.005389965\n85       0.84 0.94039849 0.002873563  1.00000000 0.005389965\n86       0.85 0.94039849 0.002873563  1.00000000 0.005389965\n87       0.86 0.94039849 0.002873563  1.00000000 0.005389965\n88       0.87 0.94039849 0.002873563  1.00000000 0.005389965\n89       0.88 0.94039849 0.002873563  1.00000000 0.005389965\n90       0.89 0.94039849 0.002873563  1.00000000 0.005389965\n91       0.90 0.94039849 0.002873563  1.00000000 0.005389965\n92       0.91 0.94039849 0.002873563  1.00000000 0.005389965\n93       0.92 0.94039849 0.002873563  1.00000000 0.005389965\n94       0.93 0.94039849 0.002873563  1.00000000 0.005389965\n95       0.94 0.94039849 0.002873563  1.00000000 0.005389965\n96       0.95 0.94039849 0.002873563  1.00000000 0.005389965\n97       0.96 0.94022673 0.000000000  1.00000000 0.000000000\n98       0.97 0.94022673 0.000000000  1.00000000 0.000000000\n99       0.98 0.94022673 0.000000000  1.00000000 0.000000000\n100      0.99 0.94022673 0.000000000  1.00000000 0.000000000\n101      1.00 0.94022673 0.000000000  1.00000000 0.000000000\n```\n:::\n\n```{.r .cell-code}\n# df_rocr_mod_both_BIC\ndf_rocr_mod_both_BIC = matrix(0, nrow=nrow(as.matrix(car$Purchase)), ncol = 3)\ndf_rocr_mod_both_BIC[,1] = 1:nrow(as.matrix(car$Purchase))\ndf_rocr_mod_both_BIC[,2] = as.numeric(mod_both_BIC$y)\ndf_rocr_mod_both_BIC[,3] = mod_both_BIC$fitted\ndf_rocr_mod_both_BIC = as.data.frame(df_rocr_mod_both_BIC)\ndimnames(df_rocr_mod_both_BIC)[[2]] = c('ID', \"Observed\", \"Predicted\")\ndimnames(df_rocr_mod_both_BIC)[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ID\"        \"Observed\"  \"Predicted\"\n```\n:::\n\n```{.r .cell-code}\n# matrice de confusion \ncmx(df_rocr_mod_both_BIC, threshold = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         observed\npredicted    1    0\n        1    3    5\n        0  345 5469\n```\n:::\n\n```{.r .cell-code}\n# Calcul de la specificite et de la sensibilite\nsensitivity(cmx(df_rocr_mod_both_BIC,threshold=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  sensitivity sensitivity.sd\n1  0.00862069    0.004962793\n```\n:::\n\n```{.r .cell-code}\nspecificity(cmx(df_rocr_mod_both_BIC,threshold=0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  specificity specificity.sd\n1   0.9990866   0.0004083396\n```\n:::\n\n```{.r .cell-code}\n# Courbe ROC pour le modele logistique CHD\nroc.plot.calculate(df_rocr_mod_both_BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    threshold        PCC sensitivity specificity       Kappa\n1        0.00 0.05977327 1.000000000   0.0000000 0.000000000\n2        0.01 0.11387839 0.991379310   0.0580928 0.006236199\n3        0.02 0.30573686 0.951149425   0.2647059 0.033766823\n4        0.03 0.44005496 0.876436782   0.4123128 0.054786724\n5        0.04 0.53349364 0.816091954   0.5155280 0.073989259\n6        0.05 0.61164548 0.755747126   0.6024845 0.093942266\n7        0.06 0.66626589 0.712643678   0.6633175 0.112391504\n8        0.07 0.72260392 0.649425287   0.7272561 0.132419851\n9        0.08 0.75798695 0.606321839   0.7676288 0.147977641\n10       0.09 0.79422879 0.540229885   0.8103763 0.160732911\n11       0.10 0.81312264 0.505747126   0.8326635 0.169120085\n12       0.11 0.84111989 0.448275862   0.8660943 0.181939429\n13       0.12 0.86413604 0.399425287   0.8936792 0.195162378\n14       0.13 0.87873583 0.382183908   0.9103033 0.213285407\n15       0.14 0.88869804 0.353448276   0.9227256 0.218078399\n16       0.15 0.90089316 0.293103448   0.9395323 0.208761324\n17       0.16 0.90398488 0.287356322   0.9431860 0.212527844\n18       0.17 0.91411886 0.221264368   0.9581659 0.190176854\n19       0.18 0.91824115 0.218390805   0.9627329 0.199363483\n20       0.19 0.92459636 0.149425287   0.9738765 0.155263076\n21       0.20 0.92528341 0.143678161   0.9749726 0.151459249\n22       0.21 0.93266919 0.103448276   0.9853855 0.129145502\n23       0.22 0.93404328 0.100574713   0.9870296 0.129898981\n24       0.23 0.93455857 0.097701149   0.9877603 0.127998390\n25       0.24 0.93679148 0.048850575   0.9932408 0.069636886\n26       0.25 0.93696324 0.040229885   0.9939715 0.057479164\n27       0.26 0.93765029 0.037356322   0.9948849 0.054930146\n28       0.27 0.93799382 0.031609195   0.9956156 0.047030424\n29       0.28 0.93816558 0.028735632   0.9959810 0.042997359\n30       0.29 0.93833734 0.017241379   0.9968944 0.025119942\n31       0.30 0.93850910 0.017241379   0.9970771 0.025505557\n32       0.31 0.93868087 0.017241379   0.9972598 0.025893025\n33       0.32 0.93885263 0.017241379   0.9974425 0.026282360\n34       0.33 0.93885263 0.017241379   0.9974425 0.026282360\n35       0.34 0.93885263 0.014367816   0.9976251 0.021569883\n36       0.35 0.93885263 0.014367816   0.9976251 0.021569883\n37       0.36 0.93885263 0.014367816   0.9976251 0.021569883\n38       0.37 0.93885263 0.014367816   0.9976251 0.021569883\n39       0.38 0.93885263 0.014367816   0.9976251 0.021569883\n40       0.39 0.93919615 0.014367816   0.9979905 0.022335101\n41       0.40 0.93936791 0.014367816   0.9981732 0.022720507\n42       0.41 0.93936791 0.014367816   0.9981732 0.022720507\n43       0.42 0.93953968 0.014367816   0.9983559 0.023107798\n44       0.43 0.93971144 0.014367816   0.9985385 0.023496985\n45       0.44 0.93971144 0.014367816   0.9985385 0.023496985\n46       0.45 0.93971144 0.014367816   0.9985385 0.023496985\n47       0.46 0.93953968 0.008620690   0.9987212 0.013465453\n48       0.47 0.93953968 0.008620690   0.9987212 0.013465453\n49       0.48 0.93953968 0.008620690   0.9987212 0.013465453\n50       0.49 0.93971144 0.008620690   0.9989039 0.013834639\n51       0.50 0.93988320 0.008620690   0.9990866 0.014205656\n52       0.51 0.93988320 0.008620690   0.9990866 0.014205656\n53       0.52 0.94005496 0.008620690   0.9992693 0.014578518\n54       0.53 0.93988320 0.005747126   0.9992693 0.009292050\n55       0.54 0.93988320 0.005747126   0.9992693 0.009292050\n56       0.55 0.93988320 0.005747126   0.9992693 0.009292050\n57       0.56 0.93988320 0.005747126   0.9992693 0.009292050\n58       0.57 0.93988320 0.005747126   0.9992693 0.009292050\n59       0.58 0.93988320 0.005747126   0.9992693 0.009292050\n60       0.59 0.93988320 0.005747126   0.9992693 0.009292050\n61       0.60 0.93988320 0.005747126   0.9992693 0.009292050\n62       0.61 0.94005496 0.005747126   0.9994520 0.009654499\n63       0.62 0.94022673 0.005747126   0.9996346 0.010018763\n64       0.63 0.94022673 0.005747126   0.9996346 0.010018763\n65       0.64 0.94005496 0.002873563   0.9996346 0.004681023\n66       0.65 0.94005496 0.002873563   0.9996346 0.004681023\n67       0.66 0.94005496 0.002873563   0.9996346 0.004681023\n68       0.67 0.94005496 0.002873563   0.9996346 0.004681023\n69       0.68 0.94022673 0.002873563   0.9998173 0.005034602\n70       0.69 0.94022673 0.002873563   0.9998173 0.005034602\n71       0.70 0.94039849 0.002873563   1.0000000 0.005389965\n72       0.71 0.94039849 0.002873563   1.0000000 0.005389965\n73       0.72 0.94039849 0.002873563   1.0000000 0.005389965\n74       0.73 0.94039849 0.002873563   1.0000000 0.005389965\n75       0.74 0.94039849 0.002873563   1.0000000 0.005389965\n76       0.75 0.94039849 0.002873563   1.0000000 0.005389965\n77       0.76 0.94039849 0.002873563   1.0000000 0.005389965\n78       0.77 0.94039849 0.002873563   1.0000000 0.005389965\n79       0.78 0.94039849 0.002873563   1.0000000 0.005389965\n80       0.79 0.94039849 0.002873563   1.0000000 0.005389965\n81       0.80 0.94039849 0.002873563   1.0000000 0.005389965\n82       0.81 0.94039849 0.002873563   1.0000000 0.005389965\n83       0.82 0.94039849 0.002873563   1.0000000 0.005389965\n84       0.83 0.94039849 0.002873563   1.0000000 0.005389965\n85       0.84 0.94039849 0.002873563   1.0000000 0.005389965\n86       0.85 0.94039849 0.002873563   1.0000000 0.005389965\n87       0.86 0.94039849 0.002873563   1.0000000 0.005389965\n88       0.87 0.94039849 0.002873563   1.0000000 0.005389965\n89       0.88 0.94039849 0.002873563   1.0000000 0.005389965\n90       0.89 0.94039849 0.002873563   1.0000000 0.005389965\n91       0.90 0.94022673 0.000000000   1.0000000 0.000000000\n92       0.91 0.94022673 0.000000000   1.0000000 0.000000000\n93       0.92 0.94022673 0.000000000   1.0000000 0.000000000\n94       0.93 0.94022673 0.000000000   1.0000000 0.000000000\n95       0.94 0.94022673 0.000000000   1.0000000 0.000000000\n96       0.95 0.94022673 0.000000000   1.0000000 0.000000000\n97       0.96 0.94022673 0.000000000   1.0000000 0.000000000\n98       0.97 0.94022673 0.000000000   1.0000000 0.000000000\n99       0.98 0.94022673 0.000000000   1.0000000 0.000000000\n100      0.99 0.94022673 0.000000000   1.0000000 0.000000000\n101      1.00 0.94022673 0.000000000   1.0000000 0.000000000\n```\n:::\n\n```{.r .cell-code}\n#####\n\npar(mfrow=c(2,2))\nauc.roc.plot(df_rocr_mod, main = \"ROC plot mod_full\")\nauc.roc.plot(df_rocr_mod_both_AIC, main = \"ROC plot mod_both_AIC\")\nauc.roc.plot(df_rocr_mod_both_AIC_2.0, main = \"ROC plot mod_both_AIC_2.0\")\nauc.roc.plot(df_rocr_mod_both_BIC, main = \"ROC plot mod_both_BIC\") # graphe courbe ROC\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n\n\n# On veut maximiser l'AUC, ici les valeurs sont très proche \n# donc on peut quand même préféré le petit modele (BIC) meme si AUC plus faible\n\n## On compare avec anova \n\nanova(mod_full, mod_both_AIC_2.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: car$Purchase ~ MOSTYPE + MAANTHUI + MGEMOMV + MGEMLEEF + MOSHOOFD + \n    MGODRK + MGODPR + MGODOV + MGODGE + MRELGE + MRELSA + MRELOV + \n    MFALLEEN + MFGEKIND + MFWEKIND + MOPLHOOG + MOPLMIDD + MOPLLAAG + \n    MBERHOOG + MBERZELF + MBERBOER + MBERMIDD + MBERARBG + MBERARBO + \n    MSKA + MSKB1 + MSKB2 + MSKC + MSKD + MHHUUR + MHKOOP + MAUT1 + \n    MAUT2 + MAUT0 + MZFONDS + MZPART + MINKM30 + MINK3045 + MINK4575 + \n    MINK7512 + MINK123M + MINKGEM + MKOOPKLA + PWAPART + PWABEDR + \n    PWALAND + PPERSAUT + PBESAUT + PMOTSCO + PVRAAUT + PAANHANG + \n    PTRACTOR + PWERKT + PBROM + PLEVEN + PPERSONG + PGEZONG + \n    PWAOREG + PBRAND + PZEILPL + PPLEZIER + PFIETS + PINBOED + \n    PBYSTAND + AWAPART + AWABEDR + AWALAND + APERSAUT + ABESAUT + \n    AMOTSCO + AVRAAUT + AAANHANG + ATRACTOR + AWERKT + ABROM + \n    ALEVEN + APERSONG + AGEZONG + AWAOREG + ABRAND + AZEILPL + \n    APLEZIER + AFIETS + AINBOED + ABYSTAND\nModel 2: car$Purchase ~ PPERSAUT + MKOOPKLA + PBRAND + APLEZIER + MOPLLAAG + \n    MBERBOER + MRELGE + PWALAND + AFIETS + MINK123M + MINKGEM + \n    MGEMLEEF + PWAPART + ABYSTAND + ABRAND + AWERKT + MGODPR + \n    MSKC + MOPLHOOG + MBERMIDD\n  Resid. Df Resid. Dev  Df Deviance\n1      5736     2243.5             \n2      5801     2301.9 -65  -58.458\n```\n:::\n\n```{.r .cell-code}\nanova(mod_full, mod_both_BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: car$Purchase ~ MOSTYPE + MAANTHUI + MGEMOMV + MGEMLEEF + MOSHOOFD + \n    MGODRK + MGODPR + MGODOV + MGODGE + MRELGE + MRELSA + MRELOV + \n    MFALLEEN + MFGEKIND + MFWEKIND + MOPLHOOG + MOPLMIDD + MOPLLAAG + \n    MBERHOOG + MBERZELF + MBERBOER + MBERMIDD + MBERARBG + MBERARBO + \n    MSKA + MSKB1 + MSKB2 + MSKC + MSKD + MHHUUR + MHKOOP + MAUT1 + \n    MAUT2 + MAUT0 + MZFONDS + MZPART + MINKM30 + MINK3045 + MINK4575 + \n    MINK7512 + MINK123M + MINKGEM + MKOOPKLA + PWAPART + PWABEDR + \n    PWALAND + PPERSAUT + PBESAUT + PMOTSCO + PVRAAUT + PAANHANG + \n    PTRACTOR + PWERKT + PBROM + PLEVEN + PPERSONG + PGEZONG + \n    PWAOREG + PBRAND + PZEILPL + PPLEZIER + PFIETS + PINBOED + \n    PBYSTAND + AWAPART + AWABEDR + AWALAND + APERSAUT + ABESAUT + \n    AMOTSCO + AVRAAUT + AAANHANG + ATRACTOR + AWERKT + ABROM + \n    ALEVEN + APERSONG + AGEZONG + AWAOREG + ABRAND + AZEILPL + \n    APLEZIER + AFIETS + AINBOED + ABYSTAND\nModel 2: car$Purchase ~ PPERSAUT + PBRAND + APLEZIER + MOPLLAAG + MBERBOER + \n    MRELGE\n  Resid. Df Resid. Dev  Df Deviance\n1      5736     2243.5             \n2      5815     2353.0 -79  -109.48\n```\n:::\n\n```{.r .cell-code}\nanova(mod_both_AIC_2.0, mod_both_BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: car$Purchase ~ PPERSAUT + MKOOPKLA + PBRAND + APLEZIER + MOPLLAAG + \n    MBERBOER + MRELGE + PWALAND + AFIETS + MINK123M + MINKGEM + \n    MGEMLEEF + PWAPART + ABYSTAND + ABRAND + AWERKT + MGODPR + \n    MSKC + MOPLHOOG + MBERMIDD\nModel 2: car$Purchase ~ PPERSAUT + PBRAND + APLEZIER + MOPLLAAG + MBERBOER + \n    MRELGE\n  Resid. Df Resid. Dev  Df Deviance\n1      5801     2301.9             \n2      5815     2353.0 -14  -51.019\n```\n:::\n\n```{.r .cell-code}\n### INTERPRETATION ???????\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Question 5 --------------------------------------------------------------\n\n#demarche aléatoire conduit à 6%\nround(table(car$Purchase)*100/n, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    No    Yes \n94.023  5.977 \n```\n:::\n\n```{.r .cell-code}\n# Question 6 --------------------------------------------------------------\n\n\nmod_full.probs=predict(mod_full, car, type=\"response\")# --> donne les proba\nmod_full.pred=rep(\"No\", n)\nmod_full.pred[mod_full.probs>.5]=\"Yes\"\ntable(mod_full.pred, car$Purchase)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             \nmod_full.pred   No  Yes\n          No  5466  341\n          Yes    8    7\n```\n:::\n\n```{.r .cell-code}\n# taux de reussitz de démarchage \n# (i.e, le nb de vrais positifs par rapport au nb de positifs prédit)\n# 7/(8+7) = 0.4666667\n# on arrive à prévoir les souscriptions d'assurances dans 47% des cas\n# (8+341)/n = 6% erreur de classification\n# (7+5466)/n = 94% precision ou accuracy\n# sensibility = 7/(7+341) =0.02\nmean(mod_full.pred == car$Purchase) # ne marche pas ??\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.940055\n```\n:::\n\n```{.r .cell-code}\n# = 0\n\nmod_both_AIC_2.0.probs=predict(mod_both_AIC_2.0, car, type=\"response\")# --> donne les proba\nmod_both_AIC_2.0.pred=rep(\"No\", n)\nmod_both_AIC_2.0.pred[mod_both_AIC_2.0.probs>.5]=\"Yes\"\ntable(mod_both_AIC_2.0.pred, car$Purchase)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     \nmod_both_AIC_2.0.pred   No  Yes\n                  No  5464  345\n                  Yes   10    3\n```\n:::\n\n```{.r .cell-code}\n# refaire l'analyse du dessus \n\nmod_both_BIC.probs=predict(mod_both_BIC, car, type=\"response\")# --> donne les proba\nmod_both_BIC.pred=rep(\"No\", n)\nmod_both_BIC.pred[mod_both_BIC.probs>.5]=\"Yes\"\ntable(mod_both_BIC.pred, car$Purchase)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 \nmod_both_BIC.pred   No  Yes\n              No  5469  345\n              Yes    5    3\n```\n:::\n\n```{.r .cell-code}\n# refaire l'analyse du dessus \n\n\n\n#  % de clients ayant une proba >0.5\nround(sum(fitted.values(mod_full)>0.5)*100/n, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.258\n```\n:::\n\n```{.r .cell-code}\n# 0.26%\nboxplot(fitted.values(mod_full))\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\nquantile(fitted.values(mod_full), 0.94) # seuil pour que ce % corresponde à environ 6% des clients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      94% \n0.1807236 \n```\n:::\n\n```{.r .cell-code}\n# 0.1807236 \n\nround(sum(fitted.values(mod_both_AIC_2.0)>0.5)*100/n, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.223\n```\n:::\n\n```{.r .cell-code}\n# 0.223\nboxplot(fitted.values(mod_both_AIC_2.0))\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-18-2.png){width=672}\n:::\n\n```{.r .cell-code}\nquantile(fitted.values(mod_both_AIC_2.0), 0.94)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      94% \n0.1759159 \n```\n:::\n\n```{.r .cell-code}\n# 0.1759159\n\nround(sum(fitted.values(mod_both_BIC)>0.5)*100/n, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.137\n```\n:::\n\n```{.r .cell-code}\n# 0.137\nboxplot(fitted.values(mod_both_BIC))\n```\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-18-3.png){width=672}\n:::\n\n```{.r .cell-code}\nquantile(fitted.values(mod_both_BIC), 0.94)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      94% \n0.1659636 \n```\n:::\n\n```{.r .cell-code}\n# 0.1659636 \n\n\n\n########\n# On décide dans la suite de fixer ce seuil à 0.2 et on cherche à sélectionner \n# le meilleur modèle parmi les 3 précédents.\n\n# on change le seuil\nmod_full.pred[mod_full.probs>0.2]=1 # = \"Yes\"\ntable(mod_full.pred, car$Purchase)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             \nmod_full.pred   No  Yes\n           1   197   87\n           No 5277  261\n```\n:::\n\n```{.r .cell-code}\n# taux de reussite demarchage (vrais positifs par rapport au positifs predits)\n# 87/(197+87) = 0.306338 = 30.6%\n\n### refaire pour les autres modèles \n\nmod_both_AIC_2.0.pred[mod_both_AIC_2.0.probs>0.2]=1\ntable(mod_both_AIC_2.0.pred, car$Purchase)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     \nmod_both_AIC_2.0.pred   No  Yes\n                   1   181   75\n                   No 5293  273\n```\n:::\n\n```{.r .cell-code}\n# 0.2929688\n\nmod_both_BIC.pred[mod_both_BIC.probs>0.2]=1\ntable(mod_both_BIC.pred, car$Purchase)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 \nmod_both_BIC.pred   No  Yes\n               1   137   50\n               No 5337  298\n```\n:::\n\n```{.r .cell-code}\n# 0.2673797\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Question 7 --------------------------------------------------------------\n\n# Estimer le taux de réussite du démarchage (c’est à dire le nombre de vrais\n# positifs par rapport au nombre de positifs prédits) sur \n# l’échantillon d’apprentissage pour chaque modèle\n\n# fct de cout : freq vrai posi parmi les posi preddit\ncost = function(r, p){\n  # r = reponse\n  # p = prevision\n  s = sum(p>0.2 & r==1)/sum(p>0.2)\n  return(s)\n}\n\ncost(mod_full$y, fitted.values(mod_full))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.306338\n```\n:::\n\n```{.r .cell-code}\n# 0.306338 = 30.6 % \n# on obtient effectivement le même resultat que calculé précédement \n\ncost(mod_both_AIC_2.0$y, fitted.values(mod_both_AIC_2.0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2929688\n```\n:::\n\n```{.r .cell-code}\n#  0.2929688\n# ca correspond \n\ncost(mod_both_BIC$y, fitted.values(mod_both_BIC))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2673797\n```\n:::\n\n```{.r .cell-code}\n# 0.2673797\n# ca correspond \n\n## Selection  du modele complet\n# MAIS les resultats sont trop optimistes car ils sontobtenus via l'echant d'apprent\n# Dans ce cas, il n'est pas etonnant que le plus gros mod (mod_full) soit selectionné \n# il faut évaluer l'erreur sur echant test\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Question 8 --------------------------------------------------------------\nlibrary(boot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttachement du package : 'boot'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nL'objet suivant est masqué depuis 'package:psych':\n\n    logit\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nL'objet suivant est masqué depuis 'package:car':\n\n    logit\n```\n:::\n\n```{.r .cell-code}\nres = cv.glm(data = car, glmfit = mod_full, cost = cost, K=10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in model.frame.default(formula = car$Purchase ~ ., data = structure(list(: les longueurs des variables diffèrent (trouvé pour 'MOSTYPE')\n```\n:::\n\n```{.r .cell-code}\nres2 = cv.glm(car, mod_both_AIC_2.0, cost, K=10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in model.frame.default(formula = car$Purchase ~ PPERSAUT + MKOOPKLA + : les longueurs des variables diffèrent (trouvé pour 'PPERSAUT')\n```\n:::\n\n```{.r .cell-code}\nres3 = cv.glm(car, mod_both_BIC, cost, K=10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in model.frame.default(formula = car$Purchase ~ PPERSAUT + PBRAND + : les longueurs des variables diffèrent (trouvé pour 'PPERSAUT')\n```\n:::\n\n```{.r .cell-code}\n## marche pas. pourquoi??? (voir photo code pour réponse)\n\nres$delta\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): objet 'res' introuvable\n```\n:::\n\n```{.r .cell-code}\nres2$delta\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): objet 'res2' introuvable\n```\n:::\n\n```{.r .cell-code}\nres3$delta\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in eval(expr, envir, enclos): objet 'res3' introuvable\n```\n:::\n\n```{.r .cell-code}\ndim(car)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5822   86\n```\n:::\n\n```{.r .cell-code}\n###### test #####\nmod_test = glm(Caravan$Purchase~., family = \"binomial\", data = Caravan)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n```\n:::\n\n```{.r .cell-code}\ncv.glm(Caravan, mod_test, cost = cost, K=10)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in model.frame.default(formula = Caravan$Purchase ~ ., data = structure(list(: les longueurs des variables diffèrent (trouvé pour 'MOSTYPE')\n```\n:::\n\n```{.r .cell-code}\nsummary(Caravan)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    MOSTYPE         MAANTHUI         MGEMOMV         MGEMLEEF    \n Min.   : 1.00   Min.   : 1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:10.00   1st Qu.: 1.000   1st Qu.:2.000   1st Qu.:2.000  \n Median :30.00   Median : 1.000   Median :3.000   Median :3.000  \n Mean   :24.25   Mean   : 1.111   Mean   :2.679   Mean   :2.991  \n 3rd Qu.:35.00   3rd Qu.: 1.000   3rd Qu.:3.000   3rd Qu.:3.000  \n Max.   :41.00   Max.   :10.000   Max.   :5.000   Max.   :6.000  \n    MOSHOOFD          MGODRK           MGODPR          MGODOV    \n Min.   : 1.000   Min.   :0.0000   Min.   :0.000   Min.   :0.00  \n 1st Qu.: 3.000   1st Qu.:0.0000   1st Qu.:4.000   1st Qu.:0.00  \n Median : 7.000   Median :0.0000   Median :5.000   Median :1.00  \n Mean   : 5.774   Mean   :0.6965   Mean   :4.627   Mean   :1.07  \n 3rd Qu.: 8.000   3rd Qu.:1.0000   3rd Qu.:6.000   3rd Qu.:2.00  \n Max.   :10.000   Max.   :9.0000   Max.   :9.000   Max.   :5.00  \n     MGODGE          MRELGE          MRELSA           MRELOV    \n Min.   :0.000   Min.   :0.000   Min.   :0.0000   Min.   :0.00  \n 1st Qu.:2.000   1st Qu.:5.000   1st Qu.:0.0000   1st Qu.:1.00  \n Median :3.000   Median :6.000   Median :1.0000   Median :2.00  \n Mean   :3.259   Mean   :6.183   Mean   :0.8835   Mean   :2.29  \n 3rd Qu.:4.000   3rd Qu.:7.000   3rd Qu.:1.0000   3rd Qu.:3.00  \n Max.   :9.000   Max.   :9.000   Max.   :7.0000   Max.   :9.00  \n    MFALLEEN        MFGEKIND       MFWEKIND      MOPLHOOG        MOPLMIDD    \n Min.   :0.000   Min.   :0.00   Min.   :0.0   Min.   :0.000   Min.   :0.000  \n 1st Qu.:0.000   1st Qu.:2.00   1st Qu.:3.0   1st Qu.:0.000   1st Qu.:2.000  \n Median :2.000   Median :3.00   Median :4.0   Median :1.000   Median :3.000  \n Mean   :1.888   Mean   :3.23   Mean   :4.3   Mean   :1.461   Mean   :3.351  \n 3rd Qu.:3.000   3rd Qu.:4.00   3rd Qu.:6.0   3rd Qu.:2.000   3rd Qu.:4.000  \n Max.   :9.000   Max.   :9.00   Max.   :9.0   Max.   :9.000   Max.   :9.000  \n    MOPLLAAG        MBERHOOG        MBERZELF        MBERBOER     \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:3.000   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :5.000   Median :2.000   Median :0.000   Median :0.0000  \n Mean   :4.572   Mean   :1.895   Mean   :0.398   Mean   :0.5223  \n 3rd Qu.:6.000   3rd Qu.:3.000   3rd Qu.:1.000   3rd Qu.:1.0000  \n Max.   :9.000   Max.   :9.000   Max.   :5.000   Max.   :9.0000  \n    MBERMIDD        MBERARBG       MBERARBO          MSKA           MSKB1      \n Min.   :0.000   Min.   :0.00   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:2.000   1st Qu.:1.00   1st Qu.:1.000   1st Qu.:0.000   1st Qu.:1.000  \n Median :3.000   Median :2.00   Median :2.000   Median :1.000   Median :2.000  \n Mean   :2.899   Mean   :2.22   Mean   :2.306   Mean   :1.621   Mean   :1.607  \n 3rd Qu.:4.000   3rd Qu.:3.00   3rd Qu.:3.000   3rd Qu.:2.000   3rd Qu.:2.000  \n Max.   :9.000   Max.   :9.00   Max.   :9.000   Max.   :9.000   Max.   :9.000  \n     MSKB2            MSKC            MSKD           MHHUUR     \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:1.000   1st Qu.:2.000   1st Qu.:0.000   1st Qu.:2.000  \n Median :2.000   Median :4.000   Median :1.000   Median :4.000  \n Mean   :2.203   Mean   :3.759   Mean   :1.067   Mean   :4.237  \n 3rd Qu.:3.000   3rd Qu.:5.000   3rd Qu.:2.000   3rd Qu.:7.000  \n Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  \n     MHKOOP          MAUT1          MAUT2           MAUT0          MZFONDS     \n Min.   :0.000   Min.   :0.00   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:2.000   1st Qu.:5.00   1st Qu.:0.000   1st Qu.:1.000   1st Qu.:5.000  \n Median :5.000   Median :6.00   Median :1.000   Median :2.000   Median :7.000  \n Mean   :4.772   Mean   :6.04   Mean   :1.316   Mean   :1.959   Mean   :6.277  \n 3rd Qu.:7.000   3rd Qu.:7.00   3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:8.000  \n Max.   :9.000   Max.   :9.00   Max.   :7.000   Max.   :9.000   Max.   :9.000  \n     MZPART         MINKM30         MINK3045        MINK4575    \n Min.   :0.000   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:1.000  \n Median :2.000   Median :2.000   Median :4.000   Median :3.000  \n Mean   :2.729   Mean   :2.574   Mean   :3.536   Mean   :2.731  \n 3rd Qu.:4.000   3rd Qu.:4.000   3rd Qu.:5.000   3rd Qu.:4.000  \n Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  \n    MINK7512         MINK123M         MINKGEM         MKOOPKLA    \n Min.   :0.0000   Min.   :0.0000   Min.   :0.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:3.000  \n Median :0.0000   Median :0.0000   Median :4.000   Median :4.000  \n Mean   :0.7961   Mean   :0.2027   Mean   :3.784   Mean   :4.236  \n 3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:4.000   3rd Qu.:6.000  \n Max.   :9.0000   Max.   :9.0000   Max.   :9.000   Max.   :8.000  \n    PWAPART          PWABEDR           PWALAND           PPERSAUT   \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00  \n Median :0.0000   Median :0.00000   Median :0.00000   Median :5.00  \n Mean   :0.7712   Mean   :0.04002   Mean   :0.07162   Mean   :2.97  \n 3rd Qu.:2.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:6.00  \n Max.   :3.0000   Max.   :6.00000   Max.   :4.00000   Max.   :8.00  \n    PBESAUT           PMOTSCO          PVRAAUT            PAANHANG      \n Min.   :0.00000   Min.   :0.0000   Min.   :0.000000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.000000   1st Qu.:0.00000  \n Median :0.00000   Median :0.0000   Median :0.000000   Median :0.00000  \n Mean   :0.04827   Mean   :0.1754   Mean   :0.009447   Mean   :0.02096  \n 3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:0.000000   3rd Qu.:0.00000  \n Max.   :7.00000   Max.   :7.0000   Max.   :9.000000   Max.   :5.00000  \n    PTRACTOR           PWERKT            PBROM           PLEVEN      \n Min.   :0.00000   Min.   :0.00000   Min.   :0.000   Min.   :0.0000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000   1st Qu.:0.0000  \n Median :0.00000   Median :0.00000   Median :0.000   Median :0.0000  \n Mean   :0.09258   Mean   :0.01305   Mean   :0.215   Mean   :0.1948  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000   3rd Qu.:0.0000  \n Max.   :6.00000   Max.   :6.00000   Max.   :6.000   Max.   :9.0000  \n    PPERSONG          PGEZONG           PWAOREG            PBRAND     \n Min.   :0.00000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000  \n Median :0.00000   Median :0.00000   Median :0.00000   Median :2.000  \n Mean   :0.01374   Mean   :0.01529   Mean   :0.02353   Mean   :1.828  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:4.000  \n Max.   :6.00000   Max.   :3.00000   Max.   :7.00000   Max.   :8.000  \n    PZEILPL             PPLEZIER           PFIETS           PINBOED       \n Min.   :0.0000000   Min.   :0.00000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.0000000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.0000000   Median :0.00000   Median :0.00000   Median :0.00000  \n Mean   :0.0008588   Mean   :0.01889   Mean   :0.02525   Mean   :0.01563  \n 3rd Qu.:0.0000000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :3.0000000   Max.   :6.00000   Max.   :1.00000   Max.   :6.00000  \n    PBYSTAND          AWAPART         AWABEDR           AWALAND       \n Min.   :0.00000   Min.   :0.000   Min.   :0.00000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.000   1st Qu.:0.00000   1st Qu.:0.00000  \n Median :0.00000   Median :0.000   Median :0.00000   Median :0.00000  \n Mean   :0.04758   Mean   :0.403   Mean   :0.01477   Mean   :0.02061  \n 3rd Qu.:0.00000   3rd Qu.:1.000   3rd Qu.:0.00000   3rd Qu.:0.00000  \n Max.   :5.00000   Max.   :2.000   Max.   :5.00000   Max.   :1.00000  \n    APERSAUT         ABESAUT           AMOTSCO           AVRAAUT        \n Min.   :0.0000   Min.   :0.00000   Min.   :0.00000   Min.   :0.000000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000000  \n Median :1.0000   Median :0.00000   Median :0.00000   Median :0.000000  \n Mean   :0.5622   Mean   :0.01048   Mean   :0.04105   Mean   :0.002233  \n 3rd Qu.:1.0000   3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000000  \n Max.   :7.0000   Max.   :4.00000   Max.   :8.00000   Max.   :3.000000  \n    AAANHANG          ATRACTOR           AWERKT             ABROM        \n Min.   :0.00000   Min.   :0.00000   Min.   :0.000000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.00000  \n Median :0.00000   Median :0.00000   Median :0.000000   Median :0.00000  \n Mean   :0.01254   Mean   :0.03367   Mean   :0.006183   Mean   :0.07042  \n 3rd Qu.:0.00000   3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:0.00000  \n Max.   :3.00000   Max.   :4.00000   Max.   :6.000000   Max.   :2.00000  \n     ALEVEN           APERSONG           AGEZONG            AWAOREG        \n Min.   :0.00000   Min.   :0.000000   Min.   :0.000000   Min.   :0.000000  \n 1st Qu.:0.00000   1st Qu.:0.000000   1st Qu.:0.000000   1st Qu.:0.000000  \n Median :0.00000   Median :0.000000   Median :0.000000   Median :0.000000  \n Mean   :0.07661   Mean   :0.005325   Mean   :0.006527   Mean   :0.004638  \n 3rd Qu.:0.00000   3rd Qu.:0.000000   3rd Qu.:0.000000   3rd Qu.:0.000000  \n Max.   :8.00000   Max.   :1.000000   Max.   :1.000000   Max.   :2.000000  \n     ABRAND          AZEILPL             APLEZIER            AFIETS       \n Min.   :0.0000   Min.   :0.0000000   Min.   :0.000000   Min.   :0.00000  \n 1st Qu.:0.0000   1st Qu.:0.0000000   1st Qu.:0.000000   1st Qu.:0.00000  \n Median :1.0000   Median :0.0000000   Median :0.000000   Median :0.00000  \n Mean   :0.5701   Mean   :0.0005153   Mean   :0.006012   Mean   :0.03178  \n 3rd Qu.:1.0000   3rd Qu.:0.0000000   3rd Qu.:0.000000   3rd Qu.:0.00000  \n Max.   :7.0000   Max.   :1.0000000   Max.   :2.000000   Max.   :3.00000  \n    AINBOED            ABYSTAND       Purchase  \n Min.   :0.000000   Min.   :0.00000   No :5474  \n 1st Qu.:0.000000   1st Qu.:0.00000   Yes: 348  \n Median :0.000000   Median :0.00000             \n Mean   :0.007901   Mean   :0.01426             \n 3rd Qu.:0.000000   3rd Qu.:0.00000             \n Max.   :2.000000   Max.   :2.00000             \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Question 9 --------------------------------------------------------------\n\n# Estimer de même le taux de réussite pour chaque modèle lorsque le seuil\n# varie de 0.10 à 0.30 par pas de 0.01\n\n\nmat=matrix(nrow=3,ncol=21)\nk=0\nfor(s in seq(0.1,0.3,0.01)){\n  k=k+1\n  cat(k) # Concatenate and Print\n  res=cv.glm(Caravan,mod_full,cost,K=10)\n  res2=cv.glm(Caravan,mod_both_AIC_2.0,cost,K=10)\n  res3=cv.glm(Caravan,mod_both_BIC,cost,K=10)\n  mat[1,k]=res$delta[1]\n  mat[2,k]=res2$delta[1]\n  mat[3,k]=res3$delta[1]\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in model.frame.default(formula = car$Purchase ~ ., data = structure(list(: les longueurs des variables diffèrent (trouvé pour 'MOSTYPE')\n```\n:::\n\n```{.r .cell-code}\nmatplot(seq(0.1,0.3,0.01),t(mat),type='l')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in min(x): aucun argument trouvé pour min ; Inf est renvoyé\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in max(x): aucun argument pour max ; -Inf est renvoyé\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in plot.window(...): valeurs finies requises pour 'ylim'\n```\n:::\n\n::: {.cell-output-display}\n![](Exercice_04_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlegend('topleft',c('Tout','AIC','BIC'),col=1:3,lty=1:3)\n\n\n\n\n\n#repetition 5 fois (tres long)\nres_seuil=NULL\nfor(i in 1:5){\n  mat=matrix(nrow=3,ncol=21)\n  k=0\n  for(s in seq(0.1,0.3,0.01)){\n    k=k+1\n    cat(k)\n    res=cv.glm(Caravan,mod,cost,K=10)\n    res2=cv.glm(Caravan,mod2,cost,K=10)\n    res3=cv.glm(Caravan,mod3,cost,K=10)\n    mat[1,k]=res$delta[1]\n    mat[2,k]=res2$delta[1]\n    mat[3,k]=res3$delta[1]\n  }\n  res_seuil[[i]]=mat\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1\n```\n:::\n\n::: {.cell-output .cell-output-error}\n```\nError in cv.glm(Caravan, mod, cost, K = 10): objet 'mod' introuvable\n```\n:::\n\n```{.r .cell-code}\nmatplot(seq(0.1,0.3,0.01),t(res_seuil[[1]]),type='l',ylim=c(0.15,0.35),lty=2)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in t.default(res_seuil[[1]]): l'argument n'est pas une matrice\n```\n:::\n\n```{.r .cell-code}\nfor(i in 2:5) matplot(seq(0.1,0.3,0.01),t(res_seuil[[i]]),type='l',lty=2,add=T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in t.default(res_seuil[[i]]): l'argument n'est pas une matrice\n```\n:::\n\n```{.r .cell-code}\nres_moy=res_seuil[[1]]\nfor(i in 2:5) res_moy=res_moy+res_seuil[[i]]\nmatplot(seq(0.1,0.3,0.01),t(res_moy)/5,type='l',lty=1,lwd=2,add=T)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in matplot(seq(0.1, 0.3, 0.01), t(res_moy)/5, type = \"l\", lty = 1, : 'x' et 'y' doivent avoir le même nombre de lignes\n```\n:::\n\n```{.r .cell-code}\nlegend('topleft',c('Tout','AIC','BIC'),col=1:3,lty=1:3)\n\n#On observe que les courbes deviennent tres variables lorsque le seuil grandit. Il y a meme des valeurs manquantes\n#C'est parcequ'il y a tres peu de positifs predits dans ce cas (voire aucun dans certains folds) et donc l'estimation de l'erreur est tres peu precise\n#Le meilleur modele semble celui par BIC. \n#Si on veut avoir a la fois un taux de demarchage optimum et un nombre de positifs pas trop faible, un seuil de 0.2 semble pas mal\n```\n:::\n",
    "supporting": [
      "Exercice_04_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
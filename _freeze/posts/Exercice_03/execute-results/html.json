{
  "hash": "f5457ea803232077b7372350e44212a4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercice 03\"\nauthor: \"Clément Poupelin\"\ndate: \"2025-02-17\"\ndate-modified: \"2025-02-21\"\nformat: \n  html:\n    embed-resources: false\n    toc: true\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: true\n    toc-location: right\n    page-layout: article\n    code-overflow: wrap\ntoc: true\nnumber-sections: false\neditor: visual\ncategories: [\"regression linéaire\", \"validation croisée\"]\nimage: \"/img/validation2.png\"\ndescription: \"Première essais de techniques de validations croisées sur des données générées manuellement\"\n---\n\n\n\n\n# Setup\n\n\n::: panel-tabset\n\n## Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Données\nlibrary(dplyr)        # manipulation des données\n\nlibrary(boot) ## CV\n\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n```\n:::\n\n\n\n\n\n\n## Fonctions \n\n::: panel-tabset\n\n### boxplot \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_boxplot <- function(data) {\n  # Transformer les données en format long pour ggplot\n  data_long <- reshape2::melt(data)\n  \n  ggplot(data_long, aes(x = variable, y = value, fill = variable)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +  # Palette de couleurs harmonieuse\n    labs(title = \"Distribution des Variables (Boxplot)\", x = \"Variables\", y = \"Valeurs\") +\n    theme_minimal() +  # Thème épuré\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotation des étiquettes\n}\n```\n:::\n\n\n\n### pairs.panels \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_pairs.panels <- function(data) {\n  psych::pairs.panels(\n    data,\n    method = \"pearson\",\n    # Méthode de corrélation\n    hist.col = RColorBrewer::brewer.pal(9, \"Set3\"),\n    # Couleurs des histogrammes\n    density = TRUE,\n    # Ajout des courbes de densité\n    ellipses = TRUE,\n    # Ajout d'ellipses\n    smooth = TRUE,\n    # Ajout de régressions lissées\n    lm = TRUE,\n    # Ajout des droites de régression\n    col = \"#69b3a2\",\n    # Couleur des points\n    alpha = 0.5              # Transparence\n  )\n}\n```\n:::\n\n\n\n\n### LOO\n\nPremière fonction LOO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo <- function(mod) {\n  n <- nrow(mod$model)\n  Call <- mod$call\n  erreur <- 1:n\n  for (i in 1:n) {\n    Call$data <- mod$model[-i, ] # mod$call$data transforme en data.frame\n    fit <- eval.parent(Call)\n    pred = predict(fit, mod$model[i, ])\n    erreur[i] <- (pred - mod$model[i, 1])^2\n  }\n  return(round(mean(erreur), 3))\n}\n```\n:::\n\n\n\nDeuxième fonction LOO\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo2 <- function(mod) {\n  round(mean((residuals(mod) / (1 - hatvalues(mod)))^2), 3)\n}\n```\n:::\n\n\n\nFonction pour obtenir les résultats\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_loo_results <- function(model, func) {\n  start_time <- Sys.time()        \n  result <- func(model)          \n  end_time <- Sys.time()         \n  time_taken <- round(end_time - start_time, 3)  \n  \n  return(list(result = result, time = time_taken))\n}\n```\n:::\n\n\n\n\n\n:::\n\n\n\n## Seed\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(140400)\n```\n:::\n\n\n\n:::\n\n# Données \n\nPour cette exercice, on va générer un modèle de régression linéaire classique : \n\n$$y = X\\beta + \\mathcal{E}$$ \n\n- $y \\in \\mathbb{R}^{n}$ la variable réponse ou variable à expliquer\n\n- $X \\in \\mathbb{R}^{n\\times p}$ la matrice contenant nos variables explicatives\n\n- $\\beta \\in \\mathbb{R}^{p}$ le vecteur composée des coefficients de régression\n\n- $\\mathcal{E} \\in \\mathbb{R}^{n}$ le vecteur d'erreur suivant une loi $\\mathcal{N}(0, 1)$\n\nPour la génération de nos données, nous allons alors poser que $\\beta = (1, -2)'$ et $X = [x, x^2]$, $x \\in \\mathbb{R}^n$ suivant une loi $\\mathcal{N}(0,1)$.\n\nOn aura alors que $y = x - 2x^2 + \\mathcal{E}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- rnorm(1000)\ny <- x - 2*(x^2) + rnorm(1000)\nSimu_data <- data.frame(y = y, x = x)\n```\n:::\n\n\n\n::: callout-note\npour des raisons de repouctibilité, une graine ou seed a été défini dans le setup afin que la génération aléatoire reste identique.\n:::\n\n\nOn va ainsi supposer avoir observé les deux vecteurs $x$ et $y$ précédents, sans connaître le lien théorique précédent qui lie $x$ et $y$. \\\nEt donc on cherchera à estimer ce lien.\n\n\n# Analyse descriptive\n\n::: panel-tabset\n\n## Boxplot\n\nOn peut regarder un peu la distribution de nos différents variables quantitatives via des boxplots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_boxplot(Simu_data)\n```\n\n::: {.cell-output-display}\n![](Exercice_03_files/figure-html/unnamed-chunk-9-1.png){width=768}\n:::\n:::\n\n\n\n::: success-header\n::: success-icon\n::: \nRésultats\n:::\n\n::: success\nOn voit bien que notre variable $x$ a une distribution normale centré réduite mais quelle n'est pas non plus parfaitement symétrique (forcément entre la \"perfection\" de la théorie et la génération par ordinateur il y a toujours une légère différence).\n\nEt concernant $y$, de manière logique avec le modèle simulé, on peut voir d'avantages de valeurs négatives.\n:::\n\n\n## Correlation panel\n\n\nOn regarde ici la corrélation calculée entre chacune de nos variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_pairs.panels(Simu_data)\n```\n\n::: {.cell-output-display}\n![](Exercice_03_files/figure-html/unnamed-chunk-10-1.png){width=2688}\n:::\n:::\n\n\n\n::: success-header\n::: success-icon\n::: \nRésultats\n:::\n\n::: success\nTout d'abord, on peut remarquer une corrélation faible de 34% entre $x$ et $y$. Pourtant le nuage de point semble quand à lui témoigner d'une influence de $x$ sur $y$ pouvant justifier d'un lien linéaire.\n\nAussi on retrouve un belle histogramme de distibution $\\mathcal{N}(0, 1)$ pour notre variable $x$.\n:::\n\n:::\n\n\n\n\n# Analyse inférentielle\n\nMaintenant, on va ajuster différents modèles à tester :\n\n- mod1 : $y = \\beta_0 + \\beta_1x + \\mathcal{E}$\n\n- mod2 : $y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\mathcal{E}$\n\n- mod3 : $y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\mathcal{E}$\n\n- mod4 : $y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\beta_4x^4 + \\mathcal{E}$\n\n\nOn va donc commencer par compléter notre data frame avec des variables correspondant à $x^2$, $x^3$ et $x^4$. Puis nous pourrons ajuster les différents modèles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSimu_data_complete <- cbind(Simu_data, x^2, x^3, x^4)\ncolnames(Simu_data_complete) <- c(\"y\", \"x1\", \"x2\", \"x3\", \"x4\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(y ~ x1, Simu_data_complete)\nmod2 <- lm(y ~ x1 + x2, Simu_data_complete)\nmod3 <- lm(y ~ x1 + x2 + x3, Simu_data_complete)\nmod4 <- lm(y ~ x1 + x2 + x3 + x4, Simu_data_complete)\n```\n:::\n\n\n::: callout-note\nThéoriquement, on est dans une situation où le $R^2$ pour le modèle 1 est égale à la corrélation de pearson au carré de $x$ et $y$.\n\n- $R^2 =$ 0.116\n\n- $\\rho^2 =$ 0.116\n:::\n\nEnsuite, partir du *summary()* et de différentes fonctions *R*, on est capable capable d'obtenir différents critères permettant de comparer la qualité de nos modèles.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels <- list(mod1, mod2, mod3, mod4)\nmodel_names <- c(\"mod1\", \"mod2\", \"mod3\", \"mod4\")\n\nresults <- data.frame(\n  Model = model_names,\n  R2 = unlist(lapply(models, function(m) round(summary(m)$r.squared, 3))),\n  R2adj = unlist(lapply(models, function(m) round(summary(m)$adj.r.squared, 3))),\n  AIC = unlist(lapply(models, function(m) round(AIC(m), 1))),\n  BIC = unlist(lapply(models, function(m) round(BIC(m), 1)))\n)\n\n\nresults %>% DT::datatable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-5ca2671e1d845ab306c1\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-5ca2671e1d845ab306c1\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\"],[\"mod1\",\"mod2\",\"mod3\",\"mod4\"],[0.116,0.913,0.913,0.913],[0.115,0.913,0.913,0.913],[5152.3,2832.9,2834.5,2835.6],[5167.1,2852.5,2859,2865.1]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Model<\\/th>\\n      <th>R2<\\/th>\\n      <th>R2adj<\\/th>\\n      <th>AIC<\\/th>\\n      <th>BIC<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Model\",\"targets\":1},{\"name\":\"R2\",\"targets\":2},{\"name\":\"R2adj\",\"targets\":3},{\"name\":\"AIC\",\"targets\":4},{\"name\":\"BIC\",\"targets\":5}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\nOn peut voir ici que c'est *mod2* qui ressort comme étant le meilleur modèle avec de forte valeur de $R^2$ et $R^2_{adjusted}$ puis des critères AIC et BIC minimisés.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = Simu_data_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1117 -0.7078 -0.0106  0.6605  3.3936 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.03778    0.03820   0.989    0.323    \nx1           1.02939    0.03105  33.151   <2e-16 ***\nx2          -2.01737    0.02107 -95.725   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.995 on 997 degrees of freedom\nMultiple R-squared:  0.9133,\tAdjusted R-squared:  0.9131 \nF-statistic:  5248 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nOn constate en plus avec le *summary()* que toutes nos variables sont significatives sauf l'*intercept*.\n\n\n# Validation croisée\n\n::: panel-tabset \n\n## Rappel validation croisée \n\nLe principe de la validation croisée (*cross validation*) est d’estimer le risque de prédiction en confrontant notre modèle à un échantillon test qui n’a pas été utilisé pour l’ajustement de celui ci.\\\nLa validation croisée possède ainsi de nombreux avantages mais a comme principal inconvénient son temps de calcul qui peut rapidement devenir important.\\\n\nDans le cas du K-fold, on coupe l’échantillon de taille $n$ en environ K parties égales. Ensuite, on fait l'ajustement du modèle sur K-1 échantillons et on garde le K-ième comme échantillon test pour calculer l'erreur de prédiction. On répète alors le procédé de telle sorte à ce que chaque échantillon serve une fois de test. Cela nous fait donc calculer K erreurs.\n\nA savoir que selon la valeur de K, on peut se retrouver dans des cas particuliers très utilisés.\n\n- Lorsque K = n, il s’agit de la procédure Leave One Out (LOO).\n\n- Lorsque K = 2, on est sur une procédure Hold out ou testset \n\nLa validation croisée par K-fold est donc un outil couramment utilisé. Le choix de K est quant à lui très important et il faut penser que si K est trop grand, le biais sera faible mais à contrario, la variance deviendra très grande. Par contre, si K est trop petit, l’estimation risque de posséder un grand biais puisque notre taille d’échantillon test sera beaucoup plus grande que celle de l’échantillon d’apprentissage. On a donc ici un bel exemple de compromis entre biais et variance pour trouver le K le plus judicieux.\n\n## Validation \"manuelle\"\n\nDans un premier temps et pour bien comprendre la méthode, on va utiliser deux fonctions (construite pour l'occasion et dont le code se trouve dans la partie **fonction** du **Setup**) permettant d’estimer l’erreur test par une validation\ncroisée *LOO* (*Leave-one-out*) pour un modèle ajusté par la fonction *lm* :\n\n- la première, *loo*, en utilisant le principe général de cette méthode qui nécessite donc l’estimation de n modèles différents \n\n\n- la seconde, *loo2*, en utilisant la formule adaptée à la régression linéaire donnant directement le risque LOO à partir de la seule estimation du modèle complet (on pourra utiliser la fonction *hatvalues*)\n\n\nAinsi, en testant sur nos quatre modèle, on obtient les résultats suivants : \n\n\n::: {.cell}\n\n```{.r .cell-code}\nloo_results <- lapply(models, function(m) get_loo_results(m, loo))\nloo2_results <- lapply(models, function(m) get_loo_results(m, loo2))\n\nresults <- data.frame(\n  Model = model_names,\n  LOO = unlist(lapply(loo_results, function(x) x$result)),       \n  Time_LOO = unlist(lapply(loo_results, function(x) x$time)),    \n  LOO2 = unlist(lapply(loo2_results, function(x) x$result)),     \n  Time_LOO2 = unlist(lapply(loo2_results, function(x) x$time))   \n)\n\nresults %>% DT::datatable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-b57d78979e16a7b99a95\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-b57d78979e16a7b99a95\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\"],[\"mod1\",\"mod2\",\"mod3\",\"mod4\"],[10.182,0.993,0.994,0.995],[0.506,0.5600000000000001,0.651,0.718],[10.182,0.993,0.994,0.995],[0,0.001,0,0]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Model<\\/th>\\n      <th>LOO<\\/th>\\n      <th>Time_LOO<\\/th>\\n      <th>LOO2<\\/th>\\n      <th>Time_LOO2<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Model\",\"targets\":1},{\"name\":\"LOO\",\"targets\":2},{\"name\":\"Time_LOO\",\"targets\":3},{\"name\":\"LOO2\",\"targets\":4},{\"name\":\"Time_LOO2\",\"targets\":5}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n::: success-header\n::: success-icon\n::: \nRésultats\n:::\n\n::: success\nOn voit que les résultats donnés par nos deux fonctions coincident bien. Mais la première semble tout de même plus lente pour le calcul.\n\nEt en terme de qualité de modèle, c'est bien mod2 qui minimise l'erreur de la *cross validation* par *loo*.\n:::\n\n\n\n\n\n## Library *boot*\n\n\nOn va donc maintenant utiliser la fonction *cv.glm* de la library \\{ boot \\} permettant d’estimer l’erreur test par validation croisée *K-fold*. Cela va nécessité de recalculer le modèle mais cette fois ci avec la fonction *glm* en spécifiant que l'on veut un modèle gaussien (ce qui nous donnera le même résultat qu'avec *lm*).\n\n::: callout-note\nOn spécifi ici dans *glm* que le modèle est gaussien mais dans la pratique ce n'est pas nécéssaire pusiqu'il s'agit de la valeur par défaut de la fonction.\n:::\n\nDe plus, nous utiliserons $K=10$ qui est une valeurs assez communément utilisé sachant que si on voulait reproduire la procédure *LOO* il faudrait utiliser $K=n$ (cf rappel).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1_glm <- glm(formula = formula(mod1) ,\n                family = gaussian,\n                data = Simu_data_complete)\nmod2_glm <- glm(formula = formula(mod2) ,\n                family = gaussian,\n                data = Simu_data_complete)\nmod3_glm <- glm(formula = formula(mod3) ,\n                family = gaussian,\n                data = Simu_data_complete)\nmod4_glm <- glm(formula = formula(mod4) ,\n                family = gaussian,\n                data = Simu_data_complete)\n\n\ncvmod1 <- cv.glm(data = Simu_data_complete, glmfit = mod1_glm, K = 10)\ncvmod2 <- cv.glm(data = Simu_data_complete, glmfit = mod2_glm, K = 10)\ncvmod3 <- cv.glm(data = Simu_data_complete, glmfit = mod3_glm, K = 10)\ncvmod4 <- cv.glm(data = Simu_data_complete, glmfit = mod4_glm, K = 10) \n\nresults <- data.frame(\n  Model = model_names,\n  CV_Mean = round( c(mean(cvmod1$delta), mean(cvmod2$delta), mean(cvmod3$delta), mean(cvmod4$delta)), 3),\n  LOO2 = c(loo2(mod1), loo2(mod2), loo2(mod3), loo2(mod4))\n)\n\n\nresults %>% DT::datatable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-00a30d75ffe3be7361ca\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-00a30d75ffe3be7361ca\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\",\"3\",\"4\"],[\"mod1\",\"mod2\",\"mod3\",\"mod4\"],[10.239,0.994,0.996,0.995],[10.182,0.993,0.994,0.995]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Model<\\/th>\\n      <th>CV_Mean<\\/th>\\n      <th>LOO2<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Model\",\"targets\":1},{\"name\":\"CV_Mean\",\"targets\":2},{\"name\":\"LOO2\",\"targets\":3}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n\n\n::: success-header\n::: success-icon\n::: \nRésultats\n:::\n\n::: success\nOn voit qu'en moyenne *cv.glm* nous donne des résultats qui sont du même ordre de grandeur que notre fonction *loo2*.\n\nEt en terme de qualité de modèle, c'est bien mod2 qui minimise l'erreur de la *cross validation*.\n:::\n\n\n:::\n\n# Ajustement du meilleur modèle\n\nD'après tout ce que l'on a pu voir durant cette étude, jusqu'à présent le meilleur modèle semble être mod2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = Simu_data_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1117 -0.7078 -0.0106  0.6605  3.3936 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.03778    0.03820   0.989    0.323    \nx1           1.02939    0.03105  33.151   <2e-16 ***\nx2          -2.01737    0.02107 -95.725   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.995 on 997 degrees of freedom\nMultiple R-squared:  0.9133,\tAdjusted R-squared:  0.9131 \nF-statistic:  5248 on 2 and 997 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nDe manière naturelle, l'*intercept* ne semblant pas significatif il conviendrait de tester sans.\n\nAinsi nous allons essayer le modèle $y \\sim x + x^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2_without_intercept <- lm(y ~ 0 + x1 + x2, Simu_data_complete)\nmod2_without_intercept %>% summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ 0 + x1 + x2, data = Simu_data_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0785 -0.6834  0.0212  0.6855  3.4288 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|)    \nx1  1.02988    0.03105   33.17   <2e-16 ***\nx2 -2.00555    0.01736 -115.54   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.995 on 998 degrees of freedom\nMultiple R-squared:  0.9364,\tAdjusted R-squared:  0.9363 \nF-statistic:  7352 on 2 and 998 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\nOn voit un modèle avec de très bon résultats et qui à toutes ces variables significatives. Comparons le avec *mod2* : \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodels <- list(mod2, mod2_without_intercept)\nmodel_names <- c(\"mod2\", \"mod2_without_intercept\")\n\nresults <- data.frame(\n  Model = model_names,\n  R2 = unlist(lapply(models, function(m)\n    round(summary(m)$r.squared, 3))),\n  R2adj = unlist(lapply(models, function(m)\n    round(summary(m)$adj.r.squared, 3))),\n  AIC = unlist(lapply(models, function(m)\n    round(AIC(\n      m\n    ), 1))),\n  BIC = unlist(lapply(models, function(m)\n    round(BIC(\n      m\n    ), 1))),\n  LOO2 = c(loo2(mod2), loo2(mod2_without_intercept)),\n  CV_Mean = round(c(\n    mean(cv.glm(\n      data = Simu_data_complete,\n      glmfit = glm(\n        formula = formula(mod2),\n        data = Simu_data_complete\n      ),\n      K = 10\n    )$delta), mean(cv.glm(\n      data = Simu_data_complete,\n      glmfit = glm(\n        formula = formula(mod2_without_intercept),\n        data = Simu_data_complete\n      ),\n      K = 10\n    )$delta)\n  ), 3)\n)\n\n\nresults %>% DT::datatable()\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"datatables html-widget html-fill-item\" id=\"htmlwidget-cb474ddd59216635016d\" style=\"width:100%;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-cb474ddd59216635016d\">{\"x\":{\"filter\":\"none\",\"vertical\":false,\"data\":[[\"1\",\"2\"],[\"mod2\",\"mod2_without_intercept\"],[0.913,0.9360000000000001],[0.913,0.9360000000000001],[2832.9,2831.8],[2852.5,2846.6],[0.993,0.992],[0.991,0.993]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>Model<\\/th>\\n      <th>R2<\\/th>\\n      <th>R2adj<\\/th>\\n      <th>AIC<\\/th>\\n      <th>BIC<\\/th>\\n      <th>LOO2<\\/th>\\n      <th>CV_Mean<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[2,3,4,5,6,7]},{\"orderable\":false,\"targets\":0},{\"name\":\" \",\"targets\":0},{\"name\":\"Model\",\"targets\":1},{\"name\":\"R2\",\"targets\":2},{\"name\":\"R2adj\",\"targets\":3},{\"name\":\"AIC\",\"targets\":4},{\"name\":\"BIC\",\"targets\":5},{\"name\":\"LOO2\",\"targets\":6},{\"name\":\"CV_Mean\",\"targets\":7}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n\n::: success-header\n::: success-icon\n::: \nRésultats\n:::\n\n::: success\nOn voit bien avec tout nos critère qu'enlever l'*intercept* à apporté une amélioration à notre modèle.\n\nAinsi, si l'on se base sur les résultat de ce nouveau modèle, on obtient la relation linéaire suivante :\n\n- $y =$ 1.03$x$ + -2.006$x^2 + \\mathcal{E}$\n\nAlors que pour rappel, on a le lien linéaire théorique qui est:\n\n- $y = x - 2x^2 + \\mathcal{E}$.\n\nDonc je pense que l'on peut dire sans prendre trop de risques que notre estimation et notre méthode de sélection est bonne.\n:::\n\n\n# Conclusion\n\nOn voit qu'on a bien réussi à retrouver le lien théorique via le test de pluseurs modèle et l'utilisation de plusieurs critères couplés à de la validation croisée pour affiner notre recherche du modèle le mieux ajusté.\n\nAinsi, il est important d'avancer étape par étape car ici chaques étapes était importante pour trouver le meilleur modèle. Et ici on se basait sur un modèle généré par nous même et possédant un lien linéaire bien défini ce qui nous permettait tout de même si bien orienter nos recherche. La réalité nous offre souvent des situations plus compliquées et tout ces outils deviennent donc cruciaux pour bien avancer.\n\n# Session info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessioninfo::session_info(pkgs = \"attached\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-02-21\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n boot      * 1.3-31  2024-08-28 [4] CRAN (R 4.3.3)\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n```\n\n\n:::\n:::\n",
    "supporting": [
      "Exercice_03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<link href=\"../site_libs/datatables-css-0.0.0/datatables-crosstalk.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/datatables-binding-0.33/datatables.js\"></script>\n<script src=\"../site_libs/jquery-3.6.0/jquery-3.6.0.min.js\"></script>\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.min.css\" rel=\"stylesheet\" />\n<link href=\"../site_libs/dt-core-1.13.6/css/jquery.dataTables.extra.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/dt-core-1.13.6/js/jquery.dataTables.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
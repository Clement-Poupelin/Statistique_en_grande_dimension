{
  "hash": "a275566a6ab41431073a4d00ad4eaced",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercice Bonus : Ridge vs Lasso\"\nauthor: \"Clément Poupelin\"\ndate: \"2023-2024\"\nformat: \n  html:\n    embed-resources: false\n    toc: true\n    code-fold: true\n    code-summary: \"Show the code\"\n    code-tools: true\n    toc-location: right\n    page-layout: article\n    code-overflow: wrap\ntoc: true\nnumber-sections: false\neditor: visual\ncategories: [\"Bonus\"]\nimage: \"\"\ndescription: \"Comparaison de la regression Ridge et Lasso \"\n---\n\n\n\nSource : https://lrouviere.github.io/TUTO_GRANDE_DIM/correction/03-ridge-lasso.html\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ozone <- read.csv(\"~/1.Workspace/Master_IS/M2/X3MS020_Statistique_en_grande_dimension/ozone.txt\", sep=\"\")\nozone <-read.table(\"https://r-stat-sc-donnees.github.io/ozone.txt\", header=TRUE)\nhead(ozone)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15 maxO3v\n20010601    87 15.6 18.5 18.4   4    4    8  0.6946 -1.7101 -0.6946     84\n20010602    82 17.0 18.4 17.7   5    5    7 -4.3301 -4.0000 -3.0000     87\n20010603    92 15.3 17.6 19.5   2    5    4  2.9544  1.8794  0.5209     82\n20010604   114 16.2 19.7 22.5   1    1    0  0.9848  0.3473 -0.1736     92\n20010605    94 17.4 20.5 20.4   8    8    7 -0.5000 -2.9544 -4.3301    114\n20010606    80 17.7 19.8 18.3   6    6    7 -5.6382 -5.0000 -6.0000     94\n          vent pluie\n20010601  Nord   Sec\n20010602  Nord   Sec\n20010603   Est   Sec\n20010604  Nord   Sec\n20010605 Ouest   Sec\n20010606 Ouest Pluie\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(ozone)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     maxO3              T9             T12             T15       \n Min.   : 42.00   Min.   :11.30   Min.   :14.00   Min.   :14.90  \n 1st Qu.: 70.75   1st Qu.:16.20   1st Qu.:18.60   1st Qu.:19.27  \n Median : 81.50   Median :17.80   Median :20.55   Median :22.05  \n Mean   : 90.30   Mean   :18.36   Mean   :21.53   Mean   :22.63  \n 3rd Qu.:106.00   3rd Qu.:19.93   3rd Qu.:23.55   3rd Qu.:25.40  \n Max.   :166.00   Max.   :27.00   Max.   :33.50   Max.   :35.50  \n      Ne9             Ne12            Ne15           Vx9         \n Min.   :0.000   Min.   :0.000   Min.   :0.00   Min.   :-7.8785  \n 1st Qu.:3.000   1st Qu.:4.000   1st Qu.:3.00   1st Qu.:-3.2765  \n Median :6.000   Median :5.000   Median :5.00   Median :-0.8660  \n Mean   :4.929   Mean   :5.018   Mean   :4.83   Mean   :-1.2143  \n 3rd Qu.:7.000   3rd Qu.:7.000   3rd Qu.:7.00   3rd Qu.: 0.6946  \n Max.   :8.000   Max.   :8.000   Max.   :8.00   Max.   : 5.1962  \n      Vx12             Vx15            maxO3v           vent          \n Min.   :-7.878   Min.   :-9.000   Min.   : 42.00   Length:112        \n 1st Qu.:-3.565   1st Qu.:-3.939   1st Qu.: 71.00   Class :character  \n Median :-1.879   Median :-1.550   Median : 82.50   Mode  :character  \n Mean   :-1.611   Mean   :-1.691   Mean   : 90.57                     \n 3rd Qu.: 0.000   3rd Qu.: 0.000   3rd Qu.:106.00                     \n Max.   : 6.578   Max.   : 5.000   Max.   :166.00                     \n    pluie          \n Length:112        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(psych)\npairs.panels(ozone)\n```\n\n::: {.cell-output-display}\n![](Exercice_6Bonus_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\nozone.X <- model.matrix(maxO3~.,data=ozone)[,-1] #  codage des variables qualitatives avec la fonction model.matrix\nozone.Y <- ozone$maxO3\n\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLe chargement a nécessité le package : Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\nmod.R <- glmnet(ozone.X, ozone.Y, alpha=0) ## Ridge \n\nmod.L <- glmnet(ozone.X, ozone.Y, alpha=1) ## Lasso\n\n# Par défaut standardize = TRUE, intercept = TRUE\n\n## Analyse Modèle Ridge\nmod.R$lambda |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 22007.27 20052.20 18270.82 16647.69 15168.76 13821.21\n```\n\n\n:::\n\n```{.r .cell-code}\n# When alpha=0, the largest lambda reported does not quite give \n# the zero coefficients reported (lambda=inf would in principle).\n# Instead, the largest lambda for alpha=0.001 is used, and the sequence \n# of lambda values is derived from this.\n\n\nmod.R$beta[,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           T9           T12           T15           Ne9          Ne12 \n 6.376767e-36  5.523924e-36  4.867402e-36 -6.821464e-36 -7.994984e-36 \n         Ne15           Vx9          Vx12          Vx15        maxO3v \n-5.839057e-36  5.706014e-36  4.387350e-36  3.970583e-36  6.892387e-37 \n     ventNord     ventOuest       ventSud      pluieSec \n-5.830507e-36 -1.022483e-35  1.519222e-35  2.772246e-35 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Our coefficients\n\npar(mfrow=c(1,2))\nplot(mod.R,label=TRUE)  \n# lecture du graphe : \n#   - chaque courbe c'est lévolution d'un beta\n#   - à droite on à les valeurs de beta MCO \n#   - à gauche c'est quand lambda augmente, on tend vers 0\n\nplot(mod.R,xvar=\"lambda\",label=TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercice_6Bonus_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n## Analyse Modèle Lasso\nmod.L$lambda |> head()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 22.00727 20.05220 18.27082 16.64769 15.16876 13.82121\n```\n\n\n:::\n\n```{.r .cell-code}\nmod.L$beta[,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       T9       T12       T15       Ne9      Ne12      Ne15       Vx9      Vx12 \n        0         0         0         0         0         0         0         0 \n     Vx15    maxO3v  ventNord ventOuest   ventSud  pluieSec \n        0         0         0         0         0         0 \n```\n\n\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,2))\nplot(mod.L,label=TRUE)  \nplot(mod.L,xvar=\"lambda\",label=TRUE)\n```\n\n::: {.cell-output-display}\n![](Exercice_6Bonus_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\npar(mfrow=c(1,1))\n\n####\n# Sélection des paramètres de régularisation ####\n\nridgeCV <- cv.glmnet(ozone.X, ozone.Y, alpha=0)\nplot(ridgeCV)\n```\n\n::: {.cell-output-display}\n![](Exercice_6Bonus_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# abline(v=log(ridgeCV$lambda.1se), col='red')\n# abline(v=log(ridgeCV$lambda.min), col='red')\n\n# On visualise les erreurs quadratiques calculées \n# par validation croisée 10 blocs en fonction de lambda (échelle log)\n\n# Deux traites verticaux :\n#   - celui de gauche correspond à la valeur de `lambda`\n#     qui minimise l’erreur quadratique ;\n# \n#   - celui de droite correspond à la plus grande valeur de `lambda` \n#     telle que l’erreur ne dépasse pas \n#     l’erreur minimale + 1 écart-type estimé de cette erreur.\n\n\n# D’un point de vu pratique, cela signifie que l’utilisateur\n# peut choisir n’importe quelle valeur de lambda entre \n# les deux traits verticaux. Si on veut diminuer \n# la complexité du modèle on choisira la valeur de droite.\n# On peut obtenir ces deux valeurs \n\nridgeCV$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8.884372\n```\n\n\n:::\n\n```{.r .cell-code}\nridgeCV$lambda.1se\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 57.1094\n```\n\n\n:::\n\n```{.r .cell-code}\nlassoCV <- cv.glmnet(ozone.X, ozone.Y, alpha=1)\nplot(lassoCV)\n```\n\n::: {.cell-output-display}\n![](Exercice_6Bonus_files/figure-html/unnamed-chunk-1-5.png){width=672}\n:::\n\n```{.r .cell-code}\n# abline(v=log(lassoCV$lambda.1se), col='red')\n# abline(v=log(lassoCV$lambda.min), col='red')\n\nlassoCV$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.482003\n```\n\n\n:::\n\n```{.r .cell-code}\nlassoCV$lambda.1se\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.967084\n```\n\n\n:::\n\n```{.r .cell-code}\n####\n# Prédiction de la variable cible pour de nouveaux individus ####\n\n# Première approche :\n# réajuster le modèle sur toutes les données pour la valeur \n# de lambda sélectionnée.\n# Cette étape est en réalité déjà effectuée par la fonction cv.glmnet.\n# Il suffit par conséquent d’appliquer la fonction predict à l’objet \n# obtenu avec cv.glmnet en spécifiant la valeur de lambda souhaitée.\npredict(ridgeCV, newx = ozone.X[50:51,],s=\"lambda.min\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         lambda.min\n20010723   89.87168\n20010724   96.76818\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(ridgeCV, newx = ozone.X[50:51,],s=\"lambda.1se\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         lambda.1se\n20010723   93.50409\n20010724   96.00444\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(lassoCV, newx = ozone.X[50:51,],s=\"lambda.min\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         lambda.min\n20010723   87.19354\n20010724   97.97744\n```\n\n\n:::\n\n```{.r .cell-code}\npredict(lassoCV, newx = ozone.X[50:51,],s=\"lambda.1se\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         lambda.1se\n20010723   87.44713\n20010724   95.61077\n```\n\n\n:::\n\n```{.r .cell-code}\n# Comparaison performances MCO, ridge et lasso ####\n\n# validation croisée pour comparer les performances des estimateurs\n# MCO, ridge et lasso.\n# On pourra utiliser les données ozone_complet.txt\n# qui contiennent plus d’individus et de variables.\n\n# ozone1 <- read.csv(\"~/1. Workspace/Master IS/M2/X3MS020 Statistique en grande dimension/ozone_complet.txt\", sep=\";\") |> na.omit()\nozone1 <-read.table(\"https://r-stat-sc-donnees.github.io/ozone.txt\", header=TRUE) |> na.omit()\nozone1.X <- model.matrix(maxO3~., data=ozone1)[,-1]\nozone1.Y <- ozone1$maxO3\n\nlibrary(tibble)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttachement du package : 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n\n```{.r .cell-code}\ncv.ridge.lasso <- function(data,form){\n  set.seed(1234)\n  data.X <- model.matrix(form,data=data)[,-1]\n  data.Y <- data$maxO3\n  blocs <- caret::createFolds(1:nrow(data),k=10)\n  prev <- matrix(0,ncol=3,nrow=nrow(data)) |> as.data.frame()\n  names(prev) <- c(\"lin\",\"ridge\",\"lasso\")\n  for (k in 1:10){\n    app <- data[-blocs[[k]],]\n    test <- data[blocs[[k]],]\n    app.X <- data.X[-blocs[[k]],]\n    app.Y <- data.Y[-blocs[[k]]]\n    test.X <- data.X[blocs[[k]],]\n    test.Y <- data.Y[blocs[[k]]]\n    ridge <- cv.glmnet(app.X,app.Y,alpha=0)\n    lasso <- cv.glmnet(app.X,app.Y,alpha=1)\n    lin <- lm(form,data=app)\n    prev[blocs[[k]],] <- tibble(lin=predict(lin,newdata=test),\n                                ridge=as.vector(predict(ridge,newx=test.X)),\n                                lasso=as.vector(predict(lasso,newx=test.X)))\n  }\n  err <- prev |> mutate(obs=data$maxO3) |> summarise_at(1:3,~mean((obs-.)^2))\n  return(err)\n}\n\ncv.ridge.lasso(ozone1, form=formula(maxO3~.))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       lin    ridge    lasso\n1 247.4596 271.8111 272.9936\n```\n\n\n:::\n\n```{.r .cell-code}\n# On remarque que les approches régularisées \n# n’apportent rien par rapport aux estimateurs MCO ici.\n# Ceci peut s’expliquer par le fait que le nombre de variables\n# n’est pas très important.\n\n# Considérons toutes les interactions d’ordre 2\ncv.ridge.lasso(ozone1, form=formula(maxO3~.^2))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in predict.lm(lin, newdata = test): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(lin, newdata = test): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(lin, newdata = test): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(lin, newdata = test): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(lin, newdata = test): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(lin, newdata = test): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(lin, newdata = test): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\nWarning in predict.lm(lin, newdata = test): prediction from rank-deficient fit;\nattr(*, \"non-estim\") has doubtful cases\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       lin    ridge    lasso\n1 196622.7 335.9692 268.1647\n```\n\n\n:::\n\n```{.r .cell-code}\n# Les méthodes régularisées permettent ici de diminuer\n# les erreurs quadratiques de manière intéressante.\n# Cela vient certainement du fait du nombre de \n# variables explicatives qui est beaucoup plus \n# important lorsqu’on prend en compte toutes \n# les interactions d’ordre 2, nous en avons en effet 253 :\nozone2.X <- model.matrix(maxO3~.^2,data=ozone1)[,-1]\ndim(ozone2.X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 112 102\n```\n\n\n:::\n:::\n",
    "supporting": [
      "Exercice_6Bonus_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
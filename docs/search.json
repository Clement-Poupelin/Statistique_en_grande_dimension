[
  {
    "objectID": "posts/Exercice_Bonus.html",
    "href": "posts/Exercice_Bonus.html",
    "title": "Exercice Bonus : Ridge vs Lasso",
    "section": "",
    "text": "Source : https://lrouviere.github.io/TUTO_GRANDE_DIM/correction/03-ridge-lasso.html\n\n\nCode\n# ozone &lt;- read.csv(\"~/1.Workspace/Master_IS/M2/X3MS020_Statistique_en_grande_dimension/ozone.txt\", sep=\"\")\nozone &lt;-read.table(\"https://r-stat-sc-donnees.github.io/ozone.txt\", header=TRUE)\nhead(ozone)\n\n\n         maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15 maxO3v\n20010601    87 15.6 18.5 18.4   4    4    8  0.6946 -1.7101 -0.6946     84\n20010602    82 17.0 18.4 17.7   5    5    7 -4.3301 -4.0000 -3.0000     87\n20010603    92 15.3 17.6 19.5   2    5    4  2.9544  1.8794  0.5209     82\n20010604   114 16.2 19.7 22.5   1    1    0  0.9848  0.3473 -0.1736     92\n20010605    94 17.4 20.5 20.4   8    8    7 -0.5000 -2.9544 -4.3301    114\n20010606    80 17.7 19.8 18.3   6    6    7 -5.6382 -5.0000 -6.0000     94\n          vent pluie\n20010601  Nord   Sec\n20010602  Nord   Sec\n20010603   Est   Sec\n20010604  Nord   Sec\n20010605 Ouest   Sec\n20010606 Ouest Pluie\n\n\nCode\nsummary(ozone)\n\n\n     maxO3              T9             T12             T15       \n Min.   : 42.00   Min.   :11.30   Min.   :14.00   Min.   :14.90  \n 1st Qu.: 70.75   1st Qu.:16.20   1st Qu.:18.60   1st Qu.:19.27  \n Median : 81.50   Median :17.80   Median :20.55   Median :22.05  \n Mean   : 90.30   Mean   :18.36   Mean   :21.53   Mean   :22.63  \n 3rd Qu.:106.00   3rd Qu.:19.93   3rd Qu.:23.55   3rd Qu.:25.40  \n Max.   :166.00   Max.   :27.00   Max.   :33.50   Max.   :35.50  \n      Ne9             Ne12            Ne15           Vx9         \n Min.   :0.000   Min.   :0.000   Min.   :0.00   Min.   :-7.8785  \n 1st Qu.:3.000   1st Qu.:4.000   1st Qu.:3.00   1st Qu.:-3.2765  \n Median :6.000   Median :5.000   Median :5.00   Median :-0.8660  \n Mean   :4.929   Mean   :5.018   Mean   :4.83   Mean   :-1.2143  \n 3rd Qu.:7.000   3rd Qu.:7.000   3rd Qu.:7.00   3rd Qu.: 0.6946  \n Max.   :8.000   Max.   :8.000   Max.   :8.00   Max.   : 5.1962  \n      Vx12             Vx15            maxO3v           vent          \n Min.   :-7.878   Min.   :-9.000   Min.   : 42.00   Length:112        \n 1st Qu.:-3.565   1st Qu.:-3.939   1st Qu.: 71.00   Class :character  \n Median :-1.879   Median :-1.550   Median : 82.50   Mode  :character  \n Mean   :-1.611   Mean   :-1.691   Mean   : 90.57                     \n 3rd Qu.: 0.000   3rd Qu.: 0.000   3rd Qu.:106.00                     \n Max.   : 6.578   Max.   : 5.000   Max.   :166.00                     \n    pluie          \n Length:112        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nCode\nlibrary(psych)\n\n\nWarning: le package 'psych' a été compilé avec la version R 4.2.3\n\n\nCode\npairs.panels(ozone)\n\n\n\n\n\nCode\nozone.X &lt;- model.matrix(maxO3~.,data=ozone)[,-1] #  codage des variables qualitatives avec la fonction model.matrix\nozone.Y &lt;- ozone$maxO3\n\nlibrary(glmnet)\n\n\nWarning: le package 'glmnet' a été compilé avec la version R 4.2.3\n\n\nLe chargement a nécessité le package : Matrix\n\n\nWarning: le package 'Matrix' a été compilé avec la version R 4.2.3\n\n\nLoaded glmnet 4.1-8\n\n\nCode\nmod.R &lt;- glmnet(ozone.X, ozone.Y, alpha=0) ## Ridge \n\nmod.L &lt;- glmnet(ozone.X, ozone.Y, alpha=1) ## Lasso\n\n# Par défaut standardize = TRUE, intercept = TRUE\n\n## Analyse Modèle Ridge\nmod.R$lambda |&gt; head()\n\n\n[1] 22007.27 20052.20 18270.82 16647.69 15168.76 13821.21\n\n\nCode\n# When alpha=0, the largest lambda reported does not quite give \n# the zero coefficients reported (lambda=inf would in principle).\n# Instead, the largest lambda for alpha=0.001 is used, and the sequence \n# of lambda values is derived from this.\n\n\nmod.R$beta[,1]\n\n\n           T9           T12           T15           Ne9          Ne12 \n 6.376767e-36  5.523924e-36  4.867402e-36 -6.821464e-36 -7.994984e-36 \n         Ne15           Vx9          Vx12          Vx15        maxO3v \n-5.839057e-36  5.706014e-36  4.387350e-36  3.970583e-36  6.892387e-37 \n     ventNord     ventOuest       ventSud      pluieSec \n-5.830507e-36 -1.022483e-35  1.519222e-35  2.772246e-35 \n\n\nCode\n# Our coefficients\n\npar(mfrow=c(1,2))\nplot(mod.R,label=TRUE)  \n# lecture du graphe : \n#   - chaque courbe c'est lévolution d'un beta\n#   - à droite on à les valeurs de beta MCO \n#   - à gauche c'est quand lambda augmente, on tend vers 0\n\nplot(mod.R,xvar=\"lambda\",label=TRUE)\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\n\n\n\n\n\n\n\n## Analyse Modèle Lasso\nmod.L$lambda |&gt; head()\n\n\n[1] 22.00727 20.05220 18.27082 16.64769 15.16876 13.82121\n\n\nCode\nmod.L$beta[,1]\n\n\n       T9       T12       T15       Ne9      Ne12      Ne15       Vx9      Vx12 \n        0         0         0         0         0         0         0         0 \n     Vx15    maxO3v  ventNord ventOuest   ventSud  pluieSec \n        0         0         0         0         0         0 \n\n\nCode\npar(mfrow=c(1,2))\nplot(mod.L,label=TRUE)  \nplot(mod.L,xvar=\"lambda\",label=TRUE)\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\n\n####\n# Sélection des paramètres de régularisation ####\n\nridgeCV &lt;- cv.glmnet(ozone.X, ozone.Y, alpha=0)\nplot(ridgeCV)\n\n\n\n\n\nCode\n# abline(v=log(ridgeCV$lambda.1se), col='red')\n# abline(v=log(ridgeCV$lambda.min), col='red')\n\n# On visualise les erreurs quadratiques calculées \n# par validation croisée 10 blocs en fonction de lambda (échelle log)\n\n# Deux traites verticaux :\n#   - celui de gauche correspond à la valeur de `lambda`\n#     qui minimise l’erreur quadratique ;\n# \n#   - celui de droite correspond à la plus grande valeur de `lambda` \n#     telle que l’erreur ne dépasse pas \n#     l’erreur minimale + 1 écart-type estimé de cette erreur.\n\n\n# D’un point de vu pratique, cela signifie que l’utilisateur\n# peut choisir n’importe quelle valeur de lambda entre \n# les deux traits verticaux. Si on veut diminuer \n# la complexité du modèle on choisira la valeur de droite.\n# On peut obtenir ces deux valeurs \n\nridgeCV$lambda.min\n\n\n[1] 9.750588\n\n\nCode\nridgeCV$lambda.1se\n\n\n[1] 43.20116\n\n\nCode\nlassoCV &lt;- cv.glmnet(ozone.X, ozone.Y, alpha=1)\nplot(lassoCV)\n\n\n\n\n\nCode\n# abline(v=log(lassoCV$lambda.1se), col='red')\n# abline(v=log(lassoCV$lambda.min), col='red')\n\nlassoCV$lambda.min\n\n\n[1] 1.230385\n\n\nCode\nlassoCV$lambda.1se\n\n\n[1] 4.967084\n\n\nCode\n####\n# Prédiction de la variable cible pour de nouveaux individus ####\n\n# Première approche :\n# réajuster le modèle sur toutes les données pour la valeur \n# de lambda sélectionnée.\n# Cette étape est en réalité déjà effectuée par la fonction cv.glmnet.\n# Il suffit par conséquent d’appliquer la fonction predict à l’objet \n# obtenu avec cv.glmnet en spécifiant la valeur de lambda souhaitée.\npredict(ridgeCV, newx = ozone.X[50:51,],s=\"lambda.min\")\n\n\n         lambda.min\n20010723   90.10981\n20010724   96.74374\n\n\nCode\npredict(ridgeCV, newx = ozone.X[50:51,],s=\"lambda.1se\")\n\n\n         lambda.1se\n20010723   93.23058\n20010724   96.21185\n\n\nCode\npredict(lassoCV, newx = ozone.X[50:51,],s=\"lambda.min\")\n\n\n         lambda.min\n20010723   87.18235\n20010724   98.23752\n\n\nCode\npredict(lassoCV, newx = ozone.X[50:51,],s=\"lambda.1se\")\n\n\n         lambda.1se\n20010723   87.44713\n20010724   95.61077\n\n\nCode\n# Comparaison performances MCO, ridge et lasso ####\n\n# validation croisée pour comparer les performances des estimateurs\n# MCO, ridge et lasso.\n# On pourra utiliser les données ozone_complet.txt\n# qui contiennent plus d’individus et de variables.\n\n# ozone1 &lt;- read.csv(\"~/1. Workspace/Master IS/M2/X3MS020 Statistique en grande dimension/ozone_complet.txt\", sep=\";\") |&gt; na.omit()\nozone1 &lt;-read.table(\"https://r-stat-sc-donnees.github.io/ozone.txt\", header=TRUE) |&gt; na.omit()\nozone1.X &lt;- model.matrix(maxO3~., data=ozone1)[,-1]\nozone1.Y &lt;- ozone1$maxO3\n\nlibrary(tibble)\n\n\nWarning: le package 'tibble' a été compilé avec la version R 4.2.3\n\n\nCode\nlibrary(dplyr)\n\n\nWarning: le package 'dplyr' a été compilé avec la version R 4.2.3\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\ncv.ridge.lasso &lt;- function(data,form){\n  set.seed(1234)\n  data.X &lt;- model.matrix(form,data=data)[,-1]\n  data.Y &lt;- data$maxO3\n  blocs &lt;- caret::createFolds(1:nrow(data),k=10)\n  prev &lt;- matrix(0,ncol=3,nrow=nrow(data)) |&gt; as.data.frame()\n  names(prev) &lt;- c(\"lin\",\"ridge\",\"lasso\")\n  for (k in 1:10){\n    app &lt;- data[-blocs[[k]],]\n    test &lt;- data[blocs[[k]],]\n    app.X &lt;- data.X[-blocs[[k]],]\n    app.Y &lt;- data.Y[-blocs[[k]]]\n    test.X &lt;- data.X[blocs[[k]],]\n    test.Y &lt;- data.Y[blocs[[k]]]\n    ridge &lt;- cv.glmnet(app.X,app.Y,alpha=0)\n    lasso &lt;- cv.glmnet(app.X,app.Y,alpha=1)\n    lin &lt;- lm(form,data=app)\n    prev[blocs[[k]],] &lt;- tibble(lin=predict(lin,newdata=test),\n                                ridge=as.vector(predict(ridge,newx=test.X)),\n                                lasso=as.vector(predict(lasso,newx=test.X)))\n  }\n  err &lt;- prev |&gt; mutate(obs=data$maxO3) |&gt; summarise_at(1:3,~mean((obs-.)^2))\n  return(err)\n}\n\ncv.ridge.lasso(ozone1, form=formula(maxO3~.))\n\n\n       lin    ridge    lasso\n1 247.4596 271.8111 272.9936\n\n\nCode\n# On remarque que les approches régularisées \n# n’apportent rien par rapport aux estimateurs MCO ici.\n# Ceci peut s’expliquer par le fait que le nombre de variables\n# n’est pas très important.\n\n# Considérons toutes les interactions d’ordre 2\ncv.ridge.lasso(ozone1, form=formula(maxO3~.^2))\n\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\nWarning in predict.lm(lin, newdata = test): les prédictions venant d'un modèle\nde rang faible peuvent être trompeuses\n\n\n       lin    ridge    lasso\n1 196622.7 335.9692 268.1647\n\n\nCode\n# Les méthodes régularisées permettent ici de diminuer\n# les erreurs quadratiques de manière intéressante.\n# Cela vient certainement du fait du nombre de \n# variables explicatives qui est beaucoup plus \n# important lorsqu’on prend en compte toutes \n# les interactions d’ordre 2, nous en avons en effet 253 :\nozone2.X &lt;- model.matrix(maxO3~.^2,data=ozone1)[,-1]\ndim(ozone2.X)\n\n\n[1] 112 102"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistique en grande dimension",
    "section": "",
    "text": "Exercice 9\n\n\n\n\n\n\n\nTP\n\n\n\n\nComparaison de différents modèles de regression\n\n\n\n\n\n\nInvalid Date\n\n\nClément Poupelin\n\n\n\n\n\n\n  \n\n\n\n\nExercice Bonus : Ridge vs Lasso\n\n\n\n\n\n\n\nBonus\n\n\n\n\nComparaison de la regression Ridge et Lasso\n\n\n\n\n\n\nInvalid Date\n\n\nClément Poupelin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Exercice_9.html",
    "href": "posts/Exercice_9.html",
    "title": "Exercice 9",
    "section": "",
    "text": "On souhaite réaliser une petite étude par simulation pour évaluer les qualités respectives de 4 méthodes d’estimation d’un modèle de régression linéaire. On s’intéresse pour chacune d’elle à ses qualités de sélection de variables et à ses qualités prédictives. Le programme SimusReg.R permet de réaliser cette étude. Il contient deux fonctions, Simudata et la fonction principale fun, et un exemple d’utilisation en fin de programme.\n\n\nCode\nn = 100\np = 500\nX = matrix(rnorm(n*p), n, p)\n\n\n\n\nCode\nlibrary(lars)\n\n\nLoaded lars 1.3\n\n\n\nAttachement du package : 'lars'\n\n\nL'objet suivant est masqué depuis 'package:psych':\n\n    error.bars\n\n\nCode\nlibrary(leaps)\nlibrary(glmnet)\n\n\nWarning: le package 'glmnet' a été compilé avec la version R 4.2.3\n\n\nLe chargement a nécessité le package : Matrix\n\n\nWarning: le package 'Matrix' a été compilé avec la version R 4.2.3\n\n\nLoaded glmnet 4.1-8\n\n\nCode\nDataSimulation = function(n,p){\n  if(p &lt; 4){stop(\"p&gt;3 require\")}\n  # We create our matrix of explanatory variables\n  X = matrix(rnorm(n*p), n, p)\n  \n  # We define our coefficients of regression \n  coeff = matrix(0, p)\n  coeff[1:3] = 2\n  \n  # We build our explanatory variables\n  y = X%*%coeff + rnorm(n, sd = 2)\n  return(list(X = X, y = y, coeff = coeff))\n}\n\n\n\nfun = function(n, p, M = 100){ # By default, we make M = 100 simulation \n  \n  ## Initialization \n  #################\n  selec_method1 = NULL; selec_method2 = NULL; selec_method3 = NULL;\n  taille_method1 = NULL; taille_method2 = NULL; taille_method3 = NULL;\n  prev_method1 = NULL; prev_method2 = NULL; prev_method3 = NULL; prev_method4 = NULL;\n  temps1 = NULL; temps2 = NULL; temps3 = NULL; temps4 = NULL;\n  \n  for(i in 1:M){\n    cat(paste(i, \":\")) # counter to see progress\n    \n    # We define our train set\n    datatrain = DataSimulation(n, p)\n    Xtrain = datatrain$X\n    y = datatrain$y\n    coeff = datatrain$coeff\n    \n    # We define our test set\n    datatest = DataSimulation(n, p)\n    Xtest = datatest$X\n    ytest = datatest$y\n    \n    \n    ## Regression \n    #################\n    \n    # Method 1 : Forward-Hybrid with BIC\n    tic = proc.time()\n    tab = data.frame(y = y, X = Xtrain)\n    fit0 = lm(y~1, tab)\n    fit = lm(y~., tab)\n    tmp = step(fit0, scope = formula(fit),\n               k = log(n), # BIC criteria\n               direction = \"both\", # Hybrid\n               trace = 0)\n    noms = sort(names(tmp$model))\n    selec_method1[i] = identical(\n      noms[-length(noms)], sort(paste(\"X.\", which(coeff != 0), sep = \"\"))\n      )\n    taille_method1[i] = length(noms) - 1\n    prev_method1[i] = mean((predict(tmp,data.frame(X = Xtest)) - ytest)^2)\n    tac = proc.time() - tic\n    temps1[i] = tac[3]\n    \n    # Method 2 : Lasso\n    tic = proc.time()\n    cvglm = cv.glmnet(Xtrain, y) # By default we have Lasso\n    lambda = cvglm$lambda.min\n    coef2 = coef(cvglm, s = lambda)[-1]\n    index = which(coef2 != 0) \n    selec_method2[i] = identical(sort(index), which(coeff != 0))\n    taille_method2[i] = length(index)\n    prev_method2[i] = mean((predict(cvglm, Xtest, s = lambda) - ytest)^2)\n    tac = proc.time() - tic\n    temps2[i] = tac[3]\n    \n    # Methods 3 and 4 : Adaptive Lasso and  Gauss-Lasso  \n    if(length(index) == 0){\n      selec_method3[i] = selec_method2[i]\n      taille_method3[i] = taille_method2[i]\n      prev_method3[i] = prev_method2[i]\n      prev_method4[i] = prev_method2[i]}\n    else{\n      # Adaptive Lasso part\n      cvglm = cv.glmnet(Xtrain, y,\n                        penalty.factor = 1/abs(coef2))\n      lambda = cvglm$lambda.min\n      coef3 = coef(cvglm, s = lambda)[-1]\n      index = which(coef3 != 0) \n      selec_method3[i] = identical(sort(index), which(coeff != 0))\n      taille_method3[i] = length(index)\n      prev_method3[i] = mean((predict(cvglm, Xtest, s = lambda) - ytest)^2)\n      tac = proc.time() - tic\n      temps3[i] = tac[3]\n      \n      # Gauss-Lasso part\n      if(length(index) == 0){\n        prev_method4[i] = mean((mean(y) - ytest)^2)}\n      else{\n        tab = data.frame(y = y, X = Xtrain)\n        reg = lm(y~., \n                 data = tab[, c(1, index + 1)])\n        prev_method4[i] = mean((predict(reg, data.frame(X = Xtest)) - ytest)^2)\n        tac = proc.time() - tic\n        temps4[i] = tac[3]\n      }\n    }\n  }\n  \n  ## Results\n  #################\n  res = list(mean(selec_method1), mean(selec_method2), mean(selec_method3), taille_method1, taille_method2, taille_method3, prev_method1, prev_method2, prev_method3, prev_method4, mean(temps1), mean(temps2), mean(temps3), mean(temps4))\n  \n  names(res) = c(\"selec_method1\", \"selec_method2\", \"selec_method3\", \"taille_method1\", \"taille_method2\", \"taille_method3\", \"prev_method1\", \"prev_method2\", \"prev_method3\", \"prev_method4\", \"temps1\", \"temps2\", \"temps3\", \"temps4\")\n  \n  return(res)\n}\n\nfun2 = function(n, p, M = 100){ # By default, we make M = 100 simulation \n  \n  ## Initialization \n  #################\n  selec_method1 = NULL; selec_method2 = NULL; selec_method3 = NULL;\n  taille_method1 = NULL; taille_method2 = NULL; taille_method3 = NULL;\n  prev_method1 = NULL; prev_method2 = NULL; prev_method3 = NULL; prev_method4 = NULL;\n  temps1 = NULL; temps2 = NULL; temps3 = NULL; temps4 = NULL;\n  \n  for(i in 1:M){\n    cat(paste(i, \":\")) # counter to see progress\n    \n    # We define our train set\n    datatrain = DataSimulation(n, p)\n    Xtrain = datatrain$X\n    y = datatrain$y\n    coeff = datatrain$coeff\n    \n    # We define our test set\n    datatest = DataSimulation(n, p)\n    Xtest = datatest$X\n    ytest = datatest$y\n    \n    \n    ## Regression \n    #################\n    \n    # Method 2 : Lasso\n    tic = proc.time()\n    cvglm = cv.glmnet(Xtrain, y) # By default we have Lasso\n    lambda = cvglm$lambda.min\n    coef2 = coef(cvglm, s = lambda)[-1]\n    index = which(coef2 != 0) \n    selec_method2[i] = identical(sort(index), which(coeff != 0))\n    taille_method2[i] = length(index)\n    prev_method2[i] = mean((predict(cvglm, Xtest, s = lambda) - ytest)^2)\n    tac = proc.time() - tic\n    temps2[i] = tac[3]\n    \n    # Methods 3 and 4 : Adaptive Lasso and  Gauss-Lasso  \n    if(length(index) == 0){\n      selec_method3[i] = selec_method2[i]\n      taille_method3[i] = taille_method2[i]\n      prev_method3[i] = prev_method2[i]\n      prev_method4[i] = prev_method2[i]}\n    else{\n      # Adaptive Lasso part\n      cvglm = cv.glmnet(Xtrain, y,\n                        penalty.factor = 1/abs(coef2))\n      lambda = cvglm$lambda.min\n      coef3 = coef(cvglm, s = lambda)[-1]\n      index = which(coef3 != 0) \n      selec_method3[i] = identical(sort(index), which(coeff != 0))\n      taille_method3[i] = length(index)\n      prev_method3[i] = mean((predict(cvglm, Xtest, s = lambda) - ytest)^2)\n      tac = proc.time() - tic\n      temps3[i] = tac[3]\n      \n      # Gauss-Lasso part\n      if(length(index) == 0){\n        prev_method4[i] = mean((mean(y) - ytest)^2)}\n      else{\n        tab = data.frame(y = y, X = Xtrain)\n        reg = lm(y~., \n                 data = tab[, c(1, index + 1)])\n        prev_method4[i] = mean((predict(reg, data.frame(X = Xtest)) - ytest)^2)\n        tac = proc.time() - tic\n        temps4[i] = tac[3]\n      }\n    }\n  }\n  \n  ## Results\n  #################\n  res = list(mean(selec_method1), mean(selec_method2), mean(selec_method3), taille_method1, taille_method2, taille_method3, prev_method1, prev_method2, prev_method3, prev_method4, mean(temps1), mean(temps2), mean(temps3), mean(temps4))\n  \n  names(res) = c(\"selec_method1\", \"selec_method2\", \"selec_method3\", \"taille_method1\", \"taille_method2\", \"taille_method3\", \"prev_method1\", \"prev_method2\", \"prev_method3\", \"prev_method4\", \"temps1\", \"temps2\", \"temps3\", \"temps4\")\n  \n  return(res)\n}\n\n\n\n\nCode\n###### Exemple\na=fun(50,5,100)\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\n\n\nCode\na$selec_method1\n\n\n[1] 0.9\n\n\nCode\na$selec_method2\n\n\n[1] 0.16\n\n\nCode\na$selec_method3\n\n\n[1] 0.7\n\n\nCode\na$taille_method1\n\n\n  [1] 3 3 4 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3\n [38] 3 3 4 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 4 3 3 3 3 3\n [75] 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 4 3 3\n\n\nCode\na$taille_method2\n\n\n  [1] 5 5 5 4 4 5 3 5 3 5 5 5 5 4 4 4 3 4 4 4 5 4 3 4 5 5 4 4 3 5 5 4 4 4 5 4 4\n [38] 5 4 5 5 5 5 4 5 5 4 4 5 3 5 4 5 5 5 3 3 4 5 4 4 5 4 3 5 4 5 3 5 4 4 4 5 5\n [75] 5 3 4 5 3 4 5 5 5 5 5 5 5 5 3 3 5 5 3 4 5 5 4 4 3 5\n\n\nCode\na$taille_method3\n\n\n  [1] 3 3 5 3 3 5 3 3 3 3 3 3 5 3 3 3 3 3 4 3 3 3 3 3 5 3 3 3 3 4 5 4 3 3 4 3 3\n [38] 4 3 4 3 3 3 3 5 5 3 3 3 3 3 4 3 4 3 3 3 3 5 3 4 4 3 3 4 3 3 3 4 4 3 3 3 4\n [75] 4 3 3 4 3 3 3 5 3 4 3 3 4 3 3 3 5 4 3 3 3 3 3 4 3 3\n\n\nCode\nboxplot(sqrt(a$prev_method1),sqrt(a$prev_method2),sqrt(a$prev_method3),sqrt(a$prev_method4),names=c(\"Method1\",\"Method2\",\"Method3\",\"Method4\"),main=\"Title\")\n\n\n\n\n\nCode\nmean(a$prev_method1)\n\n\n[1] 4.317454\n\n\nCode\nmean(a$prev_method2)\n\n\n[1] 4.482734\n\n\nCode\nmean(a$prev_method3)\n\n\n[1] 4.405399\n\n\nCode\nmean(a$prev_method4)\n\n\n[1] 4.397463\n\n\nCode\na$temps1\n\n\n[1] 0.0363\n\n\nCode\na$temps2\n\n\n[1] 0.0887\n\n\nCode\na$temps3\n\n\n[1] 0.1785\n\n\nCode\na$temps4\n\n\n[1] 0.1824\n\n\n\nQuestion 1\nQuel modèle génère la fonction Simudata ? Combien de variables explicatives sont générées ? Parmi elles, lesquelles sont pertinentes pour la modélisation ? Ecrire l’équation du modèle.\n\n\nQuestion 2\nIdentifier les 4 méthodes d’estimation mises en oeuvre dans la fonction fun.\n\n\nQuestion 3\nDétailler les différentes sorties proposées par la fonction fun.\n\n\nQuestion 4\nRemplacer la valeur des options names et title du boxplot réalisé dans l’exemple par les bonnes informations.\n\n\nCode\nboxplot(sqrt(a$prev_method1),sqrt(a$prev_method2),sqrt(a$prev_method3),sqrt(a$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", 100,\"et p=\", 10),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"),\n        ylim = c(1,3))\n\n\n\n\n\n\n\nQuestion 5\nRéaliser une étude comparative des méthodes lorsque \\(n = 50\\) et \\(p = n/10\\), \\(p = n\\), \\(p = 2n\\), \\(p = 10n\\). Pour chaque situation, on considèrera \\(100\\) simulations afin de calculer les différents critères. On synthétisera les résultats en terme de qualité de sélection, nombre de variables sélectionnées, erreurs de prévision et temps de calcul.\n\n\nCode\n#parallelisation\nfuture::plan(multisession, workers = 2)\n\nn = 50\np_list = c(n/10, n, 2*n, 10*n)\n\nr_cas1 = fun(n, p_list[1])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas2 = fun(n, p_list[2])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas3 = fun(n, p_list[3])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas4 = fun(n, p_list[4],1)\n\n\n1 :\n\n\nCode\nr_cas4bis = fun2(n, p_list[4])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\n# quit parallelisation\nfuture::plan(\"sequential\")\n\n\n\n\nCode\nres_cas1 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas1$selec_method1,r_cas1$selec_method2,r_cas1$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas1$taille_method1),mean(r_cas1$taille_method2),mean(r_cas1$taille_method3),NA),\n  Prevision_error = c(mean(r_cas1$prev_method1),mean(r_cas1$prev_method2),mean(r_cas1$prev_method3),mean(r_cas1$prev_method4)),\n  Running_time = c(r_cas1$temps1,r_cas1$temps2,r_cas1$temps3,r_cas1$temps4)\n)\nt(res_cas1)\n\n\n                     [,1]       [,2]       [,3]               [,4]         \nMethod               \"Forward\"  \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.95\"     \"0.24\"     \"0.76\"             NA           \nMean_nb_selected_var \"3.05\"     \"4.19\"     \"3.29\"             NA           \nPrevision_error      \"4.267999\" \"4.372448\" \"4.311278\"         \"4.298561\"   \nRunning_time         \"0.0395\"   \"0.0926\"   \"0.1862\"           \"0.1901\"     \n\n\nCode\nboxplot(sqrt(r_cas1$prev_method1),sqrt(r_cas1$prev_method2),sqrt(r_cas1$prev_method3),sqrt(r_cas1$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[1]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas2 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas2$selec_method1,r_cas2$selec_method2,r_cas2$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas2$taille_method1),mean(r_cas2$taille_method2),mean(r_cas2$taille_method3),NA),\n  Prevision_error = c(mean(r_cas2$prev_method1),mean(r_cas2$prev_method2),mean(r_cas2$prev_method3),mean(r_cas2$prev_method4)),\n  Running_time = c(r_cas2$temps1,r_cas2$temps2,r_cas2$temps3,r_cas2$temps4)\n)\nt(res_cas2)\n\n\n                     [,1]       [,2]       [,3]               [,4]         \nMethod               \"Forward\"  \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.06\"     \"0.01\"     \"0.09\"             NA           \nMean_nb_selected_var \" 7.92\"    \"12.88\"    \" 8.17\"            NA           \nPrevision_error      \"7.050225\" \"5.609938\" \"5.487207\"         \"6.401796\"   \nRunning_time         \"0.3002\"   \"0.2568\"   \"0.3666\"           \"0.3720\"     \n\n\nCode\nboxplot(sqrt(r_cas2$prev_method1),sqrt(r_cas2$prev_method2),sqrt(r_cas2$prev_method3),sqrt(r_cas2$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[2]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas3 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas3$selec_method1,r_cas3$selec_method2,r_cas3$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas3$taille_method1),mean(r_cas3$taille_method2),mean(r_cas3$taille_method3),NA),\n  Prevision_error = c(mean(r_cas3$prev_method1),mean(r_cas3$prev_method2),mean(r_cas3$prev_method3),mean(r_cas3$prev_method4)),\n  Running_time = c(r_cas3$temps1,r_cas3$temps2,r_cas3$temps3,r_cas3$temps4)\n)\nt(res_cas3)\n\n\n                     [,1]        [,2]        [,3]               [,4]         \nMethod               \"Forward\"   \"Lasso\"     \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.00\"      \"0.00\"      \"0.06\"             NA           \nMean_nb_selected_var \"40.57\"     \"15.80\"     \" 9.09\"            NA           \nPrevision_error      \"16.207790\" \" 6.138163\" \" 5.738140\"        \" 6.723044\"  \nRunning_time         \"3.0726\"    \"0.2392\"    \"0.3635\"           \"0.3691\"     \n\n\nCode\nboxplot(sqrt(r_cas3$prev_method1),sqrt(r_cas3$prev_method2),sqrt(r_cas3$prev_method3),sqrt(r_cas3$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[3]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas4 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas4$selec_method1,r_cas4$selec_method2,r_cas4$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas4$taille_method1),mean(r_cas4$taille_method2),mean(r_cas4$taille_method3),NA),\n  Prevision_error = c(mean(r_cas4$prev_method1),mean(r_cas4$prev_method2),mean(r_cas4$prev_method3),mean(r_cas4$prev_method4)),\n  Running_time = c(r_cas4$temps1,r_cas4$temps2,r_cas4$temps3,r_cas4$temps4)\n)\nt(res_cas4)\n\n\n                     [,1]        [,2]        [,3]               [,4]         \nMethod               \"Forward\"   \"Lasso\"     \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \" 0\"        \" 0\"        \" 0\"               NA           \nMean_nb_selected_var \"49\"        \"41\"        \"19\"               NA           \nPrevision_error      \"11.762266\" \" 4.734249\" \" 4.562205\"        \" 5.403622\"  \nRunning_time         \"25.65\"     \" 0.19\"     \" 0.36\"            \" 0.38\"      \n\n\nCode\nboxplot(sqrt(r_cas4$prev_method1),sqrt(r_cas4$prev_method2),sqrt(r_cas4$prev_method3),sqrt(r_cas4$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[3]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas4bis = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas4bis$selec_method1,r_cas4bis$selec_method2,r_cas4bis$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas4bis$taille_method1),mean(r_cas4bis$taille_method2),mean(r_cas4bis$taille_method3),NA),\n  Prevision_error = c(mean(r_cas4bis$prev_method1),mean(r_cas4bis$prev_method2),mean(r_cas4bis$prev_method3),mean(r_cas4bis$prev_method4)),\n  Running_time = c(r_cas4bis$temps1,r_cas4bis$temps2,r_cas4bis$temps3,r_cas4bis$temps4)\n)\n\n\nWarning in mean.default(r_cas4bis$taille_method1): l'argument n'est ni\nnumérique, ni logique : renvoi de NA\n\n\nWarning in mean.default(r_cas4bis$prev_method1): l'argument n'est ni numérique,\nni logique : renvoi de NA\n\n\nCode\nt(res_cas4bis)\n\n\n                     [,1]      [,2]       [,3]               [,4]         \nMethod               \"Forward\" \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection NA        \"0.00\"     \"0.04\"             NA           \nMean_nb_selected_var NA        \"24.87\"    \"12.34\"            NA           \nPrevision_error      NA        \"7.230664\" \"6.781834\"         \"7.861071\"   \nRunning_time         NA        \"0.1812\"   \"0.3352\"           \"0.3494\"     \n\n\nCode\nboxplot(sqrt(r_cas4$prev_method1),sqrt(r_cas4bis$prev_method2),sqrt(r_cas4bis$prev_method3),sqrt(r_cas4bis$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[4]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nQuestion 6\nRéaliser la même étude pour \\(n = 100\\) et \\(p = n/10\\), \\(p = n\\), \\(p = 2n\\), toujours basée sur \\(100\\) simulations dans chaque cas. Considérer de plus le cas \\(p = 10n\\) en ne faisant qu’une seule simulation afin d’en évaluer le temps de calcul. Une fois ce temps analysé, lancer \\(100\\) simulations pour \\(p = 10n\\) mais en omettant la méthode la plus couteuse en temps de calcul.\n\n\nCode\n#parallelisation\nfuture::plan(multisession, workers = 2)\n\nn = 100\np_list = c(n/10, n, 2*n, 10*n)\n\nr_cas1 = fun(n, p_list[1])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas2 = fun(n, p_list[2])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas3 = fun(n, p_list[3])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas4 = fun(n, p_list[4],1)\n\n\n1 :\n\n\nCode\nr_cas4bis = fun2(n, p_list[4])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\n# quit parallelisation\nfuture::plan(\"sequential\")\n\n\n\n\nCode\nres_cas1 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas1$selec_method1,r_cas1$selec_method2,r_cas1$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas1$taille_method1),mean(r_cas1$taille_method2),mean(r_cas1$taille_method3),NA),\n  Prevision_error = c(mean(r_cas1$prev_method1),mean(r_cas1$prev_method2),mean(r_cas1$prev_method3),mean(r_cas1$prev_method4)),\n  Running_time = c(r_cas1$temps1,r_cas1$temps2,r_cas1$temps3,r_cas1$temps4)\n)\nt(res_cas1)\n\n\n                     [,1]       [,2]       [,3]               [,4]         \nMethod               \"Forward\"  \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.71\"     \"0.08\"     \"0.42\"             NA           \nMean_nb_selected_var \"3.30\"     \"6.35\"     \"4.15\"             NA           \nPrevision_error      \"4.270127\" \"4.367182\" \"4.300236\"         \"4.347811\"   \nRunning_time         \"0.0548\"   \"0.0985\"   \"0.2030\"           \"0.2076\"     \n\n\nCode\nboxplot(sqrt(r_cas1$prev_method1),sqrt(r_cas1$prev_method2),sqrt(r_cas1$prev_method3),sqrt(r_cas1$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[1]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas2 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas2$selec_method1,r_cas2$selec_method2,r_cas2$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas2$taille_method1),mean(r_cas2$taille_method2),mean(r_cas2$taille_method3),NA),\n  Prevision_error = c(mean(r_cas2$prev_method1),mean(r_cas2$prev_method2),mean(r_cas2$prev_method3),mean(r_cas2$prev_method4)),\n  Running_time = c(r_cas2$temps1,r_cas2$temps2,r_cas2$temps3,r_cas2$temps4)\n)\nt(res_cas2)\n\n\n                     [,1]       [,2]       [,3]               [,4]         \nMethod               \"Forward\"  \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.01\"     \"0.01\"     \"0.10\"             NA           \nMean_nb_selected_var \" 8.11\"    \"14.59\"    \" 7.61\"            NA           \nPrevision_error      \"5.682778\" \"4.824673\" \"4.588443\"         \"5.223387\"   \nRunning_time         \"0.5960\"   \"0.5668\"   \"0.6884\"           \"0.6941\"     \n\n\nCode\nboxplot(sqrt(r_cas2$prev_method1),sqrt(r_cas2$prev_method2),sqrt(r_cas2$prev_method3),sqrt(r_cas2$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[2]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas3 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas3$selec_method1,r_cas3$selec_method2,r_cas3$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas3$taille_method1),mean(r_cas3$taille_method2),mean(r_cas3$taille_method3),NA),\n  Prevision_error = c(mean(r_cas3$prev_method1),mean(r_cas3$prev_method2),mean(r_cas3$prev_method3),mean(r_cas3$prev_method4)),\n  Running_time = c(r_cas3$temps1,r_cas3$temps2,r_cas3$temps3,r_cas3$temps4)\n)\nt(res_cas3)\n\n\n                     [,1]        [,2]        [,3]               [,4]         \nMethod               \"Forward\"   \"Lasso\"     \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.00\"      \"0.01\"      \"0.16\"             NA           \nMean_nb_selected_var \"48.66\"     \"17.34\"     \" 7.73\"            NA           \nPrevision_error      \"11.428807\" \" 5.016189\" \" 4.523595\"        \" 5.243494\"  \nRunning_time         \"11.0293\"   \" 0.4915\"   \" 0.6252\"          \" 0.6322\"    \n\n\nCode\nboxplot(sqrt(r_cas3$prev_method1),sqrt(r_cas3$prev_method2),sqrt(r_cas3$prev_method3),sqrt(r_cas3$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[3]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas4 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas4$selec_method1,r_cas4$selec_method2,r_cas4$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas4$taille_method1),mean(r_cas4$taille_method2),mean(r_cas4$taille_method3),NA),\n  Prevision_error = c(mean(r_cas4$prev_method1),mean(r_cas4$prev_method2),mean(r_cas4$prev_method3),mean(r_cas4$prev_method4)),\n  Running_time = c(r_cas4$temps1,r_cas4$temps2,r_cas4$temps3,r_cas4$temps4)\n)\nt(res_cas4)\n\n\n                     [,1]        [,2]        [,3]               [,4]         \nMethod               \"Forward\"   \"Lasso\"     \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \" 0\"        \" 0\"        \" 0\"               NA           \nMean_nb_selected_var \"99\"        \"11\"        \" 6\"               NA           \nPrevision_error      \"11.946047\" \" 5.699005\" \" 4.413508\"        \" 5.439750\"  \nRunning_time         \"233.75\"    \"  0.40\"    \"  0.62\"           \"  0.65\"     \n\n\nCode\nres_cas4bis = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas4bis$selec_method1,r_cas4bis$selec_method2,r_cas4bis$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas4bis$taille_method1),mean(r_cas4bis$taille_method2),mean(r_cas4bis$taille_method3),NA),\n  Prevision_error = c(mean(r_cas4bis$prev_method1),mean(r_cas4bis$prev_method2),mean(r_cas4bis$prev_method3),mean(r_cas4bis$prev_method4)),\n  Running_time = c(r_cas4bis$temps1,r_cas4bis$temps2,r_cas4bis$temps3,r_cas4bis$temps4)\n)\n\n\nWarning in mean.default(r_cas4bis$taille_method1): l'argument n'est ni\nnumérique, ni logique : renvoi de NA\n\n\nWarning in mean.default(r_cas4bis$prev_method1): l'argument n'est ni numérique,\nni logique : renvoi de NA\n\n\nCode\nt(res_cas4bis)\n\n\n                     [,1]      [,2]       [,3]               [,4]         \nMethod               \"Forward\" \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection NA        \"0.00\"     \"0.03\"             NA           \nMean_nb_selected_var NA        \"30.92\"    \"11.07\"            NA           \nPrevision_error      NA        \"5.260876\" \"4.776523\"         \"5.836417\"   \nRunning_time         NA        \"0.3399\"   \"0.5520\"           \"0.5763\"     \n\n\nCode\nboxplot(sqrt(r_cas4$prev_method1),sqrt(r_cas4$prev_method2),sqrt(r_cas4$prev_method3),sqrt(r_cas4$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[3]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\nCode\nboxplot(sqrt(r_cas4$prev_method1),sqrt(r_cas4bis$prev_method2),sqrt(r_cas4bis$prev_method3),sqrt(r_cas4bis$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[4]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nQuestion 7\nConclure sur les mérites respectifs de chaque méthode dans le contexte de l’étude.\n\n\nQuestion 8\nQuelles autres types de simulations pourrait-on envisager pour confirmer ou affiner ces conclusions ?"
  }
]
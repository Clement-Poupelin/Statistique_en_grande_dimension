[
  {
    "objectID": "posts/Exercice_10.html",
    "href": "posts/Exercice_10.html",
    "title": "Exercice 10",
    "section": "",
    "text": "Code\n####################################\n### Ex 10 : Caravan\nlibrary(ISLR)\ndata(\"Caravan\")\ndim(Caravan)\n\n\n[1] 5822   86\n\n\nCode\ndim(na.omit(Caravan))\n\n\n[1] 5822   86\n\n\nCode\nindexyes=which(Caravan$Purchase==\"Yes\")\nindexno=which(Caravan$Purchase==\"No\")\ntrain=c(sample(indexyes,length(indexyes)/2),sample(indexno,length(indexno)/2))\n\ny=Caravan$Purchase[train]\nXtrain=as.matrix(Caravan[train,-86])\nytest=Caravan$Purchase[-train]\nXtest=Caravan[-train,-86]\n\n#Forward\nmod=glm(Purchase~.,family=\"binomial\",data=Caravan,subset=train)\n\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\n\nCode\nmod0=glm(Purchase~1,family=\"binomial\",data=Caravan,subset=train)\ntmp=step(mod0,scope=formula(mod),direction=\"both\",k=log(n/2),trace=0)#par BIC\n\n\nError: objet 'n' introuvable\n\n\nCode\ntmp$formula\n\n\nError: objet 'tmp' introuvable\n\n\nCode\nmodforw=eval(tmp$call)\n\n\nError: objet 'tmp' introuvable\n\n\nCode\npred=predict(modforw,Xtest,type=\"response\")\n\n\nError: objet 'modforw' introuvable\n\n\nCode\nlibrary(ROCR)\npr = prediction(pred, ytest) \n\n\nError: objet 'pred' introuvable\n\n\nCode\nroc = performance(pr, measure = \"tpr\", x.measure = \"fpr\") \n\n\nError: objet 'pr' introuvable\n\n\nCode\nplot(roc)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'roc' introuvable\n\n\nCode\naucfwd=performance(pr,measure=\"auc\")@y.values\n\n\nError: objet 'pr' introuvable\n\n\nCode\n#Lasso\ncvglmnet=cv.glmnet(Xtrain,y,family=\"binomial\",type.measure=\"auc\",nfolds=10)\n\n\nError in cv.glmnet(Xtrain, y, family = \"binomial\", type.measure = \"auc\", : impossible de trouver la fonction \"cv.glmnet\"\n\n\nCode\nplot(cvglmnet)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'cvglmnet' introuvable\n\n\nCode\npredlasso=predict(cvglmnet,as.matrix(Xtest),s=cvglmnet$lambda.min,type=\"response\")\n\n\nError: objet 'cvglmnet' introuvable\n\n\nCode\nprlasso = prediction(predlasso, ytest) \n\n\nError: objet 'predlasso' introuvable\n\n\nCode\nroclasso = performance(prlasso, measure = \"tpr\", x.measure = \"fpr\") \n\n\nError: objet 'prlasso' introuvable\n\n\nCode\nauclasso=performance(prlasso,measure=\"auc\")@y.values\n\n\nError: objet 'prlasso' introuvable\n\n\nCode\n#Comparaison : Lasso semble un peu meilleur, mais sélectionne plus de variables\n#(il faudrait recommencer avec plusieurs découpages train/test)\nplot(roc)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'roc' introuvable\n\n\nCode\nplot(roclasso,add=T,col=2)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'roclasso' introuvable\n\n\nCode\naucfwd\n\n\nError: objet 'aucfwd' introuvable\n\n\nCode\nauclasso\n\n\nError: objet 'auclasso' introuvable\n\n\nCode\ncoef(modforw)\n\n\nError: objet 'modforw' introuvable\n\n\nCode\ncoef(cvglmnet,s=cvglmnet$lambda.min)\n\n\nError: objet 'cvglmnet' introuvable\n\n\nCode\n#D'autres méthodes possibles :\n\n#Gauss Lasso\nw=coef(cvglmnet,s=cvglmnet$lambda.min)\n\n\nError: objet 'cvglmnet' introuvable\n\n\nCode\nindex=which(w[-1]!=0)\n\n\nError: objet 'w' introuvable\n\n\nCode\nfit=glm(Purchase~.,data=Caravan[train,c(index,86)],family=\"binomial\")\n\n\nError in eval(mf, parent.frame()): objet 'index' introuvable\n\n\nCode\npredgauss=predict(fit,Xtest,type=\"response\")\n\n\nError: objet 'fit' introuvable\n\n\nCode\n#Adaptive Lasso\ncvglm=cv.glmnet(Xtrain,y,family=\"binomial\",type.measure=\"auc\",penalty.factor=1/abs(w[-1]))\n\n\nError in cv.glmnet(Xtrain, y, family = \"binomial\", type.measure = \"auc\", : impossible de trouver la fonction \"cv.glmnet\"\n\n\nCode\nplot(cvglm)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'cvglm' introuvable\n\n\nCode\npredalasso=predict(cvglm,as.matrix(Xtest),s=cvglm$lambda.min,type=\"response\")\n\n\nError: objet 'cvglm' introuvable\n\n\nCode\n#+Gauss pour finir\nwal=coef(cvglm,s=cvglm$lambda.min)\n\n\nError: objet 'cvglm' introuvable\n\n\nCode\nindex=which(wal[-1]!=0)\n\n\nError: objet 'wal' introuvable\n\n\nCode\nfit=glm(Purchase~.,data=Caravan[train,c(index,86)],family=\"binomial\")\n\n\nError in eval(mf, parent.frame()): objet 'index' introuvable\n\n\nCode\npredalgauss=predict(fit,Xtest,type=\"response\")\n\n\nError: objet 'fit' introuvable\n\n\nCode\n#Logistic Ridge\ncvglmnet=cv.glmnet(Xtrain,y,family=\"binomial\",type.measure=\"auc\",alpha=0)\n\n\nError in cv.glmnet(Xtrain, y, family = \"binomial\", type.measure = \"auc\", : impossible de trouver la fonction \"cv.glmnet\"\n\n\nCode\nplot(cvglmnet)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'cvglmnet' introuvable\n\n\nCode\npredridge=predict(cvglmnet,as.matrix(Xtest),s=cvglmnet$lambda.min,type=\"response\")\n\n\nError: objet 'cvglmnet' introuvable\n\n\nCode\n#Logistic Elastic Net (pour alpha=0.5)\ncvglmnet=cv.glmnet(Xtrain,y,family=\"binomial\",type.measure=\"auc\",alpha=1/2)\n\n\nError in cv.glmnet(Xtrain, y, family = \"binomial\", type.measure = \"auc\", : impossible de trouver la fonction \"cv.glmnet\"\n\n\nCode\nplot(cvglmnet)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'cvglmnet' introuvable\n\n\nCode\npreden=predict(cvglmnet,as.matrix(Xtest),s=cvglmnet$lambda.min,type=\"response\")\n\n\nError: objet 'cvglmnet' introuvable\n\n\nCode\n#Comparaison finale : les méthodes supplémentaires n'apportent rien d'intéressant \n#(il faudrait recommencer avec plusieurs découpages train/test)\nlibrary(ROCR)\npr = prediction(pred, ytest) \n\n\nError: objet 'pred' introuvable\n\n\nCode\nroc = performance(pr, measure = \"tpr\", x.measure = \"fpr\") \n\n\nError: objet 'pr' introuvable\n\n\nCode\nprlasso = prediction(predlasso, ytest) \n\n\nError: objet 'predlasso' introuvable\n\n\nCode\nroclasso = performance(prlasso, measure = \"tpr\", x.measure = \"fpr\") \n\n\nError: objet 'prlasso' introuvable\n\n\nCode\nprgauss = prediction(predgauss, ytest) \n\n\nError: objet 'predgauss' introuvable\n\n\nCode\nrocgauss = performance(prgauss, measure = \"tpr\", x.measure = \"fpr\")\n\n\nError: objet 'prgauss' introuvable\n\n\nCode\npralasso = prediction(predalasso, ytest) \n\n\nError: objet 'predalasso' introuvable\n\n\nCode\nrocalasso = performance(pralasso, measure = \"tpr\", x.measure = \"fpr\")\n\n\nError: objet 'pralasso' introuvable\n\n\nCode\npralgauss = prediction(predalgauss, ytest) \n\n\nError: objet 'predalgauss' introuvable\n\n\nCode\nrocalgauss = performance(pralgauss, measure = \"tpr\", x.measure = \"fpr\")\n\n\nError: objet 'pralgauss' introuvable\n\n\nCode\nprridge = prediction(predridge, ytest) \n\n\nError: objet 'predridge' introuvable\n\n\nCode\nrocridge = performance(prridge, measure = \"tpr\", x.measure = \"fpr\") \n\n\nError: objet 'prridge' introuvable\n\n\nCode\npren = prediction(preden, ytest) \n\n\nError: objet 'preden' introuvable\n\n\nCode\nrocen = performance(pren, measure = \"tpr\", x.measure = \"fpr\") \n\n\nError: objet 'pren' introuvable\n\n\nCode\nplot(roc)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'roc' introuvable\n\n\nCode\nplot(roclasso,add=T,col=2)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'roclasso' introuvable\n\n\nCode\nplot(rocgauss,add=T,col=3)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'rocgauss' introuvable\n\n\nCode\nplot(rocalasso,add=T,col=4)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'rocalasso' introuvable\n\n\nCode\nplot(rocalgauss,add=T,col=5)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'rocalgauss' introuvable\n\n\nCode\nplot(rocridge,add=T,col=6)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'rocridge' introuvable\n\n\nCode\nplot(rocen,add=T,col=7)\n\n\nError in h(simpleError(msg, call)): erreur d'ï¿½valuation de l'argument 'x' lors de la sï¿½lection d'une mï¿½thode pour la fonction 'plot' : objet 'rocen' introuvable\n\n\nCode\nperformance(pr,measure=\"auc\")@y.values\n\n\nError: objet 'pr' introuvable\n\n\nCode\nperformance(prlasso,measure=\"auc\")@y.values\n\n\nError: objet 'prlasso' introuvable\n\n\nCode\nperformance(prgauss,measure=\"auc\")@y.values\n\n\nError: objet 'prgauss' introuvable\n\n\nCode\nperformance(pralasso,measure=\"auc\")@y.values\n\n\nError: objet 'pralasso' introuvable\n\n\nCode\nperformance(pralgauss,measure=\"auc\")@y.values\n\n\nError: objet 'pralgauss' introuvable\n\n\nCode\nperformance(prridge,measure=\"auc\")@y.values\n\n\nError: objet 'prridge' introuvable\n\n\nCode\nperformance(pren,measure=\"auc\")@y.values\n\n\nError: objet 'pren' introuvable"
  },
  {
    "objectID": "posts/Exercice_09.html",
    "href": "posts/Exercice_09.html",
    "title": "Exercice 9",
    "section": "",
    "text": "On souhaite réaliser une petite étude par simulation pour évaluer les qualités respectives de 4 méthodes d’estimation d’un modèle de régression linéaire. On s’intéresse pour chacune d’elle à ses qualités de sélection de variables et à ses qualités prédictives. Le programme SimusReg.R permet de réaliser cette étude. Il contient deux fonctions, Simudata et la fonction principale fun, et un exemple d’utilisation en fin de programme.\n\n\nCode\nn = 100\np = 500\nX = matrix(rnorm(n*p), n, p)\n\n\n\n\nCode\nlibrary(lars)\n\n\nLoaded lars 1.3\n\n\n\nAttachement du package : 'lars'\n\n\nL'objet suivant est masqué depuis 'package:psych':\n\n    error.bars\n\n\nCode\nlibrary(leaps)\nlibrary(glmnet)\n\n\nLe chargement a nécessité le package : Matrix\n\n\nLoaded glmnet 4.1-8\n\n\nCode\nDataSimulation = function(n,p){\n  if(p &lt; 4){stop(\"p&gt;3 require\")}\n  # We create our matrix of explanatory variables\n  X = matrix(rnorm(n*p), n, p)\n  \n  # We define our coefficients of regression \n  coeff = matrix(0, p)\n  coeff[1:3] = 2\n  \n  # We build our explanatory variables\n  y = X%*%coeff + rnorm(n, sd = 2)\n  return(list(X = X, y = y, coeff = coeff))\n}\n\n\n\nfun = function(n, p, M = 100){ # By default, we make M = 100 simulation \n  \n  ## Initialization \n  #################\n  selec_method1 = NULL; selec_method2 = NULL; selec_method3 = NULL;\n  taille_method1 = NULL; taille_method2 = NULL; taille_method3 = NULL;\n  prev_method1 = NULL; prev_method2 = NULL; prev_method3 = NULL; prev_method4 = NULL;\n  temps1 = NULL; temps2 = NULL; temps3 = NULL; temps4 = NULL;\n  \n  for(i in 1:M){\n    cat(paste(i, \":\")) # counter to see progress\n    \n    # We define our train set\n    datatrain = DataSimulation(n, p)\n    Xtrain = datatrain$X\n    y = datatrain$y\n    coeff = datatrain$coeff\n    \n    # We define our test set\n    datatest = DataSimulation(n, p)\n    Xtest = datatest$X\n    ytest = datatest$y\n    \n    \n    ## Regression \n    #################\n    \n    # Method 1 : Forward-Hybrid with BIC\n    tic = proc.time()\n    tab = data.frame(y = y, X = Xtrain)\n    fit0 = lm(y~1, tab)\n    fit = lm(y~., tab)\n    tmp = step(fit0, scope = formula(fit),\n               k = log(n), # BIC criteria\n               direction = \"both\", # Hybrid\n               trace = 0)\n    noms = sort(names(tmp$model))\n    selec_method1[i] = identical(\n      noms[-length(noms)], sort(paste(\"X.\", which(coeff != 0), sep = \"\"))\n      )\n    taille_method1[i] = length(noms) - 1\n    prev_method1[i] = mean((predict(tmp,data.frame(X = Xtest)) - ytest)^2)\n    tac = proc.time() - tic\n    temps1[i] = tac[3]\n    \n    # Method 2 : Lasso\n    tic = proc.time()\n    cvglm = cv.glmnet(Xtrain, y) # By default we have Lasso\n    lambda = cvglm$lambda.min\n    coef2 = coef(cvglm, s = lambda)[-1]\n    index = which(coef2 != 0) \n    selec_method2[i] = identical(sort(index), which(coeff != 0))\n    taille_method2[i] = length(index)\n    prev_method2[i] = mean((predict(cvglm, Xtest, s = lambda) - ytest)^2)\n    tac = proc.time() - tic\n    temps2[i] = tac[3]\n    \n    # Methods 3 and 4 : Adaptive Lasso and  Gauss-Lasso  \n    if(length(index) == 0){\n      selec_method3[i] = selec_method2[i]\n      taille_method3[i] = taille_method2[i]\n      prev_method3[i] = prev_method2[i]\n      prev_method4[i] = prev_method2[i]}\n    else{\n      # Adaptive Lasso part\n      cvglm = cv.glmnet(Xtrain, y,\n                        penalty.factor = 1/abs(coef2))\n      lambda = cvglm$lambda.min\n      coef3 = coef(cvglm, s = lambda)[-1]\n      index = which(coef3 != 0) \n      selec_method3[i] = identical(sort(index), which(coeff != 0))\n      taille_method3[i] = length(index)\n      prev_method3[i] = mean((predict(cvglm, Xtest, s = lambda) - ytest)^2)\n      tac = proc.time() - tic\n      temps3[i] = tac[3]\n      \n      # Gauss-Lasso part\n      if(length(index) == 0){\n        prev_method4[i] = mean((mean(y) - ytest)^2)}\n      else{\n        tab = data.frame(y = y, X = Xtrain)\n        reg = lm(y~., \n                 data = tab[, c(1, index + 1)])\n        prev_method4[i] = mean((predict(reg, data.frame(X = Xtest)) - ytest)^2)\n        tac = proc.time() - tic\n        temps4[i] = tac[3]\n      }\n    }\n  }\n  \n  ## Results\n  #################\n  res = list(mean(selec_method1), mean(selec_method2), mean(selec_method3), taille_method1, taille_method2, taille_method3, prev_method1, prev_method2, prev_method3, prev_method4, mean(temps1), mean(temps2), mean(temps3), mean(temps4))\n  \n  names(res) = c(\"selec_method1\", \"selec_method2\", \"selec_method3\", \"taille_method1\", \"taille_method2\", \"taille_method3\", \"prev_method1\", \"prev_method2\", \"prev_method3\", \"prev_method4\", \"temps1\", \"temps2\", \"temps3\", \"temps4\")\n  \n  return(res)\n}\n\nfun2 = function(n, p, M = 100){ # By default, we make M = 100 simulation \n  \n  ## Initialization \n  #################\n  selec_method1 = NULL; selec_method2 = NULL; selec_method3 = NULL;\n  taille_method1 = NULL; taille_method2 = NULL; taille_method3 = NULL;\n  prev_method1 = NULL; prev_method2 = NULL; prev_method3 = NULL; prev_method4 = NULL;\n  temps1 = NULL; temps2 = NULL; temps3 = NULL; temps4 = NULL;\n  \n  for(i in 1:M){\n    cat(paste(i, \":\")) # counter to see progress\n    \n    # We define our train set\n    datatrain = DataSimulation(n, p)\n    Xtrain = datatrain$X\n    y = datatrain$y\n    coeff = datatrain$coeff\n    \n    # We define our test set\n    datatest = DataSimulation(n, p)\n    Xtest = datatest$X\n    ytest = datatest$y\n    \n    \n    ## Regression \n    #################\n    \n    # Method 2 : Lasso\n    tic = proc.time()\n    cvglm = cv.glmnet(Xtrain, y) # By default we have Lasso\n    lambda = cvglm$lambda.min\n    coef2 = coef(cvglm, s = lambda)[-1]\n    index = which(coef2 != 0) \n    selec_method2[i] = identical(sort(index), which(coeff != 0))\n    taille_method2[i] = length(index)\n    prev_method2[i] = mean((predict(cvglm, Xtest, s = lambda) - ytest)^2)\n    tac = proc.time() - tic\n    temps2[i] = tac[3]\n    \n    # Methods 3 and 4 : Adaptive Lasso and  Gauss-Lasso  \n    if(length(index) == 0){\n      selec_method3[i] = selec_method2[i]\n      taille_method3[i] = taille_method2[i]\n      prev_method3[i] = prev_method2[i]\n      prev_method4[i] = prev_method2[i]}\n    else{\n      # Adaptive Lasso part\n      cvglm = cv.glmnet(Xtrain, y,\n                        penalty.factor = 1/abs(coef2))\n      lambda = cvglm$lambda.min\n      coef3 = coef(cvglm, s = lambda)[-1]\n      index = which(coef3 != 0) \n      selec_method3[i] = identical(sort(index), which(coeff != 0))\n      taille_method3[i] = length(index)\n      prev_method3[i] = mean((predict(cvglm, Xtest, s = lambda) - ytest)^2)\n      tac = proc.time() - tic\n      temps3[i] = tac[3]\n      \n      # Gauss-Lasso part\n      if(length(index) == 0){\n        prev_method4[i] = mean((mean(y) - ytest)^2)}\n      else{\n        tab = data.frame(y = y, X = Xtrain)\n        reg = lm(y~., \n                 data = tab[, c(1, index + 1)])\n        prev_method4[i] = mean((predict(reg, data.frame(X = Xtest)) - ytest)^2)\n        tac = proc.time() - tic\n        temps4[i] = tac[3]\n      }\n    }\n  }\n  \n  ## Results\n  #################\n  res = list(mean(selec_method1), mean(selec_method2), mean(selec_method3), taille_method1, taille_method2, taille_method3, prev_method1, prev_method2, prev_method3, prev_method4, mean(temps1), mean(temps2), mean(temps3), mean(temps4))\n  \n  names(res) = c(\"selec_method1\", \"selec_method2\", \"selec_method3\", \"taille_method1\", \"taille_method2\", \"taille_method3\", \"prev_method1\", \"prev_method2\", \"prev_method3\", \"prev_method4\", \"temps1\", \"temps2\", \"temps3\", \"temps4\")\n  \n  return(res)\n}\n\n\n\n\nCode\n###### Exemple\na=fun(50,5,100)\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\n\n\nCode\na$selec_method1\n\n\n[1] 0.9\n\n\nCode\na$selec_method2\n\n\n[1] 0.16\n\n\nCode\na$selec_method3\n\n\n[1] 0.7\n\n\nCode\na$taille_method1\n\n\n  [1] 3 3 4 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3\n [38] 3 3 4 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 4 3 3 3 3 3\n [75] 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 4 3 3\n\n\nCode\na$taille_method2\n\n\n  [1] 5 5 5 4 4 5 3 5 3 5 5 5 5 4 4 4 3 4 4 4 5 4 3 4 5 5 4 4 3 5 5 4 4 4 5 4 4\n [38] 5 4 5 5 5 5 4 5 5 4 4 5 3 5 4 5 5 5 3 3 4 5 4 4 5 4 3 5 4 5 3 5 4 4 4 5 5\n [75] 5 3 4 5 3 4 5 5 5 5 5 5 5 5 3 3 5 5 3 4 5 5 4 4 3 5\n\n\nCode\na$taille_method3\n\n\n  [1] 3 3 5 3 3 5 3 3 3 3 3 3 5 3 3 3 3 3 4 3 3 3 3 3 5 3 3 3 3 4 5 4 3 3 4 3 3\n [38] 4 3 4 3 3 3 3 5 5 3 3 3 3 3 4 3 4 3 3 3 3 5 3 4 4 3 3 4 3 3 3 4 4 3 3 3 4\n [75] 4 3 3 4 3 3 3 5 3 4 3 3 4 3 3 3 5 4 3 3 3 3 3 4 3 3\n\n\nCode\nboxplot(sqrt(a$prev_method1),sqrt(a$prev_method2),sqrt(a$prev_method3),sqrt(a$prev_method4),names=c(\"Method1\",\"Method2\",\"Method3\",\"Method4\"),main=\"Title\")\n\n\n\n\n\nCode\nmean(a$prev_method1)\n\n\n[1] 4.317454\n\n\nCode\nmean(a$prev_method2)\n\n\n[1] 4.482734\n\n\nCode\nmean(a$prev_method3)\n\n\n[1] 4.405399\n\n\nCode\nmean(a$prev_method4)\n\n\n[1] 4.397463\n\n\nCode\na$temps1\n\n\n[1] 0.01149\n\n\nCode\na$temps2\n\n\n[1] 0.02251\n\n\nCode\na$temps3\n\n\n[1] 0.04536\n\n\nCode\na$temps4\n\n\n[1] 0.04648\n\n\n\nQuestion 1\nQuel modèle génère la fonction Simudata ? Combien de variables explicatives sont générées ? Parmi elles, lesquelles sont pertinentes pour la modélisation ? Ecrire l’équation du modèle.\n\n\nQuestion 2\nIdentifier les 4 méthodes d’estimation mises en oeuvre dans la fonction fun.\n\n\nQuestion 3\nDétailler les différentes sorties proposées par la fonction fun.\n\n\nQuestion 4\nRemplacer la valeur des options names et title du boxplot réalisé dans l’exemple par les bonnes informations.\n\n\nCode\nboxplot(sqrt(a$prev_method1),sqrt(a$prev_method2),sqrt(a$prev_method3),sqrt(a$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", 100,\"et p=\", 10),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"),\n        ylim = c(1,3))\n\n\n\n\n\n\n\nQuestion 5\nRéaliser une étude comparative des méthodes lorsque \\(n = 50\\) et \\(p = n/10\\), \\(p = n\\), \\(p = 2n\\), \\(p = 10n\\). Pour chaque situation, on considèrera \\(100\\) simulations afin de calculer les différents critères. On synthétisera les résultats en terme de qualité de sélection, nombre de variables sélectionnées, erreurs de prévision et temps de calcul.\n\n\nCode\n#parallelisation\nfuture::plan(multisession, workers = 2)\n\nn = 50\np_list = c(n/10, n, 2*n, 10*n)\n\nr_cas1 = fun(n, p_list[1])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas2 = fun(n, p_list[2])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas3 = fun(n, p_list[3])\n\n\n1 :2 :\n\n\n3 :\n\n\n4 :\n\n\n5 :\n\n\n6 :\n\n\n7 :\n\n\n8 :\n\n\n9 :\n\n\n10 :\n\n\n11 :\n\n\n12 :\n\n\n13 :14 :15 :\n\n\n16 :17 :\n\n\n18 :\n\n\n19 :\n\n\n20 :21 :\n\n\n22 :23 :\n\n\n24 :25 :\n\n\n26 :\n\n\n27 :28 :\n\n\n29 :\n\n\n30 :\n\n\n31 :\n\n\n32 :\n\n\n33 :\n\n\n34 :\n\n\n35 :\n\n\n36 :\n\n\n37 :\n\n\n38 :\n\n\n39 :\n\n\n40 :\n\n\n41 :\n\n\n42 :\n\n\n43 :\n\n\n44 :\n\n\n45 :\n\n\n46 :47 :\n\n\n48 :\n\n\n49 :50 :\n\n\n51 :\n\n\n52 :\n\n\n53 :54 :\n\n\n55 :\n\n\n56 :57 :\n\n\n58 :\n\n\n59 :\n\n\n60 :\n\n\n61 :\n\n\n62 :63 :\n\n\n64 :\n\n\n65 :\n\n\n66 :\n\n\n67 :\n\n\n68 :69 :\n\n\n70 :\n\n\n71 :\n\n\n72 :\n\n\n73 :74 :\n\n\n75 :76 :\n\n\n77 :78 :79 :80 :\n\n\n81 :\n\n\n82 :\n\n\n83 :\n\n\n84 :\n\n\n85 :\n\n\n86 :\n\n\n87 :\n\n\n88 :\n\n\n89 :\n\n\n90 :\n\n\n91 :\n\n\n92 :93 :\n\n\n94 :\n\n\n95 :\n\n\n96 :97 :\n\n\n98 :99 :\n\n\n100 :\n\n\nCode\nr_cas4 = fun(n, p_list[4],1)\n\n\n1 :\n\n\nCode\nr_cas4bis = fun2(n, p_list[4])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\n# file_path &lt;- file.path(\"../Data/r_cas1.rds\")\n# saveRDS(r_cas1, file = file_path)\n\n# quit parallelisation\nfuture::plan(\"sequential\")\n\n\n\n\nCode\n# file_path &lt;- file.path(\"../Data/r_cas1.rds\")\n# r_cas1 &lt;- readRDS(file_path)\n\n\n\n\nCode\nres_cas1 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas1$selec_method1,r_cas1$selec_method2,r_cas1$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas1$taille_method1),mean(r_cas1$taille_method2),mean(r_cas1$taille_method3),NA),\n  Prevision_error = c(mean(r_cas1$prev_method1),mean(r_cas1$prev_method2),mean(r_cas1$prev_method3),mean(r_cas1$prev_method4)),\n  Running_time = c(r_cas1$temps1,r_cas1$temps2,r_cas1$temps3,r_cas1$temps4)\n)\nt(res_cas1)\n\n\n                     [,1]       [,2]       [,3]               [,4]         \nMethod               \"Forward\"  \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.95\"     \"0.24\"     \"0.76\"             NA           \nMean_nb_selected_var \"3.05\"     \"4.19\"     \"3.29\"             NA           \nPrevision_error      \"4.267999\" \"4.372448\" \"4.311278\"         \"4.298561\"   \nRunning_time         \"0.01006\"  \"0.02122\"  \"0.04285\"          \"0.04382\"    \n\n\nCode\nboxplot(sqrt(r_cas1$prev_method1),sqrt(r_cas1$prev_method2),sqrt(r_cas1$prev_method3),sqrt(r_cas1$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[1]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas2 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas2$selec_method1,r_cas2$selec_method2,r_cas2$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas2$taille_method1),mean(r_cas2$taille_method2),mean(r_cas2$taille_method3),NA),\n  Prevision_error = c(mean(r_cas2$prev_method1),mean(r_cas2$prev_method2),mean(r_cas2$prev_method3),mean(r_cas2$prev_method4)),\n  Running_time = c(r_cas2$temps1,r_cas2$temps2,r_cas2$temps3,r_cas2$temps4)\n)\nt(res_cas2)\n\n\n                     [,1]       [,2]       [,3]               [,4]         \nMethod               \"Forward\"  \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.06\"     \"0.01\"     \"0.09\"             NA           \nMean_nb_selected_var \" 7.92\"    \"12.88\"    \" 8.17\"            NA           \nPrevision_error      \"7.050225\" \"5.609938\" \"5.487207\"         \"6.401796\"   \nRunning_time         \"0.07149\"  \"0.04010\"  \"0.06590\"          \"0.06738\"    \n\n\nCode\nboxplot(sqrt(r_cas2$prev_method1),sqrt(r_cas2$prev_method2),sqrt(r_cas2$prev_method3),sqrt(r_cas2$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[2]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas3 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas3$selec_method1,r_cas3$selec_method2,r_cas3$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas3$taille_method1),mean(r_cas3$taille_method2),mean(r_cas3$taille_method3),NA),\n  Prevision_error = c(mean(r_cas3$prev_method1),mean(r_cas3$prev_method2),mean(r_cas3$prev_method3),mean(r_cas3$prev_method4)),\n  Running_time = c(r_cas3$temps1,r_cas3$temps2,r_cas3$temps3,r_cas3$temps4)\n)\nt(res_cas3)\n\n\n                     [,1]        [,2]        [,3]               [,4]         \nMethod               \"Forward\"   \"Lasso\"     \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.00\"      \"0.00\"      \"0.06\"             NA           \nMean_nb_selected_var \"40.57\"     \"15.80\"     \" 9.09\"            NA           \nPrevision_error      \"16.207790\" \" 6.138163\" \" 5.738140\"        \" 6.723044\"  \nRunning_time         \"0.71804\"   \"0.04013\"   \"0.06882\"          \"0.07045\"    \n\n\nCode\nboxplot(sqrt(r_cas3$prev_method1),sqrt(r_cas3$prev_method2),sqrt(r_cas3$prev_method3),sqrt(r_cas3$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[3]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas4 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas4$selec_method1,r_cas4$selec_method2,r_cas4$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas4$taille_method1),mean(r_cas4$taille_method2),mean(r_cas4$taille_method3),NA),\n  Prevision_error = c(mean(r_cas4$prev_method1),mean(r_cas4$prev_method2),mean(r_cas4$prev_method3),mean(r_cas4$prev_method4)),\n  Running_time = c(r_cas4$temps1,r_cas4$temps2,r_cas4$temps3,r_cas4$temps4)\n)\nt(res_cas4)\n\n\n                     [,1]        [,2]        [,3]               [,4]         \nMethod               \"Forward\"   \"Lasso\"     \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \" 0\"        \" 0\"        \" 0\"               NA           \nMean_nb_selected_var \"49\"        \"41\"        \"19\"               NA           \nPrevision_error      \"11.762266\" \" 4.734249\" \" 4.562205\"        \" 5.403622\"  \nRunning_time         \"6.500\"     \"0.041\"     \"0.077\"            \"0.080\"      \n\n\nCode\nboxplot(sqrt(r_cas4$prev_method1),sqrt(r_cas4$prev_method2),sqrt(r_cas4$prev_method3),sqrt(r_cas4$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[3]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas4bis = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas4bis$selec_method1,r_cas4bis$selec_method2,r_cas4bis$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas4bis$taille_method1),mean(r_cas4bis$taille_method2),mean(r_cas4bis$taille_method3),NA),\n  Prevision_error = c(mean(r_cas4bis$prev_method1),mean(r_cas4bis$prev_method2),mean(r_cas4bis$prev_method3),mean(r_cas4bis$prev_method4)),\n  Running_time = c(r_cas4bis$temps1,r_cas4bis$temps2,r_cas4bis$temps3,r_cas4bis$temps4)\n)\n\n\nWarning in mean.default(r_cas4bis$taille_method1): l'argument n'est ni\nnumérique, ni logique : renvoi de NA\n\n\nWarning in mean.default(r_cas4bis$prev_method1): l'argument n'est ni numérique,\nni logique : renvoi de NA\n\n\nCode\nt(res_cas4bis)\n\n\n                     [,1]      [,2]       [,3]               [,4]         \nMethod               \"Forward\" \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection NA        \"0.00\"     \"0.04\"             NA           \nMean_nb_selected_var NA        \"24.87\"    \"12.34\"            NA           \nPrevision_error      NA        \"7.230664\" \"6.781834\"         \"7.861071\"   \nRunning_time         NA        \"0.04820\"  \"0.08404\"          \"0.08697\"    \n\n\nCode\nboxplot(sqrt(r_cas4$prev_method1),sqrt(r_cas4bis$prev_method2),sqrt(r_cas4bis$prev_method3),sqrt(r_cas4bis$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[4]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nQuestion 6\nRéaliser la même étude pour \\(n = 100\\) et \\(p = n/10\\), \\(p = n\\), \\(p = 2n\\), toujours basée sur \\(100\\) simulations dans chaque cas. Considérer de plus le cas \\(p = 10n\\) en ne faisant qu’une seule simulation afin d’en évaluer le temps de calcul. Une fois ce temps analysé, lancer \\(100\\) simulations pour \\(p = 10n\\) mais en omettant la méthode la plus couteuse en temps de calcul.\n\n\nCode\n#parallelisation\nfuture::plan(multisession, workers = 2)\n\nn = 100\np_list = c(n/10, n, 2*n, 10*n)\n\nr_cas1 = fun(n, p_list[1])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas2 = fun(n, p_list[2])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\nr_cas3 = fun(n, p_list[3])\n\n\n1 :\n\n\n2 :3 :4 :5 :6 :\n\n\n7 :8 :9 :\n\n\n10 :11 :\n\n\n12 :\n\n\n13 :14 :15 :16 :17 :18 :\n\n\n19 :20 :\n\n\n21 :\n\n\n22 :23 :24 :25 :\n\n\n26 :27 :\n\n\n28 :29 :30 :\n\n\n31 :\n\n\n32 :33 :34 :\n\n\n35 :\n\n\n36 :37 :38 :39 :\n\n\n40 :41 :42 :43 :\n\n\n44 :\n\n\n45 :46 :47 :\n\n\n48 :\n\n\n49 :\n\n\n50 :51 :52 :\n\n\n53 :54 :55 :56 :\n\n\n57 :58 :\n\n\n59 :\n\n\n60 :61 :\n\n\n62 :63 :\n\n\n64 :\n\n\n65 :66 :\n\n\n67 :68 :\n\n\n69 :\n\n\n70 :71 :72 :73 :74 :\n\n\n75 :\n\n\n76 :77 :78 :\n\n\n79 :80 :81 :82 :83 :\n\n\n84 :85 :86 :87 :88 :89 :90 :\n\n\n91 :\n\n\n92 :93 :\n\n\n94 :\n\n\n95 :96 :97 :98 :\n\n\n99 :100 :\n\n\nCode\nr_cas4 = fun(n, p_list[4],1)\n\n\n1 :\n\n\nCode\nr_cas4bis = fun2(n, p_list[4])\n\n\n1 :2 :3 :4 :5 :6 :7 :8 :9 :10 :11 :12 :13 :14 :15 :16 :17 :18 :19 :20 :21 :22 :23 :24 :25 :26 :27 :28 :29 :30 :31 :32 :33 :34 :35 :36 :37 :38 :39 :40 :41 :42 :43 :44 :45 :46 :47 :48 :49 :50 :51 :52 :53 :54 :55 :56 :57 :58 :59 :60 :61 :62 :63 :64 :65 :66 :67 :68 :69 :70 :71 :72 :73 :74 :75 :76 :77 :78 :79 :80 :81 :82 :83 :84 :85 :86 :87 :88 :89 :90 :91 :92 :93 :94 :95 :96 :97 :98 :99 :100 :\n\n\nCode\n# quit parallelisation\nfuture::plan(\"sequential\")\n\n\n\n\nCode\nres_cas1 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas1$selec_method1,r_cas1$selec_method2,r_cas1$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas1$taille_method1),mean(r_cas1$taille_method2),mean(r_cas1$taille_method3),NA),\n  Prevision_error = c(mean(r_cas1$prev_method1),mean(r_cas1$prev_method2),mean(r_cas1$prev_method3),mean(r_cas1$prev_method4)),\n  Running_time = c(r_cas1$temps1,r_cas1$temps2,r_cas1$temps3,r_cas1$temps4)\n)\nt(res_cas1)\n\n\n                     [,1]       [,2]       [,3]               [,4]         \nMethod               \"Forward\"  \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.71\"     \"0.08\"     \"0.42\"             NA           \nMean_nb_selected_var \"3.30\"     \"6.35\"     \"4.15\"             NA           \nPrevision_error      \"4.270127\" \"4.367182\" \"4.300236\"         \"4.347811\"   \nRunning_time         \"0.01367\"  \"0.02255\"  \"0.04586\"          \"0.04690\"    \n\n\nCode\nboxplot(sqrt(r_cas1$prev_method1),sqrt(r_cas1$prev_method2),sqrt(r_cas1$prev_method3),sqrt(r_cas1$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[1]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas2 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas2$selec_method1,r_cas2$selec_method2,r_cas2$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas2$taille_method1),mean(r_cas2$taille_method2),mean(r_cas2$taille_method3),NA),\n  Prevision_error = c(mean(r_cas2$prev_method1),mean(r_cas2$prev_method2),mean(r_cas2$prev_method3),mean(r_cas2$prev_method4)),\n  Running_time = c(r_cas2$temps1,r_cas2$temps2,r_cas2$temps3,r_cas2$temps4)\n)\nt(res_cas2)\n\n\n                     [,1]       [,2]       [,3]               [,4]         \nMethod               \"Forward\"  \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.01\"     \"0.01\"     \"0.10\"             NA           \nMean_nb_selected_var \" 8.11\"    \"14.59\"    \" 7.61\"            NA           \nPrevision_error      \"5.682778\" \"4.824673\" \"4.588443\"         \"5.223387\"   \nRunning_time         \"0.13837\"  \"0.06476\"  \"0.09358\"          \"0.09517\"    \n\n\nCode\nboxplot(sqrt(r_cas2$prev_method1),sqrt(r_cas2$prev_method2),sqrt(r_cas2$prev_method3),sqrt(r_cas2$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[2]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas3 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas3$selec_method1,r_cas3$selec_method2,r_cas3$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas3$taille_method1),mean(r_cas3$taille_method2),mean(r_cas3$taille_method3),NA),\n  Prevision_error = c(mean(r_cas3$prev_method1),mean(r_cas3$prev_method2),mean(r_cas3$prev_method3),mean(r_cas3$prev_method4)),\n  Running_time = c(r_cas3$temps1,r_cas3$temps2,r_cas3$temps3,r_cas3$temps4)\n)\nt(res_cas3)\n\n\n                     [,1]        [,2]        [,3]               [,4]         \nMethod               \"Forward\"   \"Lasso\"     \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \"0.00\"      \"0.01\"      \"0.16\"             NA           \nMean_nb_selected_var \"48.66\"     \"17.34\"     \" 7.73\"            NA           \nPrevision_error      \"11.428807\" \" 5.016189\" \" 4.523595\"        \" 5.243494\"  \nRunning_time         \"2.67996\"   \"0.06936\"   \"0.10052\"          \"0.10242\"    \n\n\nCode\nboxplot(sqrt(r_cas3$prev_method1),sqrt(r_cas3$prev_method2),sqrt(r_cas3$prev_method3),sqrt(r_cas3$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[3]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nCode\nres_cas4 = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas4$selec_method1,r_cas4$selec_method2,r_cas4$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas4$taille_method1),mean(r_cas4$taille_method2),mean(r_cas4$taille_method3),NA),\n  Prevision_error = c(mean(r_cas4$prev_method1),mean(r_cas4$prev_method2),mean(r_cas4$prev_method3),mean(r_cas4$prev_method4)),\n  Running_time = c(r_cas4$temps1,r_cas4$temps2,r_cas4$temps3,r_cas4$temps4)\n)\nt(res_cas4)\n\n\n                     [,1]        [,2]        [,3]               [,4]         \nMethod               \"Forward\"   \"Lasso\"     \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection \" 0\"        \" 0\"        \" 0\"               NA           \nMean_nb_selected_var \"99\"        \"11\"        \" 6\"               NA           \nPrevision_error      \"11.946047\" \" 5.699005\" \" 4.413508\"        \" 5.439750\"  \nRunning_time         \"61.347\"    \" 0.088\"    \" 0.129\"           \" 0.133\"     \n\n\nCode\nres_cas4bis = data.frame(\n  Method = c(\"Forward\", \"Lasso\", \"Adaptative Lasso\", \"Gauss-Lasso\"), \n  Quality_of_selection = c(r_cas4bis$selec_method1,r_cas4bis$selec_method2,r_cas4bis$selec_method3,NA),\n  Mean_nb_selected_var = c(mean(r_cas4bis$taille_method1),mean(r_cas4bis$taille_method2),mean(r_cas4bis$taille_method3),NA),\n  Prevision_error = c(mean(r_cas4bis$prev_method1),mean(r_cas4bis$prev_method2),mean(r_cas4bis$prev_method3),mean(r_cas4bis$prev_method4)),\n  Running_time = c(r_cas4bis$temps1,r_cas4bis$temps2,r_cas4bis$temps3,r_cas4bis$temps4)\n)\n\n\nWarning in mean.default(r_cas4bis$taille_method1): l'argument n'est ni\nnumérique, ni logique : renvoi de NA\n\n\nWarning in mean.default(r_cas4bis$prev_method1): l'argument n'est ni numérique,\nni logique : renvoi de NA\n\n\nCode\nt(res_cas4bis)\n\n\n                     [,1]      [,2]       [,3]               [,4]         \nMethod               \"Forward\" \"Lasso\"    \"Adaptative Lasso\" \"Gauss-Lasso\"\nQuality_of_selection NA        \"0.00\"     \"0.03\"             NA           \nMean_nb_selected_var NA        \"30.92\"    \"11.07\"            NA           \nPrevision_error      NA        \"5.260876\" \"4.776523\"         \"5.836417\"   \nRunning_time         NA        \"0.08522\"  \"0.13196\"          \"0.13632\"    \n\n\nCode\nboxplot(sqrt(r_cas4$prev_method1),sqrt(r_cas4$prev_method2),sqrt(r_cas4$prev_method3),sqrt(r_cas4$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[3]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\nCode\nboxplot(sqrt(r_cas4$prev_method1),sqrt(r_cas4bis$prev_method2),sqrt(r_cas4bis$prev_method3),sqrt(r_cas4bis$prev_method4),\n        names=c(\"Forward\",\"Lasso\",\"Adaptative Lasso\",\"Gauss-Lasso\"),\n        main=paste(\"Erreur de prévision pour n =\", n,\"et p=\", p_list[4]),\n        col = c(\"orchid3\", \"palegreen\", \"salmon2\", \"lightskyblue2\"))\n\n\n\n\n\n\n\nQuestion 7\nConclure sur les mérites respectifs de chaque méthode dans le contexte de l’étude.\n\n\nQuestion 8\nQuelles autres types de simulations pourrait-on envisager pour confirmer ou affiner ces conclusions ?"
  },
  {
    "objectID": "posts/Exercice_07.html",
    "href": "posts/Exercice_07.html",
    "title": "Exercice 07",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nCode\n# Données\nlibrary(ISLR)         # Hitters data \nlibrary(dplyr)        # manipulation des données\n\n# Infrence\nlibrary(pls) ## PCR et PLSR\nlibrary(glmnet) ## regression pénalisée\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nIntercative Boxplot\n\n\n\n\nCode\nmy_interactive_boxplot &lt;- function(data) {\n  data_long &lt;- reshape2::melt(data)\n  \n  p &lt;- ggplot(data_long,\n              aes(\n                x = variable,\n                y = value,\n                fill = variable,\n                stat = \"identity\"\n              )) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +\n    labs(title = \"Distribution des Variables (Intercative Boxplot)\", x = \"Variables\", y = \"Valeurs\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  \n  return(plotly::ggplotly(p))\n}\n\n\n\n\n\n\n\n\n\nCode\nset.seed(140400)\n\n\n\n\n\n\n\nDonnées\nCette exercice est la suite direct de l’Exercice 06 où l’on a pu utiliser les méthodes de regressions avec reduction de dimension PCR et PLSR.\nOn va donc reprendre les mêmes données avec le même découpage en train et test.\n\n\nCode\nHitters_Without_NA &lt;- Hitters %&gt;% na.omit()\npercent_to_draw &lt;- 0.75\nindex_train &lt;- sample(nrow(Hitters_Without_NA), size = floor(percent_to_draw * nrow(Hitters_Without_NA)))\n\nHitters_train &lt;- Hitters_Without_NA[index_train, ]\n\nHitters_test &lt;- Hitters_Without_NA[-index_train, ]\n\n\nNotre objectif ici sera donc de compléter l’analyse de l’Exercice 06 en utilisant cette fois ci les méthodes de regression pénalisée Ridge et Lasso (méthodes détaillées dans l’Exercice 06 Bonus : Ridge vs Lasso).\n\n\nAnalyse inférentielle\nContrairement à la plupart des autres package R qui permettent de faire de l’apprentissage, le package glmnet n’autorise pas l’utilisation de formules.\nIl faut donc spécifier explicitement la matrice \\(X\\) et le vecteur \\(y\\).\n\nOn peut obtenir la matrice \\(X\\) et notamment le codage des variables qualitatives avec la fonction model.matrix.\n\n\nCode\nX_train &lt;- model.matrix(Salary ~ ., data = Hitters_train)[, -1]\nY_train &lt;- Hitters_train$Salary\n\nX_test &lt;- model.matrix(Salary ~ ., data = Hitters_test)[, -1]\nY_test &lt;- Hitters_test$Salary\n\n\nEt ce n’est qu’après que l’on peut mettre en place la modélisation.\n\nRidgeLasso\n\n\n\n\nCode\nmod.R &lt;- glmnet(X_train, Y_train, alpha = 0) \n\n\nOn peut également visualiser les chemins de régularisation des estimateurs Ridge.\n\n\nCode\nplot(mod.R, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\nIci on voit l’évolution de nos coefficients \\(\\beta\\) en fonction des diffrentes valeurs de \\(\\lambda\\).\nAinsi, sur la gauche on se retrouve dans la situation où il n’y a pas de pénalisation et donc nos coefficients sont les \\(\\beta\\) de l’estimation par moindres carrés. Et donc plus \\(\\lambda\\) va augmenter, plus on se retrouvera dans une situation où les coefficients vont tendrent vers 0.\n\n\n\n\nCode\nmod.L &lt;- glmnet(X_train, Y_train, alpha = 1) \n\n\nOn peut également visualiser les chemins de régularisation des estimateurs Lasso.\n\n\nCode\nplot(mod.L, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\nIci on voit l’évolution de nos coefficients \\(\\beta\\) en fonction des diffrentes valeurs de \\(\\lambda\\).\nAinsi, sur la gauche on se retrouve dans la situation où il n’y a pas de pénalisation et donc nos coefficients sont les \\(\\beta\\) de l’estimation par moindres carrés. Et donc plus \\(\\lambda\\) va augmenter, plus on se retrouvera dans une situation où les coefficients vont tendrent vers 0.\n\n\n\n\n\nSelection des paramètres de régularisation\nMaintenant que les modèles sont estimés avec plusieurs valeurs de \\(\\lambda\\) possibles, il se pose la question du choix du bon paramètre.\nPour cela, on utilise la fonction cv.glmnet qui, comme son nom le laisse suggérer, permet d’effectuer une validation croisée pour notre modèle avec par défaut nfolds=10 (le nombre de pli pour le découpage de sous ensembles). Puis on peut faire un plot de l’objet.\n\nRidgeLasso\n\n\n\n\nCode\nridgeCV &lt;- cv.glmnet(X_train, Y_train, alpha = 0) \nplot(ridgeCV)\n\n\n\n\n\nOn visualise ici les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de \\(\\lambda\\) (échelle logarithmique). Deux traits verticaux sont représentés :\n\ncelui de gauche correspond à la valeur de \\(\\lambda\\) qui minimise l’erreur quadratique\ncelui de droite correspond à la plus grande valeur de \\(\\lambda\\) telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur\n\nD’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux.\nA savoir que si l’on veut diminuer la complexité du modèle on choisira la valeur de droite.\n\n\n\n\n\n\nWarning\n\n\n\nAttention, on peut remarquer ici que l’axe verticale de gauche semble toucher le bord du plot. Dans ces cas là, il convient de parametrer les \\(\\lambda\\) de telle sorte à “explorer” des valeurs de \\(\\lambda\\) plus petite.\n\n\n\n\nCode\nridgeCV2 &lt;- cv.glmnet(X_train, Y_train, alpha = 0, lambda = seq(exp(-1), exp(9), by = 1)) \nplot(ridgeCV2)\n\n\n\n\n\nCette fois ci, nos deux axes verticaux sont éloignés du bord du graphe et on voit bien qu’on a pu baisser ma valeur minimal de \\(\\lambda\\).\n\n\n\n\nRésultats\n\n\nIci on on obtient \\(\\lambda_{min} =\\) 1.368 et \\(\\lambda_{1se} =\\) 1468.368\n\n\n\n\n\nCode\nlassoCV &lt;- cv.glmnet(X_train, Y_train, alpha = 1)\nplot(lassoCV)\n\n\n\n\n\nOn visualise ici les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de \\(\\lambda\\) (échelle logarithmique). Deux traits verticaux sont représentés :\n\ncelui de gauche correspond à la valeur de \\(\\lambda\\) qui minimise l’erreur quadratique\ncelui de droite correspond à la plus grande valeur de \\(\\lambda\\) telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur\n\nD’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux.\nA savoir que si l’on veut diminuer la complexité du modèle on choisira la valeur de droite.\n\n\n\n\nRésultats\n\n\nIci on on obtient \\(\\lambda_{min} =\\) 1.167 et \\(\\lambda_{1se} =\\) 84.295\n\n\n\n\n\n\nPrédiction et comparaison\nOn souhaite maintenant prédiction pour le jeu de données test.\nUne première approche pourrait consister à réajuster le modèle sur toutes les données pour la valeur de \\(\\lambda\\) sélectionnée.\nCette étape est en réalité déjà effectuée par la fonction cv.glmnet. Il suffit par conséquent d’appliquer la fonction predict à l’objet obtenu avec cv.glmnet en spécifiant la valeur de \\(\\lambda\\) souhaitée puis on calcul l’erreur de prediction pour les modèles Ridge et Lasso.\n\n\nCode\npred.ridge_min &lt;- predict(ridgeCV, newx = X_test, s = \"lambda.min\")\nrmsep_ridge &lt;- sqrt(mean((pred.ridge_min - Y_test)^2, na.rm=T))\n\npred.lasso_min &lt;- predict(lassoCV, newx = X_test, s = \"lambda.min\") \nrmsep_lasso &lt;- sqrt(mean((pred.lasso_min - Y_test)^2, na.rm=T))\n\n\nAinsi on peut obtenir l’erreur de prédiction via le RMSEP pour les 2 modèles et les comparer avec les valeurs obtenues pour les méthodes précédemment testées : Sélection automatique both (Exercice 01), PCR (Exercice 06), PLSR (Exercice 06.\n\n\nCode\n## Sélection automatique\n########################\nmod0 &lt;- lm(Salary ~ 0, data = Hitters_train)\nmod1 &lt;- lm(Salary ~ ., data = Hitters_train)\nmod_step &lt;- step(\n  mod0,\n  scope = formula(mod1),\n  trace = FALSE,\n  direction = \"both\",\n  k = log(nrow(Hitters_train))\n)\n\nhat_Hitters_test_mod_step &lt;- predict(mod_step, Hitters_test)\nrmsep_mod_step &lt;- sqrt(mean((\n  hat_Hitters_test_mod_step - Hitters_test$Salary\n) ** 2))\n\n\n## PCR\n########################\nmod_pcr &lt;- pcr(\n  Salary ~ .,\n  scale = TRUE,\n  data = Hitters_train,\n  validation = \"CV\",\n  segments = 10\n)\nncomp.rmsep_pcr &lt;- which.min(RMSEP(mod_pcr, estimate = c(\"CV\"))$val[\"CV\", , ]) -\n  1\nhat_Hitters_test_mod_pcr &lt;- predict(mod_pcr, Hitters_test, ncomp = (which.min(RMSEP(\n  mod_pcr, estimate = c(\"CV\")\n)$val[\"CV\", , ]) - 1))\nrmsep_mod_pcr &lt;- sqrt(mean((\n  hat_Hitters_test_mod_pcr - Hitters_test$Salary\n) ** 2))\n\n## PLS\n########################\nmod_pls &lt;- plsr(\n  Salary ~ .,\n  scale = TRUE,\n  data = Hitters_train,\n  validation = \"CV\",\n  segments = 10\n)\nncomp.rmsep_pls &lt;- which.min(RMSEP(mod_pls, estimate = c(\"CV\"))$val[\"CV\", , ]) -\n  1\nhat_df_test_salary.pls &lt;- predict(mod_pls, Hitters_test, ncomp = (which.min(RMSEP(\n  mod_pls, estimate = c(\"CV\")\n)$val[\"CV\", , ]) - 1))\nrmsep_mod_pls &lt;- sqrt(mean((\n  hat_df_test_salary.pls - Hitters_test$Salary\n) ** 2))\n\n\n\n\nCode\nmethods &lt;- c(\"Err_mod_step\",\n             \"Err_PCR\",\n             \"Err_PLS\",\n             \"Err_Ridge\",\n             \"Err_Lasso_glmnet\")\nerrors &lt;- c(rmsep_mod_step,\n            rmsep_mod_pcr,\n            rmsep_mod_pls,\n            rmsep_ridge,\n            rmsep_lasso)\nrmsep_pred_df &lt;- data.frame(Method = factor(methods, levels = methods), Error = round(errors, 3))\n\np &lt;- ggplot(rmsep_pred_df, aes(x = Method, y = Error, fill = Method)) +\n  geom_bar(stat = \"identity\",\n           width = 0.6,\n           color = \"black\") +\n  geom_text(aes(label = round(Error, 3)), fontface = \"bold\", vjust = -1.5, size = 6) +\n  scale_fill_viridis_d() +\n  ylim(0, 450) +\n  theme_minimal() +\n  labs(title = \"Erreur pour les différentes méthodes\", x = \"Méthode\", y = \"Erreur\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\np\n\n\n\n\n\n\n\nCode\nrmsep_pred_df %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn peut voir ici que c’est la méthode Ridge qui minimise l’erreur de prédiction.\n\n\n\nPour confirmer ou contredire nos résultats, il convient maintenant de tester les méthodes \\(K\\) fois pour btenir \\(K\\) erreurs de prédictions et ainsi regarder la méthode qui, en moyenne, va minimiser cette erreur tout en prenant en compte la variance des erreurs.\n\n\nCode\nK &lt;- 10\nn &lt;- nrow(Hitters_Without_NA)\nseg &lt;- pls::cvsegments(n, K)\nerr_step &lt;- NULL\nerr_pcr &lt;- NULL\nerr_pls &lt;- NULL\nerr_ridge &lt;- NULL\nerr_lasso &lt;- NULL\nerr_lars &lt;- NULL\nfor (i in 1:K) {\n  test &lt;- seg[[i]]\n  mod0 &lt;- lm(Salary ~ 0, data = Hitters_Without_NA, subset = -test)\n  mod1 &lt;- lm(Salary ~ ., data = Hitters_Without_NA, subset = -test)\n  mod_step &lt;- step(\n    mod0,\n    scope = formula(mod1),\n    direction = \"both\",\n    k = log(n),\n    trace = 0\n  )\n  mod_step_pred &lt;- predict(mod_step, Hitters_Without_NA[test, ])\n  err_step[i] &lt;- sqrt(mean((mod_step_pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                            T))\n  \n  pcr.fit &lt;- pcr(\n    Salary ~ .,\n    data = Hitters_Without_NA,\n    scale = TRUE,\n    subset = -test,\n    validation = \"CV\",\n    segments = 10\n  )\n  nb_comp &lt;- which.min(RMSEP(pcr.fit, 'CV')$val[, , 1:10])\n  pcr.pred &lt;- predict(pcr.fit, Hitters_Without_NA[test, ], ncomp = nb_comp)\n  err_pcr[i] &lt;- sqrt(mean((pcr.pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                            T))\n  \n  pls.fit &lt;- plsr(\n    Salary ~ .,\n    data = Hitters_Without_NA,\n    subset = -test,\n    scale = TRUE,\n    validation = \"CV\"\n  )\n  nb_comp &lt;- which.min(RMSEP(pls.fit, 'CV')$val[, , 1:10])\n  pls.pred &lt;- predict(pls.fit, Hitters_Without_NA[test, ], ncomp = nb_comp)\n  err_pls[i] &lt;- sqrt(mean((pls.pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                            T))\n  \n  train.mat &lt;- model.matrix(Salary ~ ., data = Hitters_Without_NA[-test, ])\n  train.mat &lt;- train.mat[, -1]\n  y &lt;- Hitters_Without_NA[-test, 19]\n  test.mat &lt;- model.matrix(Salary ~ ., data = Hitters_Without_NA[test, ])\n  test.mat &lt;- test.mat[, -1]\n  ytest &lt;- Hitters_Without_NA[test, 19]\n  \n  \n  ridge.cv &lt;- cv.glmnet(train.mat, y, alpha = 0, lambda = seq(1, 5000))\n  lambdachoisi &lt;- ridge.cv$lambda.min\n  ridge.pred &lt;- predict(ridge.cv, test.mat, s = lambdachoisi)\n  err_ridge[i] &lt;- sqrt(mean((ridge.pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                              T))\n  \n  \n  lasso.cv &lt;- cv.glmnet(train.mat, y, alpha = 1)\n  lasso.pred &lt;- predict(lasso.cv, test.mat, s = lasso.cv$lambda.min)\n  err_lasso[i] &lt;- sqrt(mean((lasso.pred - Hitters_Without_NA[test, 19])^2, na.rm =\n                              T))\n  \n  # lars.cv &lt;- cv.lars(train.mat, y)\n  # choix &lt;- lars.cv$index[which.min(lars.cv$cv)]\n  # temp &lt;- lars(train.mat, y)\n  # lars.pred &lt;- predict(temp, train.mat, s=choix, mode='fraction')$fit\n  # err_lars[i] &lt;- sqrt(mean((lars.pred - Hitters_Without_NA[test,19])^2, na.rm=T))\n  \n}\n\nerr_pred_df &lt;- cbind(err_step, err_pcr, err_pls, err_ridge, err_lasso) %&gt;% as.data.frame()\ncolnames(err_pred_df) &lt;- c(\"Err_mod_step\", \"Err_PCR\", \"Err_PLS\", \"Err_Ridge\", \"Err_Lasso_glmnet\")\n\n\n\n\nCode\nmy_interactive_boxplot(err_pred_df)\n\n\n\n\n\n\n\n\nCode\nmethods &lt;- c(\"Err_mod_step\",\n             \"Err_PCR\",\n             \"Err_PLS\",\n             \"Err_Ridge\",\n             \"Err_Lasso_glmnet\")\nerrors.mean &lt;- sapply(list(err_step, err_pcr, err_pls, err_ridge, err_lasso), function(x) mean(x))\nerrors.median &lt;- sapply(list(err_step, err_pcr, err_pls, err_ridge, err_lasso), function(x) median(x))\nq1_values &lt;- sapply(list(err_step, err_pcr, err_pls, err_ridge, err_lasso), function(x) quantile(x, 0.25))\nq3_values &lt;- sapply(list(err_step, err_pcr, err_pls, err_ridge, err_lasso), function(x) quantile(x, 0.75))\niq_values &lt;- q3_values - q1_values\nerr.mean_pred_df &lt;- data.frame(\n  Method = methods,\n  Mean_Error = round(errors.mean, 3),\n  Median_Error = round(errors.median, 3),\n  InterQartile_Error = round(iq_values, 3)\n)\nerr.mean_pred_df %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nEn analysant les résultats, on constate que la méthode Ridge minimise à la fois l’erreur moyenne et l’erreur médiane, suivie de près par la méthode PCR, puis Lasso. En revanche, les valeurs d’erreurs sont moins variables pour la méthode Lasso, suivie de Ridge, et enfin PLSR.\nCela suggère que, pour choisir la méthode la plus efficace dans notre contexte, il convient de trouver un bon compromis entre la réduction de l’erreur moyenne et la faible variabilité des erreurs. Dans ce cas, les méthodes Ridge et Lasso semblent offrir le meilleur équilibre. Le choix final pourrait alors dépendre de la préférence accordée à une méthode qui minimise les fluctuations des erreurs.\n\n\n\n\n\n\n\nNote\n\n\n\nPour les curieux, il pourra être intéressant de faire varier la valeur de \\(K\\) et ainsi remarquer que les méthodes les plus performantes ne sont pas toujours les mêmes.\n\n\n\n\nConclusion\nEn conclusion, nous avons pu observer l’efficacité des méthodes Ridge et Lasso par rapport à PCR, PLSR et la sélection forward. Ces deux dernières méthodes se distinguent par une capacité à minimiser les erreurs moyennes tout en offrant une stabilité dans les prédictions.\n\nCependant, il est important de noter que PCR, PLSR et la sélection forward présentent aussi de bonnes performances et restent donc des méthodes pertinentes qui peuvent être particulièrement utiles dans certains contextes, en fonction des caractéristiques spécifiques des données et des objectifs de modélisation.\nAinsi, le choix de la méthode dépendra du compromis souhaité entre précision et variabilité des résultats.\n\n\nSession info\n\n\nCode\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-02-25\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n glmnet    * 4.1-8   2023-08-22 [1] CRAN (R 4.4.2)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.4.2)\n ISLR      * 1.4     2021-09-15 [1] CRAN (R 4.4.2)\n Matrix    * 1.6-5   2024-01-11 [4] CRAN (R 4.3.2)\n pls       * 2.8-5   2024-09-15 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_06.html",
    "href": "posts/Exercice_06.html",
    "title": "Exercice 06",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nRappels sur PCR et PLSR\nDans l’analyse des données et la modélisation statistique, la régression linéaire classique peut être limitée lorsque les variables explicatives sont fortement corrélées (problème de colinéarité) ou lorsque leur nombre est supérieur au nombre d’observations (problème de haute dimensionnalité). Pour remédier à ces défis, des méthodes de réduction de dimensionnalité comme la Régression sur Composantes Principales (PCR) et la Régression des Moindres Carrés Partiels (PLSR) sont utilisées.\n\nPCRPLSR\n\n\nLa régression sur Composantes Principales (PCR) repose sur une Analyse en Composantes Principales (ACP) pour transformer les variables explicatives en nouvelles variables orthogonales appelées composantes principales. Seules les premières composantes, capturant le plus de variance, sont conservées dans la régression.\n\nCette approche permet de réduire la multicolinéarité et d’éviter le sur-ajustement en limitant la complexité du modèle.\n\nCependant, la PCR ne prend pas en compte la relation entre les variables explicatives et la variable réponse lors de la sélection des composantes.\n\n\nContrairement à la PCR, la régression des Moindres Carrés Partiels (PLSR) cherche à maximiser la covariance entre les variables explicatives et la variable réponse.\nElle construit des composantes latentes qui capturent non seulement la variance des variables explicatives mais aussi leur corrélation avec la variable à prédire.\n\nCette méthode est souvent plus efficace que la PCR pour les problèmes de prédiction, car elle optimise directement la relation entre les prédicteurs et la réponse.\n\n\n\nEn résumé, la PCR est une approche basée sur la variance des prédicteurs, tandis que la PLSR optimise la relation entre les prédicteurs et la réponse.\nLe choix entre ces deux méthodes dépend du contexte : la PCR est utile pour la réduction de dimensionnalité, tandis que la PLSR est souvent plus performante pour la prédiction\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nCode\n# Données\nlibrary(ISLR)         # Hitters data \nlibrary(dplyr)        # manipulation des données\n\n# Infrence\nlibrary(pls) ## PCR et PLSR\n\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nPlot de validation\n\n\n\n\nCode\nmy_validationplot &lt;- function(mod, data) {\n  msep.cv &lt;- MSEP(mod, estimate = c(\"CV\", \"adjCV\"))\n  rmsep.cv &lt;- RMSEP(mod, estimate = c(\"CV\", \"adjCV\"))\n  \n  x_msep &lt;- c(msep.cv$val[1, , ], msep.cv$val[2, , ])\n  x_rmsep &lt;- c(rmsep.cv$val[1, , ], rmsep.cv$val[2, , ])\n  y &lt;- c(rep(\"CV\", length(msep.cv$val[2, , ])), rep(\"adjCV\", length(msep.cv$val[2, , ])))\n  \n  z &lt;- c(0:(ncol(data) - 1), 0:(ncol(data) - 1))\n  dt &lt;- data.frame(x_msep, x_rmsep, y, z)\n  colnames(dt) &lt;- c(\"MSEP\", \"RMSEP\", \"sample\", \"comps\")\n  \n  ## MSEP\n  p.msep &lt;- ggplot(dt, aes(x = comps, y = MSEP, col = sample)) +\n    geom_line() +\n    theme_bw() +\n    labs(\n      title = \"Évolution du MSEP en fonction du nombre de composantes\",\n      x = \"Nombre de composantes\",\n      y = \"RMSEP\",\n      color = \"Échantillon\"\n    ) +\n    theme(\n      plot.title = element_text(size = 16, face = \"bold\"),\n      axis.title = element_text(size = 14, face = \"bold\"),\n      axis.text = element_text(size = 12),\n      legend.title = element_text(size = 14, face = \"bold\"),\n      legend.text = element_text(size = 12)\n    )\n  \n  ## RMSEP\n  p.rmsep &lt;- ggplot(dt, aes(x = comps, y = RMSEP, col = sample)) +\n    geom_line() +\n    theme_bw() +\n    labs(\n      title = \"Évolution du RMSEP en fonction du nombre de composantes\",\n      x = \"Nombre de composantes\",\n      y = \"RMSEP\",\n      color = \"Échantillon\"\n    ) +\n    theme(\n      plot.title = element_text(size = 16, face = \"bold\"),\n      axis.title = element_text(size = 14, face = \"bold\"),\n      axis.text = element_text(size = 12),\n      legend.title = element_text(size = 14, face = \"bold\"),\n      legend.text = element_text(size = 12)\n    )\n  \n  ## Explain variance\n  explain_variance &lt;- explvar(mod)\n  \n  # Créer un data frame\n  dt_var &lt;- data.frame(comps = seq_along(explain_variance),\n                       variance = explain_variance * 100)\n                       \n  # Tracer le graphique\n  p.variance &lt;- ggplot(dt_var, aes(x = comps, y = variance)) +\n                         geom_line(color = \"blue\") +\n                         geom_point(color = \"red\") +\n                         theme_bw() +\n                         labs(title = \"Évolution de la Variance Expliquée en Fonction du Nombre de Composantes\", x = \"Nombre de Composantes\", y = \"Variance Expliquée (%)\") +\n                         theme(\n                           plot.title = element_text(size = 16, face = \"bold\"),\n                           axis.title = element_text(size = 14, face = \"bold\"),\n                           axis.text = element_text(size = 12)\n                         )\n                       \n                       \n  return(list(MSEP = p.msep, RMSEP = p.rmsep, Exp_Var = p.variance))\n}\n\n\n\n\n\n\n\n\n\nCode\nset.seed(140400)\n\n\n\n\n\n\n\nDonnées\nOn étudie à nouveau le jeu de données Hitters disponible dans la libraire {ISLR} de R. Il s’agit d’un jeu de données de la Major League Baseball provenant des saisons de 1986 et 1987.\nLe jeu de données possède 322 lignes/individus pour les différents joueurs et 20 variables.\nParmi les variables, on trouve les informations suivantes :\n\n\n\n\n\nAtBat\nNumber of times at bat in 1986\n\n\nHits\nNumber of hits in 1986\n\n\nHmRun\nNumber of home runs in 1986\n\n\nRuns\nNumber of runs in 1986\n\n\nRBI\nNumber of runs batted in in 1986\n\n\nWalks\nNumber of walks in 1986\n\n\nYears\nNumber of years in the major leagues\n\n\nCAtBat\nNumber of times at bat during his career\n\n\nCHits\nNumber of hits during his career\n\n\nCHmRun\nNumber of home runs during his career\n\n\nCRuns\nNumber of runs during his career\n\n\nCRBI\nNumber of runs batted in during his career\n\n\nCWalks\nNumber of walks during his career\n\n\nLeague\nA factor with levels A and N indicating player's league at the end of 1986\n\n\nDivision\nA factor with levels E and W indicating player's division at the end of 1986\n\n\nPutOuts\nNumber of put outs in 1986\n\n\nAssists\nNumber of assists in 1986\n\n\nErrors\nNumber of errors in 1986\n\n\nSalary\n1987 annual salary on opening day in thousands of dollars\n\n\nNewLeague\nA factor with levels A and N indicating player's league at the beginning of 1987\n\n\n\n\n\n\n\nComme pour l’Exercice 1, on va commencer par se débarasser des variables manquantes.\n\n\nCode\nHitters_Without_NA &lt;- Hitters %&gt;% na.omit()\n\n\nComme cela fait maintenant plusieurs fois que l’on fait affaire à ce jeu de données, on se passera des analyses descritpives faites en Exercice 1.\nAinsi, on va pouvoir tout de suite commencer par faire le découpage de notre jeu de données en échantillon train et test. Le jeu de données train contiendra 3/4 des individus sans valeurs manquantes de Hitters, tirés aléatoirement. Le reste du jeu de données composera l’échantillon test.\n\n\nCode\npercent_to_draw &lt;- 0.75\nindex_train &lt;- sample(nrow(Hitters_Without_NA), size = floor(percent_to_draw * nrow(Hitters_Without_NA)))\n\nHitters_train &lt;- Hitters_Without_NA[index_train, ]\n\nHitters_test &lt;- Hitters_Without_NA[-index_train, ]\n\n\n\n\nAnalyse Inférentielle\nOn va maintenant effectuer une régression PCR et une régression PLSR sur l’échantillon train en sélectionnant le nombre de composantes par une validation croisée K-fold où \\(K = 10\\).\n\nPCRPLSR\n\n\n\n\nCode\nmod_pcr &lt;- pcr(\n  Salary ~ .,\n  scale = TRUE,\n  data = Hitters_train,\n  validation = \"CV\",\n  segments = 10\n)\nmod_pcr %&gt;% summary()\n\n\nData:   X dimension: 197 19 \n    Y dimension: 197 1\nFit method: svdpc\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV           451.6    359.2    360.4    362.1    358.4    355.4    361.6\nadjCV        451.6    358.6    359.6    361.3    357.6    354.6    360.3\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       362.3    367.7    373.1     374.9     376.3     378.2     378.9\nadjCV    360.8    365.8    370.9     372.2     373.7     375.4     376.0\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        372.9     375.5       352     350.3     349.2     349.8\nadjCV     369.6     372.2       349     347.1     345.9     346.4\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.57    60.46     71.0    79.17    84.43    88.90    92.26     95.1\nSalary    39.51    40.48     40.6    42.04    43.04    44.32    45.36     45.8\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         96.48     97.43     98.12     98.70     99.18      99.5     99.75\nSalary    45.81     47.70     47.71     48.14     48.26      51.2     51.55\n        16 comps  17 comps  18 comps  19 comps\nX          99.91     99.97     99.99    100.00\nSalary     57.09     58.10     58.89     59.19\n\n\n\n\n\n\nCode\nmod_pls &lt;- plsr(\n  Salary ~ .,\n  scale = TRUE,\n  data = Hitters_train,\n  validation = \"CV\",\n  segments = 10\n)\nmod_pls %&gt;% summary()\n\n\nData:   X dimension: 197 19 \n    Y dimension: 197 1\nFit method: kernelpls\nNumber of components considered: 19\n\nVALIDATION: RMSEP\nCross-validated using 10 random segments.\n       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps\nCV           451.6    353.8    354.0    355.4    354.6    358.6    354.5\nadjCV        451.6    353.3    353.1    354.5    353.0    355.7    351.3\n       7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps\nCV       346.0    338.3    336.1     337.5     335.9     332.6     332.7\nadjCV    343.4    336.0    333.7     335.2     333.3     330.3     330.4\n       14 comps  15 comps  16 comps  17 comps  18 comps  19 comps\nCV        329.2     330.3     331.1     329.8     329.1     329.4\nadjCV     327.1     328.1     328.7     327.6     326.9     327.2\n\nTRAINING: % variance explained\n        1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps\nX         38.35    51.49    65.93     72.1    76.66    83.61    87.75    89.94\nSalary    41.91    45.71    47.66     49.8    52.71    54.18    55.37    56.92\n        9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps\nX         93.10     95.65     96.66     97.49     98.39     98.58     99.05\nSalary    57.52     57.78     58.24     58.40     58.52     58.81     58.88\n        16 comps  17 comps  18 comps  19 comps\nX          99.41     99.74     99.99    100.00\nSalary     58.97     59.05     59.09     59.19\n\n\n\n\n\nOn peut maintenant visualiser l’évolution du MSEP et RMSEP en fonction du nombre de composantes gardées.\n\n\n\n\n\n\nNote\n\n\n\nPour des raisons esthétiques, on à ici construit un graphique à partir de ggplot2mais on aurait pu se contenter d’utiliser la fonction validationplot de la library pls.\n\n\n\nPCRPLS\n\n\n\n\nCode\ngrid.arrange(my_validationplot(mod_pcr, Hitters_train)$RMSEP,\n             my_validationplot(mod_pcr, Hitters_train)$Exp_Var,\n             ncol=2)\n\n\n\n\n\n\n\n\n\nCode\ngrid.arrange(my_validationplot(mod_pls, Hitters_train)$RMSEP,\n             my_validationplot(mod_pls, Hitters_train)$Exp_Var,\n             ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nPour la PCR on peut voir courbe proche entre CV et adjCV avec une première valeur minimum qui semble se trouver à partir de 5 composantes.\nEnsuite la courbe remonte à nouveau pour redescendre progressivement. Et concernant le pourcentage de variance expliquée, on voit un coude au niveau de 5 composantes.\nTandus que pour la PLSR on voit plutôt que c’est à partir de 5 composantes que la décroissance commence. Et pour le pourcentage de variance expliquée, on voit un coude au niveau de 5 composantes.\n\nEt on peut alors récupérer le nombre de composantes à garder qui minimsent le MSEP et RMSEP.\n\n\nCode\nncomp.rmsep_pcr &lt;- which.min(RMSEP(mod_pcr, estimate = c(\"CV\"))$val[\"CV\",,])-1\nncomp.rmsep_pls &lt;- which.min(RMSEP(mod_pls, estimate = c(\"CV\"))$val[\"CV\",,])-1\n\n\n\n\n\n\nRésultats\n\n\nOn a que le nombre de composante à retenir est de 18 pour la PCR et 18 pour la PLSR.\n\n\n\nPrédiction\nOn va calculer le RMSEP calculé à partir de la prédiction pour l’échantillon test.\n\n\nCode\nhat_Hitters_test_mod_pcr &lt;- predict(mod_pcr,\n                                    Hitters_test,\n                                    ncomp = (which.min(RMSEP(mod_pcr, estimate = c(\"CV\"))$val[\"CV\", , ]) - 1))\nrmsep_mod_pcr_pred &lt;- sqrt(mean((hat_Hitters_test_mod_pcr - Hitters_test$Salary) ** 2))\n\nhat_df_test_salary.pls &lt;- predict(mod_pls,\n                                  Hitters_test,\n                                  ncomp = (which.min(RMSEP(mod_pls, estimate = c(\"CV\"))$val[\"CV\", , ]) - 1))\nrmsep_mod_pls_pred &lt;- sqrt(mean((hat_df_test_salary.pls - Hitters_test$Salary) ** 2))\n\n\n\n\nCode\nrmsep_pred_df &lt;- data.frame(\"prediction PCR\" = rmsep_mod_pcr_pred, \"prediction PLS\" = rmsep_mod_pls_pred) \nrownames(rmsep_pred_df) &lt;- \"RMSEP\"\nrmsep_pred_df \n\n\n      prediction.PCR prediction.PLS\nRMSEP       369.5149       373.6123\n\n\nLe choix final du modèle peut ainsi se reposer sur celui qui minimise la RMSEP pour la prediction de notre échantillon test.\n\n\nConclusion\nEn conclusion, on a ici 2 méthodes complémentaires permettant de construire des modèles linéaires pour des données de grandes dimension.\n\nCe sont des méthodes intuitives et robustes souvent utilisés par les statisticiens.\n\n\nSession info\n\n\nCode\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-02-25\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package    * version date (UTC) lib source\n dplyr      * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n forcats    * 1.0.0   2023-01-29 [1] CRAN (R 4.4.2)\n ggplot2    * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n gridExtra  * 2.3     2017-09-09 [1] CRAN (R 4.4.2)\n ISLR       * 1.4     2021-09-15 [1] CRAN (R 4.4.2)\n kableExtra * 1.4.0   2024-01-24 [1] CRAN (R 4.4.2)\n lubridate  * 1.9.4   2024-12-08 [1] CRAN (R 4.4.2)\n pls        * 2.8-5   2024-09-15 [1] CRAN (R 4.4.2)\n purrr      * 1.0.2   2023-08-10 [2] CRAN (R 4.3.3)\n readr      * 2.1.5   2024-01-10 [1] CRAN (R 4.4.2)\n stringr    * 1.5.1   2023-11-14 [2] CRAN (R 4.3.3)\n tibble     * 3.2.1   2023-03-20 [2] CRAN (R 4.3.3)\n tidyr      * 1.3.1   2024-01-24 [1] CRAN (R 4.4.2)\n tidyverse  * 2.0.0   2023-02-22 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_04.html",
    "href": "posts/Exercice_04.html",
    "title": "Exercice 4",
    "section": "",
    "text": "PackagesFonctions\n\n\n\n\nCode\n# Données\nlibrary(ISLR)         # Caravan data \nlibrary(dplyr)        # manipulation des données\n\n\nlibrary(car)          # pour VIF\n\n\n\n\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nboxplotHeatmapVIF plot\n\n\n\n\nCode\nmy_boxplot &lt;- function(data) {\n  # Transformer les données en format long pour ggplot\n  data_long &lt;- reshape2::melt(data)\n  \n  ggplot(data_long, aes(x = variable, y = value, fill = variable)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +  # Palette de couleurs harmonieuse\n    labs(title = \"Distribution des Variables (Boxplot)\", x = \"Variables\", y = \"Valeurs\") +\n    theme_minimal() +  # Thème épuré\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotation des étiquettes\n}\n\n\n\n\n\n\n\n\n\nCode\nmy_VIFplot &lt;- function(vif) {\n  vif_df &lt;- data.frame(Variable = names(vif), VIF = vif)\n  \n  p &lt;- ggplot(vif_df, aes(\n    x = reorder(Variable, VIF),\n    y = pmin(VIF, 15),\n    fill = VIF &gt; 10\n  )) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = ifelse(VIF &gt; 10, round(VIF, 1), \"\")), hjust = -0.2, size = 6) +\n    coord_flip() +\n    scale_fill_manual(values = c(\"FALSE\" = \"#0072B2\", \"TRUE\" = \"#D55E00\")) +\n    labs(title = \"Variance Inflation Factor (VIF)\", x = \"Variables\", y = \"VIF (limité à 15)\") +\n    theme_minimal() +\n    theme(\n      axis.title = element_text(size = 34, face = \"bold\"),\n      plot.title = element_text(\n        size = 54,\n        face = \"bold\",\n        hjust = 0.5\n      ),\n      axis.text.x = element_text(size = 26),\n      axis.text.y = element_text(size = 18),\n      legend.text = element_text(size = 30),\n      legend.title = element_text(size = 38, face = \"bold\")\n    )\n  \n  return(p)\n}"
  },
  {
    "objectID": "posts/Exercice_04.html#modèle-brut",
    "href": "posts/Exercice_04.html#modèle-brut",
    "title": "Exercice 4",
    "section": "Modèle brut",
    "text": "Modèle brut\nAjustons un modèle de régression logistique modélisant la probabilité de souscrire une assurance caravane en fonction de toutes les autres variables à disposition\n\n\nCode\nmod1 &lt;- glm(Caravan$Purchase~.,\n                family = binomial,\n                Caravan)\n\n\nWarning: glm.fit: des probabilités ont été ajustées numériquement à 0 ou 1\n\n\nCode\nmod1 %&gt;% summary()\n\n\n\nCall:\nglm(formula = Caravan$Purchase ~ ., family = binomial, data = Caravan)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.542e+02  1.116e+04   0.023  0.98183    \nMOSTYPE      6.580e-02  4.624e-02   1.423  0.15468    \nMAANTHUI    -1.832e-01  1.927e-01  -0.951  0.34157    \nMGEMOMV     -2.696e-02  1.399e-01  -0.193  0.84723    \nMGEMLEEF     2.096e-01  1.016e-01   2.063  0.03911 *  \nMOSHOOFD    -2.767e-01  2.076e-01  -1.333  0.18247    \nMGODRK      -1.142e-01  1.069e-01  -1.068  0.28535    \nMGODPR      -1.910e-02  1.177e-01  -0.162  0.87112    \nMGODOV      -1.618e-02  1.055e-01  -0.153  0.87818    \nMGODGE      -6.817e-02  1.113e-01  -0.612  0.54024    \nMRELGE       2.310e-01  1.566e-01   1.475  0.14031    \nMRELSA       8.509e-02  1.466e-01   0.580  0.56169    \nMRELOV       1.467e-01  1.562e-01   0.939  0.34759    \nMFALLEEN    -8.291e-02  1.311e-01  -0.633  0.52702    \nMFGEKIND    -1.154e-01  1.337e-01  -0.863  0.38813    \nMFWEKIND    -8.140e-02  1.417e-01  -0.575  0.56561    \nMOPLHOOG     9.717e-04  1.311e-01   0.007  0.99408    \nMOPLMIDD    -9.077e-02  1.365e-01  -0.665  0.50605    \nMOPLLAAG    -1.994e-01  1.376e-01  -1.449  0.14740    \nMBERHOOG     8.883e-02  9.349e-02   0.950  0.34204    \nMBERZELF     3.918e-02  9.897e-02   0.396  0.69219    \nMBERBOER    -1.169e-01  1.104e-01  -1.059  0.28951    \nMBERMIDD     1.353e-01  9.191e-02   1.472  0.14106    \nMBERARBG     3.976e-02  9.067e-02   0.438  0.66104    \nMBERARBO     9.954e-02  9.143e-02   1.089  0.27628    \nMSKA         2.690e-02  1.035e-01   0.260  0.79502    \nMSKB1       -8.801e-03  1.011e-01  -0.087  0.93064    \nMSKB2        1.200e-02  9.081e-02   0.132  0.89485    \nMSKC         9.016e-02  9.958e-02   0.905  0.36527    \nMSKD        -2.468e-02  9.724e-02  -0.254  0.79967    \nMHHUUR      -1.472e+01  8.140e+02  -0.018  0.98557    \nMHKOOP      -1.469e+01  8.140e+02  -0.018  0.98561    \nMAUT1        1.819e-01  1.514e-01   1.202  0.22953    \nMAUT2        1.507e-01  1.371e-01   1.099  0.27162    \nMAUT0        9.325e-02  1.436e-01   0.649  0.51603    \nMZFONDS     -1.445e+01  9.359e+02  -0.015  0.98768    \nMZPART      -1.451e+01  9.359e+02  -0.016  0.98763    \nMINKM30      1.181e-01  1.006e-01   1.174  0.24039    \nMINK3045     1.366e-01  9.650e-02   1.415  0.15694    \nMINK4575     1.009e-01  9.667e-02   1.043  0.29678    \nMINK7512     1.144e-01  1.027e-01   1.114  0.26513    \nMINK123M    -1.607e-01  1.449e-01  -1.109  0.26738    \nMINKGEM      9.214e-02  9.945e-02   0.927  0.35417    \nMKOOPKLA     6.856e-02  4.642e-02   1.477  0.13966    \nPWAPART      5.954e-01  3.901e-01   1.526  0.12693    \nPWABEDR     -2.757e-01  4.635e-01  -0.595  0.55196    \nPWALAND     -4.405e-01  1.035e+00  -0.425  0.67052    \nPPERSAUT     2.306e-01  4.199e-02   5.491 4.01e-08 ***\nPBESAUT      1.215e+01  4.029e+02   0.030  0.97595    \nPMOTSCO     -8.101e-02  1.147e-01  -0.706  0.48006    \nPVRAAUT     -2.106e+00  2.557e+03  -0.001  0.99934    \nPAANHANG     1.014e+00  9.371e-01   1.082  0.27917    \nPTRACTOR     7.229e-01  4.278e-01   1.690  0.09107 .  \nPWERKT      -5.525e+00  4.805e+03  -0.001  0.99908    \nPBROM        2.170e-01  4.865e-01   0.446  0.65559    \nPLEVEN      -2.382e-01  1.170e-01  -2.036  0.04173 *  \nPPERSONG    -4.523e-01  2.094e+00  -0.216  0.82901    \nPGEZONG      1.444e+00  1.029e+00   1.404  0.16033    \nPWAOREG      8.239e-01  5.943e-01   1.386  0.16565    \nPBRAND       2.401e-01  7.714e-02   3.113  0.00185 ** \nPZEILPL     -8.658e+00  3.261e+03  -0.003  0.99788    \nPPLEZIER    -1.886e-01  3.259e-01  -0.579  0.56289    \nPFIETS       3.664e-01  8.325e-01   0.440  0.65985    \nPINBOED     -1.068e+00  8.764e-01  -1.219  0.22301    \nPBYSTAND    -1.676e-01  3.321e-01  -0.505  0.61373    \nAWAPART     -9.293e-01  7.802e-01  -1.191  0.23364    \nAWABEDR      4.197e-01  1.082e+00   0.388  0.69824    \nAWALAND      2.762e-01  3.528e+00   0.078  0.93758    \nAPERSAUT    -3.902e-02  1.772e-01  -0.220  0.82566    \nABESAUT     -7.298e+01  2.417e+03  -0.030  0.97591    \nAMOTSCO      2.418e-01  3.772e-01   0.641  0.52142    \nAVRAAUT     -4.490e+00  1.078e+04   0.000  0.99967    \nAAANHANG    -1.351e+00  1.687e+00  -0.801  0.42322    \nATRACTOR    -2.376e+00  1.524e+00  -1.559  0.11899    \nAWERKT      -8.749e-01  9.682e+03   0.000  0.99993    \nABROM       -1.060e+00  1.549e+00  -0.684  0.49367    \nALEVEN       4.789e-01  2.245e-01   2.133  0.03291 *  \nAPERSONG     3.997e-01  4.329e+00   0.092  0.92644    \nAGEZONG     -3.163e+00  2.706e+00  -1.169  0.24247    \nAWAOREG     -3.212e+00  3.433e+00  -0.936  0.34939    \nABRAND      -4.118e-01  2.787e-01  -1.477  0.13956    \nAZEILPL      1.047e+01  3.261e+03   0.003  0.99744    \nAPLEZIER     2.516e+00  1.010e+00   2.490  0.01276 *  \nAFIETS       2.318e-01  5.699e-01   0.407  0.68420    \nAINBOED      1.947e+00  1.412e+00   1.378  0.16812    \nABYSTAND     1.078e+00  1.103e+00   0.977  0.32870    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2635.5  on 5821  degrees of freedom\nResidual deviance: 2243.5  on 5736  degrees of freedom\nAIC: 2415.5\n\nNumber of Fisher Scoring iterations: 17\n\n\non a ici un modèle avec beaucoup de variable. Mais si on analyse le summary, on constate que seulement 6 varaibales sont significative.\nRegardons un peu le VIF pour toutes les variables.\n\n\nCode\nmy_VIFplot(vif(mod1))\n\n\n\n\n\nOn constate la présence de beaucoup de variables avec un VIF très élevé et donc une forte colinéarité indiquant bien qu’il va falloir sélectionner les variables à garder dans notre modèle."
  },
  {
    "objectID": "posts/Exercice_04.html#sélecion-automatique",
    "href": "posts/Exercice_04.html#sélecion-automatique",
    "title": "Exercice 4",
    "section": "Sélecion automatique",
    "text": "Sélecion automatique\n\nAICBIC\n\n\n\nForwardBackwardBoth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nForwardBackwardBoth\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAprès toute ces modélisations, rappelons nous tout de même l’objectif de l’assureur est de démarcher des clients de manière ciblée pour leurs faire souscrire une assurance caravane. On pourrait alors de demander : s’il démarchait les clients de façon aléatoire, sans tenir compte de leurs caractéristiques, quel serait environ son taux de réussite ?\nPour cela il suffit juste de ce rappeler du pourcentage donné précédemment qui nous disait la proportion de oui actuellement.\n\n\nCode\nround(table(Caravan$Purchase)*100/nrow(Caravan), 3)\n\n\n\n    No    Yes \n94.023  5.977 \n\n\nLe pourcentage étant très bas, on va souhaiter utiliser l’un des 3 modèles estimés ci-dessus (le global, un de ceux sélectionnés par AIC et un de ceux sélectionnés par BIC) pour cibler les clients à démarcher.\nAinsi on regardera\nSi l’on choisissait de démarcher tous les clients ayant une probabilité de souscrire l’assurance supérieure à 0.5, quel pourcentage de clients cela représenterait il pour chacun des 3 modèles estimés ? Quel seuil faudrait-il choisir à la place de 0.5 pour que ce pourcentage corresponde à environ 6% des clients ? On décide dans la suite de fixer ce seuil à 0.2 et on cherche à sélectionner le meilleur modèle parmi les 3 précédents."
  },
  {
    "objectID": "posts/Exercice_02.html",
    "href": "posts/Exercice_02.html",
    "title": "Exercice 02",
    "section": "",
    "text": "Clément Poupelin, clementjc.poupelin@gmail.com"
  },
  {
    "objectID": "posts/Exercice_02.html#modèle-brut",
    "href": "posts/Exercice_02.html#modèle-brut",
    "title": "Exercice 02",
    "section": "Modèle brut",
    "text": "Modèle brut\nOn désire modéliser le salaire Salary en fonction des variables disponibles.\nOn va donc ajuster un modèle de régression linéaire en utilisant toutes les variables à disposition et analyser la qualité de cet ajustement.\n\n\nCode\nmod1 &lt;- lm(formula = Salary ~ .,\n           Hitters_Without_NA_18) \nmod1 %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ ., data = Hitters_Without_NA_18)\n\nResiduals:\nALL 18 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (2 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) -223.7909        NaN     NaN      NaN\nAtBat         -3.2428        NaN     NaN      NaN\nHits          13.1990        NaN     NaN      NaN\nHmRun        -60.8834        NaN     NaN      NaN\nRuns           0.6875        NaN     NaN      NaN\nRBI           10.3993        NaN     NaN      NaN\nWalks          7.0114        NaN     NaN      NaN\nYears         -2.3702        NaN     NaN      NaN\nCAtBat         0.2643        NaN     NaN      NaN\nCHits         -1.7919        NaN     NaN      NaN\nCHmRun         5.3897        NaN     NaN      NaN\nCRuns          4.0162        NaN     NaN      NaN\nCRBI          -4.0134        NaN     NaN      NaN\nCWalks         1.5822        NaN     NaN      NaN\nLeagueN      233.6380        NaN     NaN      NaN\nDivisionW    299.1771        NaN     NaN      NaN\nPutOuts       -0.1250        NaN     NaN      NaN\nAssists       -0.8539        NaN     NaN      NaN\nErrors             NA         NA      NA       NA\nNewLeagueN         NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 17 and 0 DF,  p-value: NA\n\n\n\n\n\n\nRésultats\n\n\nOn peut clairement constater que ce modèle brut ne fonctionne pas avec pourtant un \\(R^2 = 1\\). On retrouve donc le problème typique de l’analyse en grande dimension lorsque \\(p&gt;n\\) (fléau de la dimensionalité).\n\nOn peut aussi s’amuser à regarder les critères AIC et BIC de ce modèles qui théoriquement se retrouve à tendre vers l’infini.\n\n\nCode\ncat( \"AIC = \", AIC(mod1), \"et BIC = \", BIC(mod1))\n\n\nAIC =  -Inf et BIC =  -Inf\n\n\n\nPrediction\nOn va maintenant tenter de prédire la variable Salary pour les autres joueurs.\nDéjà on peut regarder sur les 18 joueurs si la prédiction via le modèle nous donne des bonnes valeur.\n\n\nCode\nSalary_hat &lt;- predict(mod1, Hitters_Without_NA_18)\nSalary &lt;- Hitters_Without_NA_18$Salary\n\n\n\n\\(\\widehat{Salary^{(1:18)}} - Salary^{(1:18)} =\\) 0\n\nCe que l’on constate c’est qu’effectivement nous sommes avec un résultat qui pourrait nous faire penser que le modèle est bien ajusté avec une prédiction quasiment égale à la variable à prédire.\nPourtant si nous regardons la prédiction obtenue par le modèle pour les autres joueurs et que nous effectuons la même soustraction pour comparer la qualité de prediction, nous voyons bien l’inéfficacité du modèle.\n\n\nCode\nHitters_Without_NA_No18 &lt;- Hitters_Without_NA[19:nrow(Hitters_Without_NA),]\nSalary_hat_No18 &lt;- predict(mod1, Hitters_Without_NA_No18)\nSalary_No18 &lt;- Hitters_Without_NA_No18$Salary\n\n\n\n\\(\\widehat{Salary^{(\\neg 1:18)}} - Salary^{(\\neg 1:18)} =\\) -70.88\n\nEn effet on voit bien au dessus que les valeurs ne sont en moyennes pas proche de 0."
  },
  {
    "objectID": "posts/Exercice_02.html#modèles-parcimonieux",
    "href": "posts/Exercice_02.html#modèles-parcimonieux",
    "title": "Exercice 02",
    "section": "Modèles parcimonieux",
    "text": "Modèles parcimonieux\nOn va maintenant mettre un oeuvre une méthode de sélection automatique classique pour réduire le nombre de variable explicative et tenter d’éviter les problèmes de grande dimension.\nPour cela nous allons donc partir du plus petit modèle (celui avec seulement l’intercept) puis faire grandir le nombre de variable. Il va donc s’agir d’une méthode de sélection automatique forward.\n\n\nCode\nmod0 &lt;- lm(Salary~1, Hitters_Without_NA_18)\nmod_forw &lt;- step(mod0,\n                 scope = formula(mod1),\n                 trace = FALSE,\n                 direction = c(\"forward\"))\nmod_forw %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ CWalks + League, data = Hitters_Without_NA_18)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-215.51  -82.67  -48.10   26.13  302.49 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  259.3539    77.1270   3.363  0.00427 ** \nCWalks         0.9699     0.1606   6.039 2.27e-05 ***\nLeagueN     -137.2850    79.1236  -1.735  0.10322    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 160.8 on 15 degrees of freedom\nMultiple R-squared:  0.7495,    Adjusted R-squared:  0.7161 \nF-statistic: 22.44 on 2 and 15 DF,  p-value: 3.095e-05\n\n\n\n\n\n\nRésultats\n\n\nNous obtenons maintenant un modèle avec 2 variable dont une significative. Puis nous pouvons constater des valeurs assez élevés pour le \\(R^2\\) et \\(R^2_{adjusted}\\).\nEt on a AIC = 238.692 et BIC = 242.253.\nDonc sans aller tester si c’est un bon modèle prédictif, on constate déjà qu’il va s’agir d’un modèle descriptif fonctionnel avec \\(n&lt;p\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistique en grande dimension",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n         \n          Modified - Oldest\n        \n         \n          Modified - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nExercice 01\n\n\n33 min\n\n\n\nRégression linéaire\n\n\nSélection automatique\n\n\n\nIl s’agit d’une première utilisation des méthodes de régression avec selection de variable via des approches stepwise sur des données de baseball\n\n\n\nClément Poupelin\n\n\nFeb 17, 2025\n\n\n\n\n\n2/26/25, 7:26:04 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 02\n\n\n8 min\n\n\n\nRégression linéaire\n\n\nSélection automatique\n\n\n\nOn continu sur les données de baseball en testant cette fois ci le lien linéaire existant et en mettant en avant le fléau de la dimensionalité\n\n\n\nClément Poupelin\n\n\nFeb 17, 2025\n\n\n\n\n\n2/26/25, 5:40:54 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 03\n\n\n12 min\n\n\n\nRégression linéaire\n\n\nValidation croisée\n\n\n\nPremière essais de techniques de validations croisées sur des données générées manuellement\n\n\n\nClément Poupelin\n\n\nFeb 17, 2025\n\n\n\n\n\n2/25/25, 9:29:19 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 04\n\n\n14 min\n\n\n\nRégression logistique\n\n\nSélection automatique\n\n\nValidation croisée\n\n\n\nUtilisation des précédentes techniques de selections de variable et de validation croisée dans le cadre de données de grandes dimension avec le jeu de données Caravan\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/25/25, 9:29:19 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 05\n\n\n14 min\n\n\n\nRégression logistique\n\n\nBiais de sélection\n\n\nCorrélations fortuites\n\n\nValidation croisée\n\n\n\nOn va illustrer dans ce document les problèmes de biais de séléction et de corrélation fortuite pour des données simulé\n\n\n\nClément Poupelin\n\n\nFeb 21, 2025\n\n\n\n\n\n2/25/25, 9:29:19 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 06\n\n\n11 min\n\n\n\nRégression sur composantes principales\n\n\nRégression des moindres carrés partiels\n\n\nValidation croisée\n\n\n\nOn reprend les données de baseball en mettant en pratique les techniques de PCR et PLSR\n\n\n\nClément Poupelin\n\n\nFeb 23, 2025\n\n\n\n\n\n2/26/25, 5:40:54 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 06 Bonus : Ridge vs Lasso\n\n\n16 min\n\n\n\nRégression Ridge\n\n\nRégression Lasso\n\n\nValidation croisée\n\n\n\nComparaison de la régression Ridge et Lasso via le github de Laurent Rouvière\n\n\n\nClément Poupelin\n\n\nFeb 21, 2025\n\n\n\n\n\n2/25/25, 9:29:19 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 07\n\n\n13 min\n\n\n\nRégression linéaire\n\n\nSélection automatique\n\n\nRégression sur composantes principales\n\n\nRégression des moindres carrés partiels\n\n\nRégression Ridge\n\n\nRégression Lasso\n\n\nValidation croisée\n\n\n\nOn continu sur les données de baseball avec les techniques de régression Ridge et Lasso. Puis on compare les résultats avec ceux obtenus précédemment\n\n\n\nClément Poupelin\n\n\nFeb 25, 2025\n\n\n\n\n\n2/26/25, 5:40:54 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 08\n\n\n8 min\n\n\n\nRégression linéaire\n\n\nSélection automatique\n\n\nRégression sur composantes principales\n\n\nRégression des moindres carrés partiels\n\n\nRégression Ridge\n\n\nRégression Lasso\n\n\nRégression adaptative Lasso\n\n\nRégression Gauss-Lasso\n\n\nRégression Elastic Net\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/25/25, 9:30:27 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 09\n\n\n15 min\n\n\n\nRégression Lasso\n\n\nRégression adaptative Lasso\n\n\nRégression Gauss-Lasso\n\n\nSélection automatique\n\n\n\nComparaison de différents modèles de regression\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/25/25, 9:31:13 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 09 Bonus : régression logistique pénélisée\n\n\n1 min\n\n\n\nRégression logistique\n\n\nRégression Ridge\n\n\nRégression Lasso\n\n\nValidation croisée\n\n\n\nRégression logistique pénélisée via le github de Laurent Rouvière\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/25/25, 9:29:19 AM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 10\n\n\n1 min\n\n\n\nRégression logistique\n\n\nRégression logistique Lasso\n\n\nSélection automatique\n\n\nValidation croisée\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/26/25, 5:40:54 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 11\n\n\n4 min\n\n\n\nTests multiples\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/26/25, 5:40:54 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercice 12\n\n\n1 min\n\n\n\nTests multiples\n\n\n\nDescription\n\n\n\nClément Poupelin\n\n\nInvalid Date\n\n\n\n\n\n2/25/25, 9:29:19 AM\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Ce blog est dédié à la reprise et des exercices de Statistique en grande dimension enseignée dans le cadre du Master 2 IS à Nantes Université pour l’année universitaire 2023-2024.\nIl s’inscrit dans une démarche pédagogique visant à proposer des analyses claires, bien structurées et reproductibles en lien avec les thématiques abordées durant le cours."
  },
  {
    "objectID": "about.html#contexte",
    "href": "about.html#contexte",
    "title": "About",
    "section": "",
    "text": "Ce blog est dédié à la reprise et des exercices de Statistique en grande dimension enseignée dans le cadre du Master 2 IS à Nantes Université pour l’année universitaire 2023-2024.\nIl s’inscrit dans une démarche pédagogique visant à proposer des analyses claires, bien structurées et reproductibles en lien avec les thématiques abordées durant le cours."
  },
  {
    "objectID": "about.html#objectif",
    "href": "about.html#objectif",
    "title": "About",
    "section": "Objectif",
    "text": "Objectif\nL’objectif de ce blog est de :\n\nFournir des analyses complètes et rigoureuses en réponse aux exercices abordés en travaux pratiques.\nProposer des solutions détaillées et commentées pour aider à la compréhension des méthodes statistiques.\nFavoriser une approche reproductible et bien documentée en utilisant R et Quarto."
  },
  {
    "objectID": "about.html#intervenant.e.s",
    "href": "about.html#intervenant.e.s",
    "title": "About",
    "section": "Intervenant.e.s",
    "text": "Intervenant.e.s\nLes rédacteurs et relecteurs des articles sont mentionnés sur chaque document du blog. Le travail collaboratif permet d’assurer la clarté, la rigueur et la qualité des contenus proposés."
  },
  {
    "objectID": "posts/Exercice_01.html",
    "href": "posts/Exercice_01.html",
    "title": "Exercice 01",
    "section": "",
    "text": "Clément Poupelin, clementjc.poupelin@gmail.com"
  },
  {
    "objectID": "posts/Exercice_01.html#modèle-brut",
    "href": "posts/Exercice_01.html#modèle-brut",
    "title": "Exercice 01",
    "section": "Modèle brut",
    "text": "Modèle brut\nOn désire modéliser le salaire Salary en fonction des variables disponibles.\nOn va donc ajuster un modèle de régression linéaire en utilisant toutes les variables à disposition et analyser la qualité de cet ajustement.\n\n\nCode\nmod1 &lt;- lm(formula = Salary ~ .,\n           Hitters_Without_NA) \nmod1 %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ ., data = Hitters_Without_NA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-907.62 -178.35  -31.11  139.09 1877.04 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  163.10359   90.77854   1.797 0.073622 .  \nAtBat         -1.97987    0.63398  -3.123 0.002008 ** \nHits           7.50077    2.37753   3.155 0.001808 ** \nHmRun          4.33088    6.20145   0.698 0.485616    \nRuns          -2.37621    2.98076  -0.797 0.426122    \nRBI           -1.04496    2.60088  -0.402 0.688204    \nWalks          6.23129    1.82850   3.408 0.000766 ***\nYears         -3.48905   12.41219  -0.281 0.778874    \nCAtBat        -0.17134    0.13524  -1.267 0.206380    \nCHits          0.13399    0.67455   0.199 0.842713    \nCHmRun        -0.17286    1.61724  -0.107 0.914967    \nCRuns          1.45430    0.75046   1.938 0.053795 .  \nCRBI           0.80771    0.69262   1.166 0.244691    \nCWalks        -0.81157    0.32808  -2.474 0.014057 *  \nLeagueN       62.59942   79.26140   0.790 0.430424    \nDivisionW   -116.84925   40.36695  -2.895 0.004141 ** \nPutOuts        0.28189    0.07744   3.640 0.000333 ***\nAssists        0.37107    0.22120   1.678 0.094723 .  \nErrors        -3.36076    4.39163  -0.765 0.444857    \nNewLeagueN   -24.76233   79.00263  -0.313 0.754218    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 315.6 on 243 degrees of freedom\nMultiple R-squared:  0.5461,    Adjusted R-squared:  0.5106 \nF-statistic: 15.39 on 19 and 243 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nNote\n\n\n\nNous sommes sur un modèle comprenant des variables quantitatives et qualitative ce qui le rapproche d’une modélisation ANOVA.\n\n\n\n\n\n\nRésultats\n\n\nCe qu’on peut remarquer en premier sur ce modèle c’est que beaucoup de variables ont un effet non significatif.\nAussi, ce modèle offre des valeurs de \\(R^2\\) et \\(R^2_{adjusted}\\) autour de 0.5 ce qui témoigne d’une mauvaise qualité d’ajustament du modèle\nEt enfin, l’écart type résiduel est de 315.6 ce qui est assez important et témoigne d’un modèle peu précis\n\nPour tenter de trouver un meilleur ajustment, il est important d’analyser d’avantage le lien entre toutes les variables explicatives. On utilise alors comunément le VIF (variance inflation factor).\nOn obtient alors pour chacune de nos variable une valeur qui, plus elle est élevé, témoigne de la multicolinéarité entre nos variables explicatives.\n\n\nCode\nvif(mod1) \n\n\n     AtBat       Hits      HmRun       Runs        RBI      Walks      Years \n 22.944366  30.281255   7.758668  15.246418  11.921715   4.148712   9.313280 \n    CAtBat      CHits     CHmRun      CRuns       CRBI     CWalks     League \n251.561160 502.954289  46.488462 162.520810 131.965858  19.744105   4.134115 \n  Division    PutOuts    Assists     Errors  NewLeague \n  1.075398   1.236317   2.709341   2.214543   4.099063 \n\n\nCode\nmy_VIFplot(vif(mod1))\n\n\n\n\n\nOn remarque ainsi que beaucoup de valeurs sont supérieur à 10 ce qui s’interprète communément comme la précence d’une forte colinéarité sur nos variables explicatives.\n\n\n\n\n\n\nNote\n\n\n\nCette colinéarité se constatait déjà durant les analyses descriptive via les graphes de corrélations (d’où l’importance de ne pas se lancer trop rapidement dans les analyses inférentielles)."
  },
  {
    "objectID": "posts/Exercice_01.html#modèles-parcimonieux",
    "href": "posts/Exercice_01.html#modèles-parcimonieux",
    "title": "Exercice 01",
    "section": "Modèles parcimonieux",
    "text": "Modèles parcimonieux\nMaintenant, on va donc tenter de trouver le meilleur sous-modèle possible. Pour cela on va suivre la procédure suivante :\n\nmettre en oeuvre une méthode de sélection automatique exhaustive et observer l’évolution des SCR (Sommes de Carrés Résiduels) pour les modèles retenus en fonction de leur taille.\ndéduire de ces SCR le \\(R^2\\), \\(R^2_{adjusted}\\), AIC, BIC et \\(C_p\\) correspondants. Les comparer avec les valeurs fournies dans le summary de regsubsets et tracer leur évolution en fonction de la taille du modèle.\n\nPuis reproduire la même procédure avec des séléctions backward, forward et stepwise\nTout d’abord, on va définir un nouveau modèle simple ne comprenant que l’intercept.\n\n\nCode\nmod0 &lt;- lm(Salary~1,\n           Hitters_Without_NA)\nsummary(mod0)$call\n\n\nlm(formula = Salary ~ 1, data = Hitters_Without_NA)\n\n\nLe principe de ces méthodes de régression avec sélection de variables est de tester et comparer les différents modèles possibles avec nos variables en allant du modèle le plus simple \\(y \\sim 1\\) jusqu’au modèle le plus complet \\(y \\sim .\\) ou l’inverse. Puis à chaque étape de rajout ou de suppression d’une variable dans le modèles, on sélectionne le meilleur modèle via le critère de notre choix.\n\n\n\n\n\n\nNote\n\n\n\nUn rappel sur nos critère se trouve dans la partie Setup, onglet Fonction, de ce document avec la création de fonction pour les calculer.\n\n\n\nExhaustiveBackwardForwardBoth\n\n\n\nModèleEvolution des critèresMeilleur modèle\n\n\n\n\nCode\nselec_auto &lt;- regsubsets(Salary~.,\n                         Hitters_Without_NA,\n                         method = \"exhaustive\",\n                         nvmax = 19 # maximum size of subsets to examine\n                         )\n# selec_auto %&gt;% summary()\n\n\nOn va déjà commencer par regarder la valeur du critère en fonction des variables des différents modèles testés.\n\n\nCode\npar(mfrow=c(2,2))\nplot(selec_auto, scale = 'bic') \nplot(selec_auto, scale = 'Cp') \nplot(selec_auto, scale = 'r2') \nplot(selec_auto, scale = 'adjr2') \n\n\n\n\n\n\n\n\n\nRésultats\n\n\nIci on remarque clairement que toutes nos variables ne sont pas gardés lorsque l’on cherche à optimiser nos critères.\nAussi, on peut voir encore de faibles valeurs pour les \\(R^2\\) et \\(R^2_{adjusted}\\) pouvant témoignés d’un mauvais ajustement de modèle.\n\n\n\n\n\n\n\nNote\n\n\n\nplot.regsubsets de {leaps} ne prend pas directement “aic” comme option de scale. Pour une sélection basée sur AIC, une approche alternative consiste à utiliser la fonction stepAIC du package {MASS}, qui permet une sélection pas à pas basée sur AIC.\n\n\n\n\nRegardons un peut l’évolution de la Somme des Carrés Résiduels (SCR).\n\n\nCode\nSCR &lt;- summary(selec_auto)$rss\nCriteria_plot(SCR, crit_name = \"Somme des Carrés Résiduels\")\n\n\n\n\n\nMaintenant regardons les autres critères mentionné précédemment\n\n\nCode\nr2 &lt;- r2_fun(Hitters_Without_NA$Salary, SCR)\nr2a &lt;- r2a_fun(Hitters_Without_NA$Salary, SCR)\ncp &lt;- cp_fun(mod1, SCR)\naic &lt;- aic_fun(SCR)\nbic &lt;- bic_fun(SCR)\n\ngrid.arrange(Criteria_plot(r2, crit_name = \"R2\"),\n             Criteria_plot(r2a, crit_name = \"R2 ajusté\"),\n             Criteria_plot(cp, crit_name = \"Cp\"),\n             Criteria_plot(aic, crit_name = \"AIC\"),\n             Criteria_plot(bic, crit_name = \"BIC\"),\n             ncol = 3)\n\n\n\n\n\nOn peut ainsi voir que ce sont plutot des modèles entre 5 et 10 variables qui optimisent nos critères (donc pas un modèle avec toutes nos variables).\n\n\n\n\n\n\nRésultats\n\n\nRegardons donc pour chaque critère quel est le modèle qui resort comme le meilleur\n\n\nCode\ncriteria_df &lt;- data.frame(r2, r2a, cp, aic, bic)\nBest_model(selec_auto, criteria_df)\n\n\nMeilleur modèle selon r2  =  0.546  : Modèle avec 19 variables\n (Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN \n \nMeilleur modèle selon r2a  =  0.523  : Modèle avec 11 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists \n \nMeilleur modèle selon cp  =  5.009  : Modèle avec 10 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks DivisionW PutOuts Assists \n \nMeilleur modèle selon aic  =  3031.258  : Modèle avec 10 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks DivisionW PutOuts Assists \n \nMeilleur modèle selon bic  =  3065.851  : Modèle avec 6 variables\n (Intercept) AtBat Hits Walks CRBI DivisionW PutOuts \n \n\n\n\n\n\n\n\n\nCette fois ci on va regarder en sélection backward. D’abord, on fait à nouveau avec la fonction regsubset.\n\nModèleEvolution des critèresMeilleur modèle\n\n\n\n\nCode\nselec_back &lt;- regsubsets(Salary~.,\n                         Hitters_Without_NA,\n                         method = \"backward\",\n                         nvmax = 19)\n\n\n\n\nCode\npar(mfrow=c(2,2))\nplot(selec_back, scale = 'bic') \nplot(selec_back, scale = 'Cp') \nplot(selec_back, scale = 'r2') \nplot(selec_back, scale = 'adjr2') \n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn remarque à nouveau une importante sélection de variables.\nMais ici aussi on a encore de faibles valeurs pour les \\(R^2\\) et \\(R^2_{adjusted}\\) pouvant témoignés d’un mauvais ajustement de modèle.\n\n\n\n\n\nCode\nSCR &lt;- summary(selec_back)$rss\nr2 &lt;- r2_fun(Hitters_Without_NA$Salary, SCR)\nr2a &lt;- r2a_fun(Hitters_Without_NA$Salary, SCR)\ncp &lt;- cp_fun(mod1, SCR)\naic &lt;- aic_fun(SCR)\nbic &lt;- bic_fun(SCR)\n\ngrid.arrange(Criteria_plot(r2, crit_name = \"R2\"),\n             Criteria_plot(r2a, crit_name = \"R2 ajusté\"),\n             Criteria_plot(cp, crit_name = \"Cp\"),\n             Criteria_plot(SCR, crit_name = \"Somme des Carrés Résiduels\"),\n             Criteria_plot(aic, crit_name = \"AIC\"),\n             Criteria_plot(bic, crit_name = \"BIC\"),\n             ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nRegardons donc pour chaque critère quel est le modèle qui resort comme le meilleur\n\n\nCode\ncriteria_df &lt;- data.frame(r2, r2a, cp, aic, bic)\nBest_model(selec_back, criteria_df)\n\n\nMeilleur modèle selon r2  =  0.546  : Modèle avec 19 variables\n (Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN \n \nMeilleur modèle selon r2a  =  0.523  : Modèle avec 11 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists \n \nMeilleur modèle selon cp  =  5.009  : Modèle avec 10 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks DivisionW PutOuts Assists \n \nMeilleur modèle selon aic  =  3031.258  : Modèle avec 10 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks DivisionW PutOuts Assists \n \nMeilleur modèle selon bic  =  3066.386  : Modèle avec 8 variables\n (Intercept) AtBat Hits Walks CRuns CRBI CWalks DivisionW PutOuts \n \n\n\n\n\n\n\n\nUtilisation de la fonction step\n\nOn peut également utiliser la fonction step de la library {stats}. Pour cela, on part du plus gros modèle défini précédemment par mod1.\nLa fonction step nous propose quel critère nous voulons utiliser pour la sélection entre le BIC, AIC et \\(C_p\\).\n\nBICAIC\\(C_p\\)\n\n\n\n\nCode\nn &lt;- nrow(Hitters_Without_NA)\nmodselect_back_bic &lt;- step(mod1,\n                       scope = formula(mod1),\n                       trace = FALSE, # trace = TRUE permet de voir le détail des étapes\n                       direction = c(\"backward\"),\n                       k = log(n) # BIC selection\n                       )\n\n\nPuis on peut regarder le modèle qui optimise le critère utilisé pour la selection.\n\n\nCode\nmodselect_back_bic %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ AtBat + Hits + Walks + CRuns + CRBI + CWalks + \n    Division + PutOuts, data = Hitters_Without_NA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-794.06 -171.94  -28.48  133.36 2017.83 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  117.15204   65.07016   1.800 0.072985 .  \nAtBat         -2.03392    0.52282  -3.890 0.000128 ***\nHits           6.85491    1.65215   4.149 4.56e-05 ***\nWalks          6.44066    1.52212   4.231 3.25e-05 ***\nCRuns          0.70454    0.24869   2.833 0.004981 ** \nCRBI           0.52732    0.18861   2.796 0.005572 ** \nCWalks        -0.80661    0.26395  -3.056 0.002483 ** \nDivisionW   -123.77984   39.28749  -3.151 0.001824 ** \nPutOuts        0.27539    0.07431   3.706 0.000259 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 314.7 on 254 degrees of freedom\nMultiple R-squared:  0.5281,    Adjusted R-squared:  0.5133 \nF-statistic: 35.54 on 8 and 254 DF,  p-value: &lt; 2.2e-16\n\n\nLa fonction step propose aussi une selection avec AIC et Cp.\n\n\n\n\nCode\nmodselect_back_aic &lt;- step(mod1,\n                       scope = formula(mod1),\n                       trace = FALSE, \n                       direction = c(\"backward\"),\n                       k = 2 # AIC selection\n                       )\n\nmodselect_back_aic %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + \n    CRBI + CWalks + Division + PutOuts + Assists, data = Hitters_Without_NA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-939.11 -176.87  -34.08  130.90 1910.55 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  162.53544   66.90784   2.429 0.015830 *  \nAtBat         -2.16865    0.53630  -4.044 7.00e-05 ***\nHits           6.91802    1.64665   4.201 3.69e-05 ***\nWalks          5.77322    1.58483   3.643 0.000327 ***\nCAtBat        -0.13008    0.05550  -2.344 0.019858 *  \nCRuns          1.40825    0.39040   3.607 0.000373 ***\nCRBI           0.77431    0.20961   3.694 0.000271 ***\nCWalks        -0.83083    0.26359  -3.152 0.001818 ** \nDivisionW   -112.38006   39.21438  -2.866 0.004511 ** \nPutOuts        0.29737    0.07444   3.995 8.50e-05 ***\nAssists        0.28317    0.15766   1.796 0.073673 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.8 on 252 degrees of freedom\nMultiple R-squared:  0.5405,    Adjusted R-squared:  0.5223 \nF-statistic: 29.64 on 10 and 252 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nCode\nmodselect_back_cp &lt;- step(mod1,\n                       scope = formula(mod1),\n                       trace = FALSE, \n                       direction = c(\"backward\"),\n                       k = 1 # Cp selection\n                       )\n\nmodselect_back_cp %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ AtBat + Hits + Walks + CAtBat + CRuns + \n    CRBI + CWalks + League + Division + PutOuts + Assists, data = Hitters_Without_NA)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-932.2 -175.4  -29.2  130.4 1897.2 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  135.75122   71.34623   1.903 0.058223 .  \nAtBat         -2.12775    0.53746  -3.959 9.81e-05 ***\nHits           6.92370    1.64612   4.206 3.62e-05 ***\nWalks          5.62028    1.59064   3.533 0.000488 ***\nCAtBat        -0.13899    0.05609  -2.478 0.013870 *  \nCRuns          1.45533    0.39270   3.706 0.000259 ***\nCRBI           0.78525    0.20978   3.743 0.000225 ***\nCWalks        -0.82286    0.26361  -3.121 0.002010 ** \nLeagueN       43.11162   39.96612   1.079 0.281755    \nDivisionW   -111.14603   39.21835  -2.834 0.004970 ** \nPutOuts        0.28941    0.07478   3.870 0.000139 ***\nAssists        0.26883    0.15816   1.700 0.090430 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.7 on 251 degrees of freedom\nMultiple R-squared:  0.5426,    Adjusted R-squared:  0.5226 \nF-statistic: 27.07 on 11 and 251 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nAvec la fonction step nous avons le modèle utilisant le critère BIC qui nous permet d’avoir toutes nos variables significatives (sauf l’intercept ce qui pourrait nous donner envie de faire des modèles sans).\nPar contre, nos valeurs de \\(R^2\\) et \\(R^2_{adjusted}\\) sont encore assez faible pour considérer ces modèles comme étant de très bonne qualité.\n\n\n\n\n\n\n\nNote\n\n\n\nAussi, si on veut comparer le modèle initial et le modèle final, on peut utiliser la fonction anova. Cela permet de voir si la sélection de variables a significativement amélioré l’ajustement.\n\n\nCode\nanova(mod0, modselect_back_bic, test = \"Chisq\")\n\n\nAnalysis of Variance Table\n\nModel 1: Salary ~ 1\nModel 2: Salary ~ AtBat + Hits + Walks + CRuns + CRBI + CWalks + Division + \n    PutOuts\n  Res.Df      RSS Df Sum of Sq  Pr(&gt;Chi)    \n1    262 53319113                           \n2    254 25159234  8  28159879 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDe manière logique on voit donc tout de même que le modèle avec nos variable explicatives sélectionnées (ici celui avec selection via le critère BIC) propose un meilleur ajustement que celui sans variable.\n\n\n\n\n\nCette fois ci on va regarder en sélection forward. D’abord, on fait à nouveau avec la fonction regsubset.\n\nModèleEvolution des critèresMeilleur modèle\n\n\n\n\nCode\nselec_forw &lt;- regsubsets(Salary~.,\n                         Hitters_Without_NA,\n                         method = \"forward\",\n                         nvmax = 19)\n\n\n\n\nCode\npar(mfrow=c(2,2))\nplot(selec_forw, scale = 'bic') \nplot(selec_forw, scale = 'Cp') \nplot(selec_forw, scale = 'r2') \nplot(selec_forw, scale = 'adjr2') \n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn remarque à nouveau une importante sélection de variables.\nMais ici aussi on a encore de faibles valeurs pour les \\(R^2\\) et \\(R^2_{adjusted}\\) pouvant témoignés d’un mauvais ajustement de modèle.\n\n\n\n\n\nCode\nSCR &lt;- summary(selec_forw)$rss\nr2 &lt;- r2_fun(Hitters_Without_NA$Salary, SCR)\nr2a &lt;- r2a_fun(Hitters_Without_NA$Salary, SCR)\ncp &lt;- cp_fun(mod1, SCR)\naic &lt;- aic_fun(SCR)\nbic &lt;- bic_fun(SCR)\n\ngrid.arrange(Criteria_plot(r2, crit_name = \"R2\"),\n             Criteria_plot(r2a, crit_name = \"R2 ajusté\"),\n             Criteria_plot(cp, crit_name = \"Cp\"),\n             Criteria_plot(SCR, crit_name = \"Somme des Carrés Résiduels\"),\n             Criteria_plot(aic, crit_name = \"AIC\"),\n             Criteria_plot(bic, crit_name = \"BIC\"),\n             ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nRegardons donc pour chaque critère quel est le modèle qui resort comme le meilleur\n\n\nCode\ncriteria_df &lt;- data.frame(r2, r2a, cp, aic, bic)\nBest_model(selec_forw, criteria_df)\n\n\nMeilleur modèle selon r2  =  0.546  : Modèle avec 19 variables\n (Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN \n \nMeilleur modèle selon r2a  =  0.523  : Modèle avec 11 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists \n \nMeilleur modèle selon cp  =  5.009  : Modèle avec 10 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks DivisionW PutOuts Assists \n \nMeilleur modèle selon aic  =  3031.258  : Modèle avec 10 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks DivisionW PutOuts Assists \n \nMeilleur modèle selon bic  =  3065.851  : Modèle avec 6 variables\n (Intercept) AtBat Hits Walks CRBI DivisionW PutOuts \n \n\n\n\n\n\n\n\nUtilisation de la fonction step\n\nOn peut également utiliser la fonction step de la library {stats}. Cette fois ci il faut définir en modèle de départ le plus petit modèle (celui composé seulement de l’intercept).\nLa fonction step nous propose quel critère nous voulons utiliser pour la sélection entre le BIC, AIC et \\(C_p\\).\n\nBICAIC\\(C_p\\)\n\n\n\n\nCode\nmodselect_forw_bic &lt;- step(mod0,\n                       scope = formula(mod1),\n                       trace = FALSE, # trace = TRUE permet de voir le détail des étapes\n                       direction = c(\"forward\"),\n                       k = log(n) # BIC selection\n                       )\n\n\nPuis on peut regarder le modèle qui optimise le critère utilisé pour la selection.\n\n\nCode\nmodselect_forw_bic %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ CRBI + Hits + PutOuts + Division + AtBat + \n    Walks, data = Hitters_Without_NA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-873.11 -181.72  -25.91  141.77 2040.47 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   91.51180   65.00006   1.408 0.160382    \nCRBI           0.64302    0.06443   9.979  &lt; 2e-16 ***\nHits           7.60440    1.66254   4.574 7.46e-06 ***\nPutOuts        0.26431    0.07477   3.535 0.000484 ***\nDivisionW   -122.95153   39.82029  -3.088 0.002239 ** \nAtBat         -1.86859    0.52742  -3.543 0.000470 ***\nWalks          3.69765    1.21036   3.055 0.002488 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 319.9 on 256 degrees of freedom\nMultiple R-squared:  0.5087,    Adjusted R-squared:  0.4972 \nF-statistic: 44.18 on 6 and 256 DF,  p-value: &lt; 2.2e-16\n\n\nLa fonction step propose aussi une selection avec AIC et Cp.\n\n\n\n\nCode\nmodselect_forw_aic &lt;- step(mod0,\n                       scope = formula(mod1),\n                       trace = FALSE, \n                       direction = c(\"forward\"),\n                       k = 2 # AIC selection\n                       )\n\nmodselect_forw_aic %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ CRBI + Hits + PutOuts + Division + AtBat + \n    Walks + CWalks + CRuns + CAtBat + Assists, data = Hitters_Without_NA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-939.11 -176.87  -34.08  130.90 1910.55 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  162.53544   66.90784   2.429 0.015830 *  \nCRBI           0.77431    0.20961   3.694 0.000271 ***\nHits           6.91802    1.64665   4.201 3.69e-05 ***\nPutOuts        0.29737    0.07444   3.995 8.50e-05 ***\nDivisionW   -112.38006   39.21438  -2.866 0.004511 ** \nAtBat         -2.16865    0.53630  -4.044 7.00e-05 ***\nWalks          5.77322    1.58483   3.643 0.000327 ***\nCWalks        -0.83083    0.26359  -3.152 0.001818 ** \nCRuns          1.40825    0.39040   3.607 0.000373 ***\nCAtBat        -0.13008    0.05550  -2.344 0.019858 *  \nAssists        0.28317    0.15766   1.796 0.073673 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.8 on 252 degrees of freedom\nMultiple R-squared:  0.5405,    Adjusted R-squared:  0.5223 \nF-statistic: 29.64 on 10 and 252 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nCode\nmodselect_forw_cp &lt;- step(mod0,\n                       scope = formula(mod1),\n                       trace = FALSE, \n                       direction = c(\"forward\"),\n                       k = 1 # Cp selection\n                       )\n\nmodselect_forw_cp %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ CRBI + Hits + PutOuts + Division + AtBat + \n    Walks + CWalks + CRuns + CAtBat + Assists + League, data = Hitters_Without_NA)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-932.2 -175.4  -29.2  130.4 1897.2 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  135.75122   71.34623   1.903 0.058223 .  \nCRBI           0.78525    0.20978   3.743 0.000225 ***\nHits           6.92370    1.64612   4.206 3.62e-05 ***\nPutOuts        0.28941    0.07478   3.870 0.000139 ***\nDivisionW   -111.14603   39.21835  -2.834 0.004970 ** \nAtBat         -2.12775    0.53746  -3.959 9.81e-05 ***\nWalks          5.62028    1.59064   3.533 0.000488 ***\nCWalks        -0.82286    0.26361  -3.121 0.002010 ** \nCRuns          1.45533    0.39270   3.706 0.000259 ***\nCAtBat        -0.13899    0.05609  -2.478 0.013870 *  \nAssists        0.26883    0.15816   1.700 0.090430 .  \nLeagueN       43.11162   39.96612   1.079 0.281755    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.7 on 251 degrees of freedom\nMultiple R-squared:  0.5426,    Adjusted R-squared:  0.5226 \nF-statistic: 27.07 on 11 and 251 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nAvec la fonction step nous avons le modèle utilisant le critère BIC qui nous permet d’avoir toutes nos variables significatives (sauf l’intercept ce qui pourrait nous donner envie de faire des modèles sans).\nPar contre, nos valeurs de \\(R^2\\) et \\(R^2_{adjusted}\\) sont encore assez faible pour considérer ces modèles comme étant de très bonne qualité.\n\n\n\n\n\n\n\nNote\n\n\n\nAussi, si on veut comparer le modèle initial et le modèle final, on peut utiliser la fonction anova. Cela permet de voir si la sélection de variables a significativement amélioré l’ajustement.\n\n\nCode\nanova(mod0, modselect_forw_bic, test = \"Chisq\")\n\n\nAnalysis of Variance Table\n\nModel 1: Salary ~ 1\nModel 2: Salary ~ CRBI + Hits + PutOuts + Division + AtBat + Walks\n  Res.Df      RSS Df Sum of Sq  Pr(&gt;Chi)    \n1    262 53319113                           \n2    256 26194904  6  27124209 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDe manière logique on voit donc tout de même que le modèle avec nos variable explicatives sélectionnées (ici celui avec selection via le critère BIC) propose un meilleur ajustement que celui sans variable.\n\n\n\n\n\nMaintenant on va regarder en sélection both. D’abord, on fait à nouveau avec la fonction regsubset.\n\nModèleEvolution des critèresMeilleur modèle\n\n\n\n\nCode\nselec_seq &lt;- regsubsets(Salary~.,\n                         Hitters_Without_NA,\n                         method = \"seqrep\",\n                         nvmax = 19)\n\n\n\n\nCode\npar(mfrow=c(2,2))\nplot(selec_seq, scale = 'bic') \nplot(selec_seq, scale = 'Cp') \nplot(selec_seq, scale = 'r2') \nplot(selec_seq, scale = 'adjr2') \n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn remarque à nouveau une importante sélection de variables.\nMais ici aussi on a encore de faibles valeurs pour les \\(R^2\\) et \\(R^2_{adjusted}\\) pouvant témoignés d’un mauvais ajustement de modèle.\n\n\n\n\n\nCode\nSCR &lt;- summary(selec_seq)$rss\nr2 &lt;- r2_fun(Hitters_Without_NA$Salary, SCR)\nr2a &lt;- r2a_fun(Hitters_Without_NA$Salary, SCR)\ncp &lt;- cp_fun(mod1, SCR)\naic &lt;- aic_fun(SCR)\nbic &lt;- bic_fun(SCR)\n\ngrid.arrange(Criteria_plot(r2, crit_name = \"R2\"),\n             Criteria_plot(r2a, crit_name = \"R2 ajusté\"),\n             Criteria_plot(cp, crit_name = \"Cp\"),\n             Criteria_plot(SCR, crit_name = \"Somme des Carrés Résiduels\"),\n             Criteria_plot(aic, crit_name = \"AIC\"),\n             Criteria_plot(bic, crit_name = \"BIC\"),\n             ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nRegardons donc pour chaque critère quel est le modèle qui resort comme le meilleur\n\n\nCode\ncriteria_df &lt;- data.frame(r2, r2a, cp, aic, bic)\nBest_model(selec_seq, criteria_df)\n\n\nMeilleur modèle selon r2  =  0.546  : Modèle avec 19 variables\n (Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN \n \nMeilleur modèle selon r2a  =  0.523  : Modèle avec 11 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks LeagueN DivisionW PutOuts Assists \n \nMeilleur modèle selon cp  =  5.009  : Modèle avec 10 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks DivisionW PutOuts Assists \n \nMeilleur modèle selon aic  =  3031.258  : Modèle avec 10 variables\n (Intercept) AtBat Hits Walks CAtBat CRuns CRBI CWalks DivisionW PutOuts Assists \n \nMeilleur modèle selon bic  =  3065.851  : Modèle avec 6 variables\n (Intercept) AtBat Hits Walks CRBI DivisionW PutOuts \n \n\n\n\n\n\n\n\nUtilisation de la fonction step\n\nOn peut également utiliser la fonction step de la library {stats}.\nLa fonction step nous propose quel critère nous voulons utiliser pour la sélection entre le BIC, AIC et \\(C_p\\).\n\nBICAIC\\(C_p\\)\n\n\n\n\nCode\nmodselect_seq_bic &lt;- step(mod0,\n                  scope = formula(mod1),\n                  trace = FALSE,\n                  direction = c(\"both\"),\n                  k = log(n))\nmodselect_seq_bic %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ CRBI + Hits + PutOuts + Division + AtBat + \n    Walks, data = Hitters_Without_NA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-873.11 -181.72  -25.91  141.77 2040.47 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   91.51180   65.00006   1.408 0.160382    \nCRBI           0.64302    0.06443   9.979  &lt; 2e-16 ***\nHits           7.60440    1.66254   4.574 7.46e-06 ***\nPutOuts        0.26431    0.07477   3.535 0.000484 ***\nDivisionW   -122.95153   39.82029  -3.088 0.002239 ** \nAtBat         -1.86859    0.52742  -3.543 0.000470 ***\nWalks          3.69765    1.21036   3.055 0.002488 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 319.9 on 256 degrees of freedom\nMultiple R-squared:  0.5087,    Adjusted R-squared:  0.4972 \nF-statistic: 44.18 on 6 and 256 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nCode\nmodselect_seq_aic &lt;- step(mod0,\n                  scope = formula(mod1),\n                  trace = FALSE,\n                  direction = c(\"both\"),\n                  k = 2)\nmodselect_seq_aic %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ CRBI + Hits + PutOuts + Division + AtBat + \n    Walks + CWalks + CRuns + CAtBat + Assists, data = Hitters_Without_NA)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-939.11 -176.87  -34.08  130.90 1910.55 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  162.53544   66.90784   2.429 0.015830 *  \nCRBI           0.77431    0.20961   3.694 0.000271 ***\nHits           6.91802    1.64665   4.201 3.69e-05 ***\nPutOuts        0.29737    0.07444   3.995 8.50e-05 ***\nDivisionW   -112.38006   39.21438  -2.866 0.004511 ** \nAtBat         -2.16865    0.53630  -4.044 7.00e-05 ***\nWalks          5.77322    1.58483   3.643 0.000327 ***\nCWalks        -0.83083    0.26359  -3.152 0.001818 ** \nCRuns          1.40825    0.39040   3.607 0.000373 ***\nCAtBat        -0.13008    0.05550  -2.344 0.019858 *  \nAssists        0.28317    0.15766   1.796 0.073673 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.8 on 252 degrees of freedom\nMultiple R-squared:  0.5405,    Adjusted R-squared:  0.5223 \nF-statistic: 29.64 on 10 and 252 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\nCode\nmodselect_seq_cp &lt;- step(mod0,\n                  scope = formula(mod1),\n                  trace = FALSE,\n                  direction = c(\"both\"),\n                  k = 1)\nmodselect_seq_cp %&gt;% summary()\n\n\n\nCall:\nlm(formula = Salary ~ CRBI + Hits + PutOuts + Division + AtBat + \n    Walks + CWalks + CRuns + CAtBat + Assists + League, data = Hitters_Without_NA)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-932.2 -175.4  -29.2  130.4 1897.2 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  135.75122   71.34623   1.903 0.058223 .  \nCRBI           0.78525    0.20978   3.743 0.000225 ***\nHits           6.92370    1.64612   4.206 3.62e-05 ***\nPutOuts        0.28941    0.07478   3.870 0.000139 ***\nDivisionW   -111.14603   39.21835  -2.834 0.004970 ** \nAtBat         -2.12775    0.53746  -3.959 9.81e-05 ***\nWalks          5.62028    1.59064   3.533 0.000488 ***\nCWalks        -0.82286    0.26361  -3.121 0.002010 ** \nCRuns          1.45533    0.39270   3.706 0.000259 ***\nCAtBat        -0.13899    0.05609  -2.478 0.013870 *  \nAssists        0.26883    0.15816   1.700 0.090430 .  \nLeagueN       43.11162   39.96612   1.079 0.281755    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 311.7 on 251 degrees of freedom\nMultiple R-squared:  0.5426,    Adjusted R-squared:  0.5226 \nF-statistic: 27.07 on 11 and 251 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nAvec la fonction step nous avons le modèle utilisant le critère BIC qui nous permet d’avoir toutes nos variables significatives (sauf l’intercept ce qui pourrait nous donner envie de faire des modèles sans).\nPar contre, nos valeurs de \\(R^2\\) et \\(R^2_{adjusted}\\) sont encore assez faible pour considérer ces modèles comme étant de très bonne qualité.\n\n\n\n\n\n\n\nNote\n\n\n\nAussi, si on veut comparer le modèle initial et le modèle final, on peut utiliser la fonction anova. Cela permet de voir si la sélection de variables a significativement amélioré l’ajustement.\n\n\nCode\nanova(mod0, modselect_seq_bic, test = \"Chisq\")\n\n\nAnalysis of Variance Table\n\nModel 1: Salary ~ 1\nModel 2: Salary ~ CRBI + Hits + PutOuts + Division + AtBat + Walks\n  Res.Df      RSS Df Sum of Sq  Pr(&gt;Chi)    \n1    262 53319113                           \n2    256 26194904  6  27124209 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDe manière logique on voit donc tout de même que le modèle avec nos variable explicatives sélectionnées (ici celui avec selection via le critère BIC) propose un meilleur ajustement que celui sans variable."
  },
  {
    "objectID": "posts/Exercice_03.html",
    "href": "posts/Exercice_03.html",
    "title": "Exercice 03",
    "section": "",
    "text": "Setup\n\nPackagesFonctionsSeed\n\n\n\n\nCode\n# Données\nlibrary(dplyr)        # manipulation des données\n\nlibrary(boot) ## CV\n\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nboxplotpairs.panelsLOO\n\n\n\n\nCode\nmy_boxplot &lt;- function(data) {\n  # Transformer les données en format long pour ggplot\n  data_long &lt;- reshape2::melt(data)\n  \n  ggplot(data_long, aes(x = variable, y = value, fill = variable)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +  # Palette de couleurs harmonieuse\n    labs(title = \"Distribution des Variables (Boxplot)\", x = \"Variables\", y = \"Valeurs\") +\n    theme_minimal() +  # Thème épuré\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotation des étiquettes\n}\n\n\n\n\n\n\nCode\nmy_pairs.panels &lt;- function(data) {\n  psych::pairs.panels(\n    data,\n    method = \"pearson\",\n    # Méthode de corrélation\n    hist.col = RColorBrewer::brewer.pal(9, \"Set3\"),\n    # Couleurs des histogrammes\n    density = TRUE,\n    # Ajout des courbes de densité\n    ellipses = TRUE,\n    # Ajout d'ellipses\n    smooth = TRUE,\n    # Ajout de régressions lissées\n    lm = TRUE,\n    # Ajout des droites de régression\n    col = \"#69b3a2\",\n    # Couleur des points\n    alpha = 0.5              # Transparence\n  )\n}\n\n\n\n\nPremière fonction LOO\n\n\nCode\nloo &lt;- function(mod) {\n  n &lt;- nrow(mod$model)\n  Call &lt;- mod$call\n  erreur &lt;- 1:n\n  for (i in 1:n) {\n    Call$data &lt;- mod$model[-i, ] # mod$call$data transforme en data.frame\n    fit &lt;- eval.parent(Call)\n    pred = predict(fit, mod$model[i, ])\n    erreur[i] &lt;- (pred - mod$model[i, 1])^2\n  }\n  return(round(mean(erreur), 3))\n}\n\n\nDeuxième fonction LOO\n\n\nCode\nloo2 &lt;- function(mod) {\n  round(mean((residuals(mod) / (1 - hatvalues(mod)))^2), 3)\n}\n\n\nFonction pour obtenir les résultats\n\n\nCode\nget_loo_results &lt;- function(model, func) {\n  start_time &lt;- Sys.time()        \n  result &lt;- func(model)          \n  end_time &lt;- Sys.time()         \n  time_taken &lt;- round(end_time - start_time, 3)  \n  \n  return(list(result = result, time = time_taken))\n}\n\n\n\n\n\n\n\n\n\nCode\nset.seed(140400)\n\n\n\n\n\n\n\nDonnées\nPour cette exercice, on va générer un modèle de régression linéaire classique :\n\\[y = X\\beta + \\mathcal{E}\\]\n\n\\(y \\in \\mathbb{R}^{n}\\) la variable réponse ou variable à expliquer\n\\(X \\in \\mathbb{R}^{n\\times p}\\) la matrice contenant nos variables explicatives\n\\(\\beta \\in \\mathbb{R}^{p}\\) le vecteur composée des coefficients de régression\n\\(\\mathcal{E} \\in \\mathbb{R}^{n}\\) le vecteur d’erreur suivant une loi \\(\\mathcal{N}(0, 1)\\)\n\nPour la génération de nos données, nous allons alors poser que \\(\\beta = (1, -2)'\\) et \\(X = [x, x^2]\\), \\(x \\in \\mathbb{R}^n\\) suivant une loi \\(\\mathcal{N}(0,1)\\).\nOn aura alors que \\(y = x - 2x^2 + \\mathcal{E}\\).\n\n\nCode\nx &lt;- rnorm(1000)\ny &lt;- x - 2*(x^2) + rnorm(1000)\nSimu_data &lt;- data.frame(y = y, x = x)\n\n\n\n\n\n\n\n\nNote\n\n\n\npour des raisons de repouctibilité, une graine ou seed a été défini dans le setup afin que la génération aléatoire reste identique.\n\n\nOn va ainsi supposer avoir observé les deux vecteurs \\(x\\) et \\(y\\) précédents, sans connaître le lien théorique précédent qui lie \\(x\\) et \\(y\\).\nEt donc on cherchera à estimer ce lien.\n\n\nAnalyse descriptive\n\nBoxplotCorrelation panel\n\n\nOn peut regarder un peu la distribution de nos différents variables quantitatives via des boxplots.\n\n\nCode\nmy_boxplot(Simu_data)\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn voit bien que notre variable \\(x\\) a une distribution normale centré réduite mais quelle n’est pas non plus parfaitement symétrique (forcément entre la “perfection” de la théorie et la génération par ordinateur il y a toujours une légère différence).\nEt concernant \\(y\\), de manière logique avec le modèle simulé, on peut voir d’avantages de valeurs négatives.\n\n\n\nOn regarde ici la corrélation calculée entre chacune de nos variables.\n\n\nCode\nmy_pairs.panels(Simu_data)\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nTout d’abord, on peut remarquer une corrélation faible de 34% entre \\(x\\) et \\(y\\). Pourtant le nuage de point semble quand à lui témoigner d’une influence de \\(x\\) sur \\(y\\) pouvant justifier d’un lien linéaire.\nAussi on retrouve un belle histogramme de distibution \\(\\mathcal{N}(0, 1)\\) pour notre variable \\(x\\).\n\n\n\n\n\n\nAnalyse inférentielle\nMaintenant, on va ajuster différents modèles à tester :\n\nmod1 : \\(y = \\beta_0 + \\beta_1x + \\mathcal{E}\\)\nmod2 : \\(y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\mathcal{E}\\)\nmod3 : \\(y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\mathcal{E}\\)\nmod4 : \\(y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\beta_4x^4 + \\mathcal{E}\\)\n\nOn va donc commencer par compléter notre data frame avec des variables correspondant à \\(x^2\\), \\(x^3\\) et \\(x^4\\). Puis nous pourrons ajuster les différents modèles.\n\n\nCode\nSimu_data_complete &lt;- cbind(Simu_data, x^2, x^3, x^4)\ncolnames(Simu_data_complete) &lt;- c(\"y\", \"x1\", \"x2\", \"x3\", \"x4\")\n\n\n\n\nCode\nmod1 &lt;- lm(y ~ x1, Simu_data_complete)\nmod2 &lt;- lm(y ~ x1 + x2, Simu_data_complete)\nmod3 &lt;- lm(y ~ x1 + x2 + x3, Simu_data_complete)\nmod4 &lt;- lm(y ~ x1 + x2 + x3 + x4, Simu_data_complete)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThéoriquement, on est dans une situation où le \\(R^2\\) pour le modèle 1 est égale à la corrélation de pearson au carré de \\(x\\) et \\(y\\).\n\n\\(R^2 =\\) 0.116\n\\(\\rho^2 =\\) 0.116\n\n\n\nEnsuite, partir du summary() et de différentes fonctions R, on est capable capable d’obtenir différents critères permettant de comparer la qualité de nos modèles.\n\n\nCode\nmodels &lt;- list(mod1, mod2, mod3, mod4)\nmodel_names &lt;- c(\"mod1\", \"mod2\", \"mod3\", \"mod4\")\n\nresults &lt;- data.frame(\n  Model = model_names,\n  R2 = unlist(lapply(models, function(m) round(summary(m)$r.squared, 3))),\n  R2adj = unlist(lapply(models, function(m) round(summary(m)$adj.r.squared, 3))),\n  AIC = unlist(lapply(models, function(m) round(AIC(m), 1))),\n  BIC = unlist(lapply(models, function(m) round(BIC(m), 1)))\n)\n\n\nresults %&gt;% DT::datatable()\n\n\n\n\n\n\n\nOn peut voir ici que c’est mod2 qui ressort comme étant le meilleur modèle avec de forte valeur de \\(R^2\\) et \\(R^2_{adjusted}\\) puis des critères AIC et BIC minimisés.\n\n\nCode\nmod2 %&gt;% summary()\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = Simu_data_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1117 -0.7078 -0.0106  0.6605  3.3936 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03778    0.03820   0.989    0.323    \nx1           1.02939    0.03105  33.151   &lt;2e-16 ***\nx2          -2.01737    0.02107 -95.725   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.995 on 997 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9131 \nF-statistic:  5248 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nOn constate en plus avec le summary() que toutes nos variables sont significatives sauf l’intercept.\n\n\nValidation croisée\n\nRappel validation croiséeValidation “manuelle”Library boot\n\n\nLe principe de la validation croisée (cross validation) est d’estimer le risque de prédiction en confrontant notre modèle à un échantillon test qui n’a pas été utilisé pour l’ajustement de celui ci.\nLa validation croisée possède ainsi de nombreux avantages mais a comme principal inconvénient son temps de calcul qui peut rapidement devenir important.\n\nDans le cas du K-fold, on coupe l’échantillon de taille \\(n\\) en environ K parties égales. Ensuite, on fait l’ajustement du modèle sur K-1 échantillons et on garde le K-ième comme échantillon test pour calculer l’erreur de prédiction. On répète alors le procédé de telle sorte à ce que chaque échantillon serve une fois de test. Cela nous fait donc calculer K erreurs.\nA savoir que selon la valeur de K, on peut se retrouver dans des cas particuliers très utilisés.\n\nLorsque K = n, il s’agit de la procédure Leave One Out (LOO).\nLorsque K = 2, on est sur une procédure Hold out ou testset\n\nLa validation croisée par K-fold est donc un outil couramment utilisé. Le choix de K est quant à lui très important et il faut penser que si K est trop grand, le biais sera faible mais à contrario, la variance deviendra très grande. Par contre, si K est trop petit, l’estimation risque de posséder un grand biais puisque notre taille d’échantillon test sera beaucoup plus grande que celle de l’échantillon d’apprentissage. On a donc ici un bel exemple de compromis entre biais et variance pour trouver le K le plus judicieux.\n\n\nDans un premier temps et pour bien comprendre la méthode, on va utiliser deux fonctions (construite pour l’occasion et dont le code se trouve dans la partie fonction du Setup) permettant d’estimer l’erreur test par une validation croisée LOO (Leave-one-out) pour un modèle ajusté par la fonction lm :\n\nla première, loo, en utilisant le principe général de cette méthode qui nécessite donc l’estimation de n modèles différents\nla seconde, loo2, en utilisant la formule adaptée à la régression linéaire donnant directement le risque LOO à partir de la seule estimation du modèle complet (on pourra utiliser la fonction hatvalues)\n\nAinsi, en testant sur nos quatre modèle, on obtient les résultats suivants :\n\n\nCode\nloo_results &lt;- lapply(models, function(m) get_loo_results(m, loo))\nloo2_results &lt;- lapply(models, function(m) get_loo_results(m, loo2))\n\nresults &lt;- data.frame(\n  Model = model_names,\n  LOO = unlist(lapply(loo_results, function(x) x$result)),       \n  Time_LOO = unlist(lapply(loo_results, function(x) x$time)),    \n  LOO2 = unlist(lapply(loo2_results, function(x) x$result)),     \n  Time_LOO2 = unlist(lapply(loo2_results, function(x) x$time))   \n)\n\nresults %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn voit que les résultats donnés par nos deux fonctions coincident bien. Mais la première semble tout de même plus lente pour le calcul.\nEt en terme de qualité de modèle, c’est bien mod2 qui minimise l’erreur de la cross validation par loo.\n\n\n\nOn va donc maintenant utiliser la fonction cv.glm de la library { boot } permettant d’estimer l’erreur test par validation croisée K-fold. Cela va nécessité de recalculer le modèle mais cette fois ci avec la fonction glm en spécifiant que l’on veut un modèle gaussien (ce qui nous donnera le même résultat qu’avec lm).\n\n\n\n\n\n\nNote\n\n\n\nOn spécifi ici dans glm que le modèle est gaussien mais dans la pratique ce n’est pas nécéssaire pusiqu’il s’agit de la valeur par défaut de la fonction.\n\n\nDe plus, nous utiliserons \\(K=10\\) qui est une valeurs assez communément utilisé sachant que si on voulait reproduire la procédure LOO il faudrait utiliser \\(K=n\\) (cf rappel).\n\n\nCode\nmod1_glm &lt;- glm(formula = formula(mod1) ,\n                family = gaussian,\n                data = Simu_data_complete)\nmod2_glm &lt;- glm(formula = formula(mod2) ,\n                family = gaussian,\n                data = Simu_data_complete)\nmod3_glm &lt;- glm(formula = formula(mod3) ,\n                family = gaussian,\n                data = Simu_data_complete)\nmod4_glm &lt;- glm(formula = formula(mod4) ,\n                family = gaussian,\n                data = Simu_data_complete)\n\n\ncvmod1 &lt;- cv.glm(data = Simu_data_complete, glmfit = mod1_glm, K = 10)\ncvmod2 &lt;- cv.glm(data = Simu_data_complete, glmfit = mod2_glm, K = 10)\ncvmod3 &lt;- cv.glm(data = Simu_data_complete, glmfit = mod3_glm, K = 10)\ncvmod4 &lt;- cv.glm(data = Simu_data_complete, glmfit = mod4_glm, K = 10) \n\nresults &lt;- data.frame(\n  Model = model_names,\n  CV_Mean = round( c(mean(cvmod1$delta), mean(cvmod2$delta), mean(cvmod3$delta), mean(cvmod4$delta)), 3),\n  LOO2 = c(loo2(mod1), loo2(mod2), loo2(mod3), loo2(mod4))\n)\n\n\nresults %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn voit qu’en moyenne cv.glm nous donne des résultats qui sont du même ordre de grandeur que notre fonction loo2.\nEt en terme de qualité de modèle, c’est bien mod2 qui minimise l’erreur de la cross validation.\n\n\n\n\n\n\nAjustement du meilleur modèle\nD’après tout ce que l’on a pu voir durant cette étude, jusqu’à présent le meilleur modèle semble être mod2.\n\n\nCode\nmod2 %&gt;% summary()\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = Simu_data_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1117 -0.7078 -0.0106  0.6605  3.3936 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.03778    0.03820   0.989    0.323    \nx1           1.02939    0.03105  33.151   &lt;2e-16 ***\nx2          -2.01737    0.02107 -95.725   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.995 on 997 degrees of freedom\nMultiple R-squared:  0.9133,    Adjusted R-squared:  0.9131 \nF-statistic:  5248 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n\nDe manière naturelle, l’intercept ne semblant pas significatif il conviendrait de tester sans.\nAinsi nous allons essayer le modèle \\(y \\sim x + x^2\\).\n\n\nCode\nmod2_without_intercept &lt;- lm(y ~ 0 + x1 + x2, Simu_data_complete)\nmod2_without_intercept %&gt;% summary()\n\n\n\nCall:\nlm(formula = y ~ 0 + x1 + x2, data = Simu_data_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0785 -0.6834  0.0212  0.6855  3.4288 \n\nCoefficients:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nx1  1.02988    0.03105   33.17   &lt;2e-16 ***\nx2 -2.00555    0.01736 -115.54   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.995 on 998 degrees of freedom\nMultiple R-squared:  0.9364,    Adjusted R-squared:  0.9363 \nF-statistic:  7352 on 2 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nOn voit un modèle avec de très bon résultats et qui à toutes ces variables significatives. Comparons le avec mod2 :\n\n\nCode\nmodels &lt;- list(mod2, mod2_without_intercept)\nmodel_names &lt;- c(\"mod2\", \"mod2_without_intercept\")\n\nresults &lt;- data.frame(\n  Model = model_names,\n  R2 = unlist(lapply(models, function(m)\n    round(summary(m)$r.squared, 3))),\n  R2adj = unlist(lapply(models, function(m)\n    round(summary(m)$adj.r.squared, 3))),\n  AIC = unlist(lapply(models, function(m)\n    round(AIC(\n      m\n    ), 1))),\n  BIC = unlist(lapply(models, function(m)\n    round(BIC(\n      m\n    ), 1))),\n  LOO2 = c(loo2(mod2), loo2(mod2_without_intercept)),\n  CV_Mean = round(c(\n    mean(cv.glm(\n      data = Simu_data_complete,\n      glmfit = glm(\n        formula = formula(mod2),\n        data = Simu_data_complete\n      ),\n      K = 10\n    )$delta), mean(cv.glm(\n      data = Simu_data_complete,\n      glmfit = glm(\n        formula = formula(mod2_without_intercept),\n        data = Simu_data_complete\n      ),\n      K = 10\n    )$delta)\n  ), 3)\n)\n\n\nresults %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn voit bien avec tout nos critère qu’enlever l’intercept à apporté une amélioration à notre modèle.\nAinsi, si l’on se base sur les résultat de ce nouveau modèle, on obtient la relation linéaire suivante :\n\n\\(y =\\) 1.03\\(x\\) + -2.006\\(x^2 + \\mathcal{E}\\)\n\nAlors que pour rappel, on a le lien linéaire théorique qui est:\n\n\\(y = x - 2x^2 + \\mathcal{E}\\).\n\nDonc je pense que l’on peut dire sans prendre trop de risques que notre estimation et notre méthode de sélection est bonne.\n\n\n\nConclusion\nOn voit qu’on a bien réussi à retrouver le lien théorique via le test de pluseurs modèle et l’utilisation de plusieurs critères couplés à de la validation croisée pour affiner notre recherche du modèle le mieux ajusté.\nAinsi, il est important d’avancer étape par étape car ici chaques étapes était importante pour trouver le meilleur modèle. Et ici on se basait sur un modèle généré par nous même et possédant un lien linéaire bien défini ce qui nous permettait tout de même si bien orienter nos recherche. La réalité nous offre souvent des situations plus compliquées et tout ces outils deviennent donc cruciaux pour bien avancer.\n\n\nSession info\n\n\nCode\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-02-21\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n boot      * 1.3-31  2024-08-28 [4] CRAN (R 4.3.3)\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_05.html",
    "href": "posts/Exercice_05.html",
    "title": "Exercice 05",
    "section": "",
    "text": "Clément Poupelin, clementjc.poupelin@gmail.com"
  },
  {
    "objectID": "posts/Exercice_05.html#corrélation",
    "href": "posts/Exercice_05.html#corrélation",
    "title": "Exercice 05",
    "section": "Corrélation",
    "text": "Corrélation\nMaintenant, nous pouvons déjà souligner que, théoriquement, il ne devrait pas y avoir de lien entre \\(y\\) et \\(X\\) puisque les simulations sont faites indépendament.\nPour visualiser cela, il suffit simplement de créer un vecteur qui stockera les différentes valeurs de corrélation entre \\(y\\) et \\(x^i\\) pour \\(i\\) allant de 1 à 5000.\n\n\nCode\ncor_vect &lt;- unlist(lapply(x, function(col) cor(col, y)))\nmy_hist(cor_vect)\n\n\n\n\n\n\nTextePreuve\n\n\nDe manière général, il faut savoir qu’une méthode de prévision de \\(y\\) basée sur les variables explicatives s’exprime nécessairement sous la forme \\(\\hat{y} = f(x^1, ..., x^{5000})\\). La forme de la fonction \\(f\\) étant généralement obtenue grâce à une estimation sur un échantillon d’apprentissage. Il faut alors découper notre jeu de données en deux échantillons, un pour l’apprentissage et un pour le test.\n\nDans ces méthodes, nous nous intéressons principalement au taux d’erreur de classification “test”, c’est-à-dire à la probabilité que \\(\\hat{y}\\) soit différent de \\(y\\), lorsque \\(y\\) et \\(x^1, ..., x^{5000}\\) sont dans l’échantillon test et donc indépendants de l’échantillon d’apprentissage.\nOn peut facilement démontrer que cette probabilité est de 50% quelle que soit la méthode utilisée (cf Preuve).\nPourtant, pour illustrer nos problèmes, supposons que nous avons observé le jeu de données simulé ci-dessus, sans connaître les liens théoriques entre les variables. Nous souhaitons alors ajuster un modèle expliquant au mieux \\(y\\) en fonction des variables à disposition, et estimer le taux d’erreur des prévisions associées.\n\n\nNous remarquons déjà que, \\(\\hat{y}\\) étant une combinaisons de nos variables \\(x^1, ..., x^{5000}\\) qui sont indépendantes de \\(y\\), on a \\(\\hat{y}\\) et y sont deux variables indépendantes l’une de l’autre.\n\nAinsi \\[\\mathbb{P}(\\hat{y} \\neq y) = \\mathbb{P}(\\hat{y}=0, y=1) + \\mathbb{P}(\\hat{y}=1, y=0)\\] \\[ \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad = \\mathbb{P}(\\hat{y}=0)\\mathbb{P}(y=1) + \\mathbb{P}(\\hat{y}=1)\\mathbb{P}(y=0) \\quad  ^{(*)}\\] \\[ \\quad \\quad \\quad \\quad = 0.5\\mathbb{P}(\\hat{y}=0) + 0.5\\mathbb{P}(\\hat{y}=1)  \\quad  ^{(**)} \\] \\[ \\quad \\quad = 0.5 \\left( \\mathbb{P}(\\hat{y}=0) + \\mathbb{P}(\\hat{y}=1) \\right)\\]\nNous reconnaissons ici une somme sur l’univers des possible de la densité discrète de \\(\\hat{y}\\).\nCelle ci est donc égale à 1 et on obtient\n\\[\\mathbb{P}(\\hat{y} \\neq y) = 0.5\\]\n\n\\(^{(*)} \\quad \\text{par indépendance de} \\quad \\hat{y} \\quad \\text{et} \\quad y\\)\n\\(^{(**)} \\quad \\text{car} \\quad y \\quad \\text{suit une loi de Bernoulli de paramètre} \\quad 0.5\\)"
  },
  {
    "objectID": "posts/Exercice_06Bonus.html",
    "href": "posts/Exercice_06Bonus.html",
    "title": "Exercice 06 Bonus : Ridge vs Lasso",
    "section": "",
    "text": "Dans cette partie et avant de passer à l’exercice 7, nous allons faire la section 3.1 sur la regression Ridge et Lasso avec glmnet sous R du tutoriel de Laurent Rouvière\n\nRappels sur Ridge et Lasso\nOn se base encore ici sur notre modèle classique de régression linéaire :\n\\[y = X\\beta + \\mathcal{E} \\quad \\text{ou} \\quad y = \\beta_0 + \\beta_1X^1 + ... + \\beta_pX^p + \\mathcal{E}\\]\n\n\\(y \\in \\mathbb{R}^{n}\\) la variable réponse ou variable à expliquer\n\\(X \\in \\mathbb{R}^{n\\times (p+1)}\\) la matrice déterministe contenant nos \\(p\\) variables explicatives\n\\(\\beta \\in \\mathbb{R}^{p+1}\\) le vecteur qui contient les coefficients de régression \\(\\beta_0, ..., \\beta_p\\) que nous cherchons à estimer\n\\(\\mathcal{E} \\in \\mathbb{R}^{n}\\) le vecteur d’erreur qui n’est pas corrélé à nos variables explicatives. C’est la part d’aléa que nous n’arrivons pas à déterminer\n\nPour l’estimation des coefficients de regression de ce type de modèle, nous utilisons souvent la méthodes des moindres carrées ordinaire (MCO) qui nous donne\n\\[\\hat{\\beta} = \\underset{\\beta}{argmin}||y-X\\beta||^2\\] Malheureusement, lorsque \\(p\\) est grand ou que les variables sont linéairement dépendantes, les estimateurs des moindres carrées peuvent être mis en défaut. Les méthodes pénalisées ou sous contraintes consistent alors à restreindre l’espace sur lequel on minimise ce critère.\n\nL’idée principale de ces méthodes est de contraindre la valeur des estimateurs MCO pour réduire la variance, quitte à augmenter un peu le biais (d’où la terminologie de “régression biaisée”). Nous obtenons donc, pour un certain \\(t&gt;0\\), des estimations de la forme suivante : \\[ \\hat{\\beta}^{pen} = \\underset{\\beta}{argmin}||y-X\\beta||^2 \\quad \\text{sous la contrainte} \\quad ||\\beta||? \\leq t\\]\n\nRidgeLasso\n\n\nLa régression Ridge contraint la norme \\(\\ell^2\\) des coefficients \\(\\beta\\) à ne pas exploser, i.e \\(||\\beta||_2 = \\sum_{j=0}^{p} \\beta_j^2 \\leq t\\). Cela conduit à la solution d’optimisation suivante :\n\\[\n\\hat{\\beta}_{Ridge} = \\underset{\\beta}{\\operatorname{argmin}} \\ ||y - X\\beta||^2 \\quad \\text{sous la contrainte} \\quad \\sum_{j=0}^{p} \\beta_j^2 \\leq t\n\\]\noù \\(\\hat{\\beta}_{Ridge}\\) est unique, contrairement à Lasso qui peut produire plusieurs solutions.\nContrairement à Lasso, Ridge ne met pas exactement à zéro certains coefficients, mais réduit leur valeur. Il n’y a donc pas de sélection de variable effectué.\nCette méthode est tout de même robuste en grande dimension et particulièrement utile lorsque les variables sont fortement corrélées. Il empêche les coefficients de devenir trop grands, ce qui réduit la variance du modèle.\n\n\n\nLa régression Lasso (pour Least Absolute Shrinkage and Selection Operator) contraint la norme \\(\\ell^1\\) de \\(\\beta\\) à ne pas exploser, i.e \\(||\\beta||_1 = \\sum_{j=0}^{p} |\\beta_j| \\leq t\\). Nous obtenons donc\n\\[\\hat{\\beta}_{Lasso} = \\underset{\\beta}{argmin}||y-X\\beta||^2 \\quad \\text{sous la contrainte} \\quad \\sum_{j=0}^{p} |\\beta_j| \\leq t  \\]\nOù \\(\\hat{\\beta}_{Lasso}\\) n’est pas nécessairement unique mais la prévision \\(\\hat{y} = X\\hat{\\beta}_{Lasso}\\) est unique.\n\nCette méthode est principalement caractérisée par le fait qu’elle est robuste à la grande dimension en sélectionnant les variables les plus pertinentes. En effet, elle nous permet de réduire les coefficients MCO des variables sélectionnées en rapprochant leur valeur de 0, ce qui est appelé la propriété de “seuillage doux” du Lasso.\n\n\n\nIci, il sera donc présenté les étapes principales qui permettent de faire ce type de régression avec R. Le package le plus souvent utilisé est glmnet.\n\n\nSetup\n\nPackagesFonctions\n\n\n\n\nCode\n# Inférence\nlibrary(glmnet) ## regression pénalisée\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nboxplotpairs.panelsComparaison de modèles\n\n\n\n\nCode\nmy_boxplot &lt;- function(data) {\n  data_long &lt;- reshape2::melt(data)\n  \n  ggplot(data_long, aes(x = variable, y = value, fill = variable)) +\n    geom_boxplot() +\n    scale_fill_viridis_d() +  # Palette de couleurs harmonieuse\n    labs(title = \"Distribution des Variables (Boxplot)\", x = \"Variables\", y = \"Valeurs\") +\n    theme_minimal() +  # Thème épuré\n    theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotation des étiquettes\n}\n\n\n\n\n\n\nCode\nmy_pairs.panels &lt;- function(data) {\n  psych::pairs.panels(\n    data,\n    method = \"pearson\",\n    # Méthode de corrélation\n    hist.col = RColorBrewer::brewer.pal(9, \"Set3\"),\n    # Couleurs des histogrammes\n    density = TRUE,\n    # Ajout des courbes de densité\n    ellipses = TRUE,\n    # Ajout d'ellipses\n    smooth = TRUE,\n    # Ajout de régressions lissées\n    lm = TRUE,\n    # Ajout des droites de régression\n    col = \"#69b3a2\",\n    # Couleur des points\n    alpha = 0.5              # Transparence\n  )\n}\n\n\n\n\n\n\nCode\ncv_ridge_vs_lasso &lt;- function(data, fmla) {\n  set.seed(140400)\n  \n  data.X &lt;- model.matrix(fmla, data = data)[, -1]\n  data.Y &lt;- data$maxO3\n  \n  blocs &lt;- caret::createFolds(1:nrow(data), k = 10)\n  \n  prev &lt;- matrix(0, ncol = 3, nrow = nrow(data)) %&gt;%  as.data.frame()\n  names(prev) &lt;- c(\"Linéaire (MCO)\", \"Ridge\", \"Lasso\")\n  for (k in 1:10) {\n    app &lt;- data[-blocs[[k]], ]\n    test &lt;- data[blocs[[k]], ]\n    \n    app.X &lt;- data.X[-blocs[[k]], ]\n    app.Y &lt;- data.Y[-blocs[[k]]]\n    \n    test.X &lt;- data.X[blocs[[k]], ]\n    test.Y &lt;- data.Y[blocs[[k]]]\n    \n    ridge &lt;- cv.glmnet(app.X, app.Y, alpha = 0)\n    lasso &lt;- cv.glmnet(app.X, app.Y, alpha = 1)\n    lin &lt;- lm(fmla, data = app)\n    \n    prev[blocs[[k]], ] &lt;- tibble::tibble(\n      lin = predict(lin, newdata = test),\n      ridge = as.vector(predict(ridge, newx =\n                                  test.X)),\n      lasso = as.vector(predict(lasso, newx =\n                                  test.X))\n    )\n  }\n  \n  err &lt;- prev %&gt;%  mutate(obs = data$maxO3) %&gt;%  summarise_at(1:3, ~ mean((obs -\n                                                                             .)^2))\n  return(err)\n}\n\n\n\n\n\n\n\n\n\n\nDonnées\nOn considère le jeu de données ozone.txt où on cherche à expliquer la concentration maximale en ozone relevée sur une journée (variable maxO3) par d’autres variables essentiellement météorologiques.\nLa base de données d’origine ozone.txt répertorie ainsi 112 données météorologiques mesurées durant l’été 2001 à Rennes. Celles-ci sont caractérisées par les 13 variables suivantes :\n\n\n\n\n\nmaxO3\nconcentration maximale d'ozone (en DU)\n\n\nT9\ntempérature à 9H (en °C)\n\n\nT12\ntempérature à 12H (en °C)\n\n\nT15\ntempérature à 15H (en °C)\n\n\nNe9\nnébulosité à 9H (en octa)\n\n\nNe12\nnébulosité à 12H (en octa)\n\n\nNe15\nnébulosité à 15H (en octa)\n\n\nVx9\nvitesse du vent à 9H\n\n\nVx12\nvitesse du vent à 12H\n\n\nVx15\nvitesse du vent à 15H\n\n\nmaxO3v\nconcentration maximale d'ozone de la veille (en DU)\n\n\nvent\ndirection principale du vent (Nord / Ouest / Sud / Est)\n\n\npluie\nprésence ou non de pluie (Sec / Pluie)\n\n\n\n\n\n\n\nOn identifie le regroupement de toutes les données météorologiques récoltées en une journée par la date à laquelle les relevés ont été effectués.\n\n\nCode\nozone &lt;-read.table(\"https://r-stat-sc-donnees.github.io/ozone.txt\", header=TRUE)\nozone %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBien évidémment on a vérifié qu’il n’y a pas de valeurs manquantes dans cette base de données avec la fonction anyNA qui renvoie FALSE\n\n\n\n\nAnalyse descriptive\n\nBoxplotCorrelationPCA\n\n\nOn peut regarder un peu la distribution de nos différents variables quantitatives via des boxplots.\n\n\nCode\nmy_boxplot(ozone)\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn remarque bien que les variabeles qui sont de même nature mais à des points de temps différents sont d’avantages similaires.\n\nPour confirmer cela, on peut faire des boxplot pour uniquement une varibale et ses différents points de temps.\n\nTempératureNébulositéVitesse du vent\n\n\n\n\nCode\nsubset(ozone, select = c(\"T9\", \"T12\", \"T15\"))  %&gt;% my_boxplot()\n\n\n\n\n\n\n\n\n\nCode\nsubset(ozone, select = c(\"Ne9\", \"Ne12\", \"Ne15\"))  %&gt;% my_boxplot()\n\n\n\n\n\n\n\n\n\nCode\nsubset(ozone, select = c(\"Vx9\", \"Vx12\", \"Vx15\"))  %&gt;% my_boxplot()\n\n\n\n\n\n\n\n\n\n\nOn regarde ici la corrélation calculée entre chacune de nos variables.\n\n\nCode\nmy_pairs.panels(ozone)\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn voit la présence de plusieurs fortes corrélations qui peut déjà nous alerter si l’on veut faire des modèles de regressions linéaires car on risque d’avoir un problème de colinéarité entre les varibales explicatives.\nCependant ces corrélation fortes sont surtout présentes pour les variables qui sont à différents points de temps ce qui est logique.\n\n\n\nAvec une Analyse en Composantes Principales (PCA) on peut regarder un peu le comportement de nos données.\nEn effet, Cette méthode respose sur la transformation des variables d’origine en nouvelles variables non corrélées, appelées composantes principales, qui capturent successivement la plus grande variance possible des données.\n\n\nCode\nres_pca &lt;- FactoMineR::PCA(\n  ozone,\n  quali.sup = c(which(colnames(ozone) %in% c(\"vent\", \"pluie\"))),\n  quanti.sup = c(which(colnames(ozone) %in% c(\"max03\", \"max03v\"))),\n  graph = FALSE\n)\n\n\nIci, on spécifi nos varibales qualitatives et on décide de mettre la variable max03 et max03v en variable supplémentaire, ce qui veut d’ire qu’elles ne seront pas considérés pour la formation de nos composantes principales (variable que l’on cherchera à estimer plus tard).\n\nBarplot des variancesIndividusVariables\n\n\nTout d’abord, on peut commencer par regarder le pourcentage de variance expliqué par nos différentes composantes principales.\n\n\nCode\nfactoextra::fviz_eig(\n  res_pca,\n  ncp = 10,\n  addlabels = TRUE,\n  barfill = \"coral\",\n  barcolor = \"coral\",\n  ylim = c(0, 60),\n  main = \"Percentage of variance of the 10 first components\"\n)\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn voit ainsi que la majorité de la variance est expliquée par nos deux premières composantes principales.\n\n\n\nLe plan des individus est une projection des observations sur les axes principaux de la PCA. Cette visualisation permet d’identifier des regroupements, tendances et anomalies au sein des données.\nAinsi, des individus proches sur le graphique ont des caractéristiques similaires par rapport aux variables utilisées.\nPuis, le placement d’un individu en fonction des axes peut permettre de savoir comment le jouer se caractérise par rapport aux variables qui contribuent le plus à ces axes.\n\nPluievent\n\n\n\n\nCode\nfactoextra::fviz_pca_ind(\n  res_pca,\n  label = \"none\",\n  pointsize = 2,\n  habillage = as.factor(ozone$pluie),\n  addEllipses = TRUE,\n  ellipse.level = 0.95\n)\n\n\n\n\n\n\n\n\n\nCode\nfactoextra::fviz_pca_ind(\n  res_pca,\n  label = \"none\",\n  pointsize = 2,\n  habillage = as.factor(ozone$vent),\n  addEllipses = TRUE,\n  ellipse.level = 0.95\n)\n\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nIci on voit une repartition plutot uniforme sur le plan qui ne semble pas permettre de distinguer une séparation forte correspodant à nos variables qualitatives.\n\n\n\nLe cercle des variables est une représentation graphique qui permet d’analyser les relations entre les variables initiales et les composantes principales qui forment nos axes. Il est basé sur les corrélations entre les variables et les axes principaux.\nAinsi, plus une variable est proche du bord du cercle, plus elle est bien représentée sur le plan factoriel et contribue fortement à la formation des axes. Ici, on utilise le cos2 pour le gradient de couleur qui va aider à l’indentifictation de ces différentes qualitées de représentation.\nDe plus, selon l’angle entre deux varibles, on peut faire des suppositions sur leur corrélation :\n\nSi deux variables ont des vecteurs proches (petit angle), elles sont fortement corrélées positivement\nSi deux variables ont des vecteurs opposés (angle proche de 180°), elles sont corrélées négativement\nSi l’angle est proche de 90°, alors les variables ne sont pas corrélées\n\n\n\nCode\nfactoextra::fviz_pca_var(\n  res_pca,\n  col.var = \"cos2\",\n  gradient.cols = rainbow(n = 8, start = .6, end = .9),\n  repel = TRUE\n)\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nDans notre cas, ce que l’on peut voir c’est que la majorité de nos variables sont bien représentées par nos deux axes (cumulant plus de 70% d’explication). Mais beaucoup semblent aussi fortement corrélées avecla formation de trois groupes. Cette corrélation ayant déjà pu être observé précédemment et touours logique du fait du côté longitudinale de nos données.\nCe que l’on peut tout de même ajouté c’est que les variables max03 et surtout max03v semblent plutot corrélées aux variables température. Constat qui peut se confirmer avec le pairs.panels précédent.\n\n\n\n\n\n\n\n\n\nAnalyse inférentielle\nContrairement à la plupart des autres package R qui permettent de faire de l’apprentissage, le package glmnet n’autorise pas l’utilisation de formules.  Il faut donc spécifier explicitement la matrice \\(X\\) et le vecteur \\(y\\).\n\nOn peut obtenir la matrice \\(X\\) et notamment le codage des variables qualitatives avec la fonction model.matrix.\n\n\nCode\nozone.X &lt;- model.matrix(maxO3 ~ ., data = ozone)[, -1]\nozone.Y &lt;- ozone$maxO3\n\n\nEt ce n’est qu’après que l’on peut mettre en place la modélisation.\n\nRidgeLasso\n\n\n\n\nCode\nmod.R &lt;- glmnet(ozone.X, ozone.Y, alpha = 0) \n\n\nLa fonction glmnet calcule tous les estimateurs pour une grille de valeurs de \\(\\lambda\\) que l’on peut récupérer.\n\n\nCode\nmod.R$lambda %&gt;% head()\n\n\n[1] 22007.27 20052.20 18270.82 16647.69 15168.76 13821.21\n\n\nEt on peut également récupérer les différentes valeurs de \\(\\beta\\) associée aux différents \\(\\lambda\\).\n\n\nCode\nbeta_values &lt;- mod.R$beta %&gt;% as.matrix() %&gt;% t()\nrownames(beta_values) &lt;- paste0(\"lambda_\", seq(1, length(mod.R$lambda), by = 1))\n\nbeta_values %&gt;% DT::datatable()\n\n\n\n\n\n\n\nPuis on peut visualiser les chemins de régularisation des estimateurs Ridge.\n\n\nCode\nplot(mod.R, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nIci on voit l’évolution de nos coefficients \\(\\beta\\) en fonction des diffrentes valeurs de \\(\\lambda\\). Ainsi, sur la gauche on se retrouve dans la situation où il n’y a pas de pénalisation et donc nos coefficients sont les \\(\\beta\\) de l’estimation par moindres carrés. Et donc plus \\(\\lambda\\) va augmenter, plus on se retrouvera dans une situation où les coefficients vont tendrent vers 0.\n\n\n\n\n\nCode\nmod.L &lt;- glmnet(ozone.X, ozone.Y, alpha = 1) \n\n\nLa fonction glmnet calcule tous les estimateurs pour une grille de valeurs de \\(\\lambda\\) que l’on peut récupérer.\n\n\nCode\nmod.L$lambda %&gt;% head()\n\n\n[1] 22.00727 20.05220 18.27082 16.64769 15.16876 13.82121\n\n\nEt on peut également récupérer les différentes valeurs de \\(\\beta\\) associée aux différents \\(\\lambda\\).\n\n\nCode\nbeta_values &lt;- mod.L$beta %&gt;% as.matrix() %&gt;% t()\nrownames(beta_values) &lt;- paste0(\"lambda_\", seq(1, length(mod.L$lambda), by = 1))\n\nbeta_values %&gt;% DT::datatable()\n\n\n\n\n\n\n\nPuis on peut visualiser les chemins de régularisation des estimateurs Lasso.\n\n\nCode\nplot(mod.L, xvar = \"lambda\", label = TRUE)\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nIci on voit l’évolution de nos coefficients \\(\\beta\\) en fonction des diffrentes valeurs de \\(\\lambda\\). Ainsi, sur la gauche on se retrouve dans la situation où il n’y a pas de pénalisation et donc nos coefficients sont les \\(\\beta\\) de l’estimation par moindres carrés. Et donc plus \\(\\lambda\\) va augmenter, plus on se retrouvera dans une situation où les coefficients vont tendrent vers 0.\n\n\n\n\n\n\nSelection des paramètres de régularisation\nMaintenant que les modèles sont estimés avec plusieurs valeurs de \\(\\lambda\\) possibles, il se pose la question du choix du bon paramètre.\nPour cela, on utilise la fonction cv.glmnet qui, comme son nom le laisse suggérer, permet d’effectuer une validation croisée pour notre modèle avec par défaut nfolds=10 (le nombre de pli pour le découpage de sous ensembles). Puis on peut faire un plot de l’objet.\n\nRidgeLasso\n\n\n\n\nCode\nridgeCV &lt;- cv.glmnet(ozone.X, ozone.Y, alpha = 0)\nplot(ridgeCV)\n\n\n\n\n\nOn visualise ici les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de \\(\\lambda\\) (échelle logarithmique). Deux traits verticaux sont représentés :\n\ncelui de gauche correspond à la valeur de \\(\\lambda\\) qui minimise l’erreur quadratique\ncelui de droite correspond à la plus grande valeur de \\(\\lambda\\) telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur\n\nD’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux.\nA savoir que si l’on veut diminuer la complexité du modèle on choisira la valeur de droite.\nOn peut obtenir ces deux valeurs assez facilement.\n\n\nCode\ncat(\" Valeur minimale : \", ridgeCV$lambda.min, \"\\n\", \"Valeur maximale : \", ridgeCV$lambda.1se)\n\n\n Valeur minimale :  9.750588 \n Valeur maximale :  39.36329\n\n\n\n\n\n\nCode\nlassoCV &lt;- cv.glmnet(ozone.X, ozone.Y, alpha = 1)\nplot(lassoCV)\n\n\n\n\n\nOn visualise ici les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de \\(\\lambda\\) (échelle logarithmique). Deux traits verticaux sont représentés :\n\ncelui de gauche correspond à la valeur de \\(\\lambda\\) qui minimise l’erreur quadratique\ncelui de droite correspond à la plus grande valeur de \\(\\lambda\\) telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur\n\nD’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux.\nA savoir que si l’on veut diminuer la complexité du modèle on choisira la valeur de droite.\nOn peut obtenir ces deux valeurs assez facilement.\n\n\nCode\ncat(\" Valeur minimale : \", lassoCV$lambda.min, \"\\n\", \"Valeur maximale : \", lassoCV$lambda.1se)\n\n\n Valeur minimale :  1.230385 \n Valeur maximale :  4.525822\n\n\n\n\n\n\n\nPrédiction\nOn souhaite maintenant prédire la variable cible pour de nouveaux individus, par exemple les 50ème et 51ème individus du jeu de données.\nUne première approche pourrait consister à réajuster le modèle sur toutes les données pour la valeur de \\(lambda\\) sélectionnée.\nCette étape est en réalité déjà effectuée par la fonction cv.glmnet. Il suffit par conséquent d’appliquer la fonction predict à l’objet obtenu avec cv.glmnet en spécifiant la valeur de \\(lambda\\) souhaitée.\n\nRidgeLasso\n\n\n\n\nCode\npredict(ridgeCV, newx = ozone.X[50:51, ], s = \"lambda.min\")\n\n\n         lambda.min\n20010723   90.10981\n20010724   96.74374\n\n\n\n\nCode\npredict(ridgeCV, newx = ozone.X[50:51, ], s = \"lambda.1se\")\n\n\n         lambda.1se\n20010723   93.11072\n20010724   96.26805\n\n\n\n\n\n\nCode\npredict(lassoCV, newx = ozone.X[50:51, ], s = \"lambda.min\")\n\n\n         lambda.min\n20010723   87.18235\n20010724   98.23752\n\n\n\n\nCode\npredict(lassoCV, newx = ozone.X[50:51, ], s = \"lambda.1se\")\n\n\n         lambda.1se\n20010723   87.40631\n20010724   95.85602\n\n\n\n\n\n\n\nComparaison\nPuisque nous avons 3 méthodes de modélisations (MCO sans pénalisation, Ridge et Lasso), la prochaine étape logique va donc ^tre de comparer les trois méthodes pour déterminer laquelle est la plus approrié pour nos données.\n\nOn peut faire cela l’aide d’une validation croisée et pour cela on utilisera les données ozone_complet.txt qui contiennent plus d’individus et de variables.\n\n\nCode\nozone1 &lt;- read.table(\"https://lrouviere.github.io/stat_grand_dim/ozone_complet.txt\", sep=\";\") %&gt;%  na.omit()\nozone1.X &lt;- model.matrix(maxO3 ~ ., data = ozone1)[, -1]\nozone1.Y &lt;- ozone1$maxO3\n\n\n\n\nCode\ncv_ridge_vs_lasso(ozone1, fmla = formula(maxO3 ~ .)) %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nOn remarque que les approches régularisées n’apportent rien par rapport aux estimateurs MCO ici. Ceci peut s’expliquer par le fait que le nombre de variables n’est pas très important.\n\nPour changer, considérons toutes les interactions d’ordre 2\n\n\nCode\ncv_ridge_vs_lasso(ozone1, fmla = formula(maxO3 ~ .^2)) %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nLes méthodes régularisées permettent ici de diminuer les erreurs quadratiques de manière intéressante. Cela vient certainement du fait du nombre de variables explicatives qui est beaucoup plus important lorsqu’on prend en compte toutes les interactions d’ordre 2.\n\n\nCode\nozone2.X &lt;- model.matrix(maxO3 ~ .^2, data = ozone1)[, -1]\n\n\nEn effet, à l’ordre 1 on est à 1366 individus pour 22 variables alors qu’à l’odre 2 on passe à 1366 individus pour 253\n\n\n\nConclusion\nDans cette analyse, nous avons exploré l’utilisation des méthodes Ridge et Lasso sous R.\nCes approches se révèlent particulièrement efficaces en grande dimension, offrant des solutions robustes pour la régression pénalisée.\nPar la suite, le choix entre Lasso et Ridge dépend souvent des objectifs poursuivis :\n\nLasso intègre un mécanisme de sélection de variables, ce qui en fait un excellent outil pour identifier les prédicteurs les plus pertinents.\nRidge conserve l’ensemble des variables en réduisant leurs coefficients sans les annuler, privilégiant ainsi une approche plus réguliarisée.\n\n(Voir la partie Rappel sur Ridge et Lasso pour plus de détails)\n\n\nSession info\n\n\nCode\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-02-21\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package    * version date (UTC) lib source\n dplyr      * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n forcats    * 1.0.0   2023-01-29 [1] CRAN (R 4.4.2)\n ggplot2    * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n glmnet     * 4.1-8   2023-08-22 [1] CRAN (R 4.4.2)\n gridExtra  * 2.3     2017-09-09 [1] CRAN (R 4.4.2)\n kableExtra * 1.4.0   2024-01-24 [1] CRAN (R 4.4.2)\n lubridate  * 1.9.4   2024-12-08 [1] CRAN (R 4.4.2)\n Matrix     * 1.6-5   2024-01-11 [4] CRAN (R 4.3.2)\n purrr      * 1.0.2   2023-08-10 [2] CRAN (R 4.3.3)\n readr      * 2.1.5   2024-01-10 [1] CRAN (R 4.4.2)\n stringr    * 1.5.1   2023-11-14 [2] CRAN (R 4.3.3)\n tibble     * 3.2.1   2023-03-20 [2] CRAN (R 4.3.3)\n tidyr      * 1.3.1   2024-01-24 [1] CRAN (R 4.4.2)\n tidyverse  * 2.0.0   2023-02-22 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "posts/Exercice_08.html",
    "href": "posts/Exercice_08.html",
    "title": "Exercice 08",
    "section": "",
    "text": "Code\n# On considère le jeu de données cookies de la libraire fdm2id qui\n# contient, pour 72 cookies, leur spectre par proche infrarouge \n# (les 700 premières variables, chacune correspondant à une longueur d’onde)\n# ainsi que la mesure de 4 ingrédients (variables 701 à 704).\n# On souhaite prédire le taux de sucre (variable 702)\n# en fonction du spectre.\n# Les 40 premiers cookies formeront l’échantillon d’apprentissage \n# et les 32 derniers l’échantillon test.\n\n\n# Question 0 ------------------------------------------------------------\n\nlibrary(fdm2id)\n\n\nLe chargement a nécessité le package : arules\n\n\nLe chargement a nécessité le package : Matrix\n\n\n\nAttachement du package : 'arules'\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    abbreviate, write\n\n\nLe chargement a nécessité le package : arulesViz\n\n\nLe chargement a nécessité le package : FactoMineR\n\n\n\nAttachement du package : 'fdm2id'\n\n\nLes objets suivants sont masqués depuis 'package:FactoMineR':\n\n    CA, MCA, PCA\n\n\nCode\n#view(cookies)\ndim(cookies)\n\n\n[1]  72 704\n\n\nCode\n# 72 704\n# p&gt;&gt;n \nsummary(cookies)\n\n\n       1                2                3                4         \n Min.   :0.2434   Min.   :0.2433   Min.   :0.2433   Min.   :0.2434  \n 1st Qu.:0.2601   1st Qu.:0.2601   1st Qu.:0.2602   1st Qu.:0.2604  \n Median :0.2744   Median :0.2744   Median :0.2745   Median :0.2746  \n Mean   :0.2754   Mean   :0.2753   Mean   :0.2753   Mean   :0.2753  \n 3rd Qu.:0.2873   3rd Qu.:0.2871   3rd Qu.:0.2872   3rd Qu.:0.2873  \n Max.   :0.3563   Max.   :0.3563   Max.   :0.3567   Max.   :0.3572  \n       5                6                7                8         \n Min.   :0.2431   Min.   :0.2429   Min.   :0.2431   Min.   :0.2433  \n 1st Qu.:0.2606   1st Qu.:0.2609   1st Qu.:0.2613   1st Qu.:0.2618  \n Median :0.2746   Median :0.2747   Median :0.2748   Median :0.2748  \n Mean   :0.2753   Mean   :0.2754   Mean   :0.2756   Mean   :0.2759  \n 3rd Qu.:0.2874   3rd Qu.:0.2875   3rd Qu.:0.2875   3rd Qu.:0.2878  \n Max.   :0.3572   Max.   :0.3578   Max.   :0.3580   Max.   :0.3584  \n       9                10               11               12        \n Min.   :0.2438   Min.   :0.2448   Min.   :0.2459   Min.   :0.2473  \n 1st Qu.:0.2621   1st Qu.:0.2632   1st Qu.:0.2644   1st Qu.:0.2658  \n Median :0.2752   Median :0.2760   Median :0.2771   Median :0.2785  \n Mean   :0.2764   Mean   :0.2772   Mean   :0.2783   Mean   :0.2798  \n 3rd Qu.:0.2884   3rd Qu.:0.2893   3rd Qu.:0.2905   3rd Qu.:0.2920  \n Max.   :0.3589   Max.   :0.3599   Max.   :0.3615   Max.   :0.3631  \n       13               14               15               16        \n Min.   :0.2488   Min.   :0.2509   Min.   :0.2534   Min.   :0.2561  \n 1st Qu.:0.2679   1st Qu.:0.2702   1st Qu.:0.2727   1st Qu.:0.2757  \n Median :0.2801   Median :0.2824   Median :0.2849   Median :0.2879  \n Mean   :0.2815   Mean   :0.2836   Mean   :0.2862   Mean   :0.2894  \n 3rd Qu.:0.2941   3rd Qu.:0.2961   3rd Qu.:0.2988   3rd Qu.:0.3019  \n Max.   :0.3653   Max.   :0.3674   Max.   :0.3704   Max.   :0.3738  \n       17               18               19               20        \n Min.   :0.2600   Min.   :0.2645   Min.   :0.2693   Min.   :0.2745  \n 1st Qu.:0.2791   1st Qu.:0.2828   1st Qu.:0.2871   1st Qu.:0.2920  \n Median :0.2914   Median :0.2958   Median :0.3009   Median :0.3064  \n Mean   :0.2932   Mean   :0.2977   Mean   :0.3028   Mean   :0.3085  \n 3rd Qu.:0.3058   3rd Qu.:0.3100   3rd Qu.:0.3153   3rd Qu.:0.3212  \n Max.   :0.3784   Max.   :0.3834   Max.   :0.3894   Max.   :0.3961  \n       21               22               23               24        \n Min.   :0.2802   Min.   :0.2863   Min.   :0.2927   Min.   :0.2992  \n 1st Qu.:0.2980   1st Qu.:0.3044   1st Qu.:0.3111   1st Qu.:0.3183  \n Median :0.3126   Median :0.3191   Median :0.3261   Median :0.3335  \n Mean   :0.3149   Mean   :0.3217   Mean   :0.3288   Mean   :0.3363  \n 3rd Qu.:0.3280   3rd Qu.:0.3354   3rd Qu.:0.3426   3rd Qu.:0.3502  \n Max.   :0.4036   Max.   :0.4114   Max.   :0.4198   Max.   :0.4284  \n       25               26               27               28        \n Min.   :0.3062   Min.   :0.3136   Min.   :0.3210   Min.   :0.3286  \n 1st Qu.:0.3258   1st Qu.:0.3335   1st Qu.:0.3415   1st Qu.:0.3495  \n Median :0.3415   Median :0.3494   Median :0.3578   Median :0.3661  \n Mean   :0.3442   Mean   :0.3524   Mean   :0.3606   Mean   :0.3689  \n 3rd Qu.:0.3582   3rd Qu.:0.3667   3rd Qu.:0.3751   3rd Qu.:0.3836  \n Max.   :0.4375   Max.   :0.4465   Max.   :0.4559   Max.   :0.4653  \n       29               30               31               32        \n Min.   :0.3363   Min.   :0.3438   Min.   :0.3510   Min.   :0.3579  \n 1st Qu.:0.3573   1st Qu.:0.3652   1st Qu.:0.3728   1st Qu.:0.3801  \n Median :0.3743   Median :0.3822   Median :0.3898   Median :0.3974  \n Mean   :0.3771   Mean   :0.3851   Mean   :0.3929   Mean   :0.4005  \n 3rd Qu.:0.3920   3rd Qu.:0.4003   3rd Qu.:0.4082   3rd Qu.:0.4158  \n Max.   :0.4746   Max.   :0.4835   Max.   :0.4924   Max.   :0.5010  \n       33               34               35               36        \n Min.   :0.3646   Min.   :0.3708   Min.   :0.3765   Min.   :0.3819  \n 1st Qu.:0.3867   1st Qu.:0.3930   1st Qu.:0.3989   1st Qu.:0.4044  \n Median :0.4045   Median :0.4110   Median :0.4172   Median :0.4229  \n Mean   :0.4076   Mean   :0.4142   Mean   :0.4204   Mean   :0.4262  \n 3rd Qu.:0.4230   3rd Qu.:0.4296   3rd Qu.:0.4358   3rd Qu.:0.4415  \n Max.   :0.5090   Max.   :0.5163   Max.   :0.5231   Max.   :0.5294  \n       37               38               39               40        \n Min.   :0.3866   Min.   :0.3914   Min.   :0.3963   Min.   :0.4015  \n 1st Qu.:0.4096   1st Qu.:0.4146   1st Qu.:0.4197   1st Qu.:0.4251  \n Median :0.4284   Median :0.4337   Median :0.4393   Median :0.4450  \n Mean   :0.4316   Mean   :0.4370   Mean   :0.4424   Mean   :0.4480  \n 3rd Qu.:0.4469   3rd Qu.:0.4524   3rd Qu.:0.4579   3rd Qu.:0.4637  \n Max.   :0.5355   Max.   :0.5416   Max.   :0.5476   Max.   :0.5540  \n       41               42               43               44        \n Min.   :0.4070   Min.   :0.4127   Min.   :0.4183   Min.   :0.4237  \n 1st Qu.:0.4309   1st Qu.:0.4369   1st Qu.:0.4430   1st Qu.:0.4487  \n Median :0.4510   Median :0.4570   Median :0.4629   Median :0.4685  \n Mean   :0.4539   Mean   :0.4599   Mean   :0.4658   Mean   :0.4715  \n 3rd Qu.:0.4697   3rd Qu.:0.4758   3rd Qu.:0.4816   3rd Qu.:0.4874  \n Max.   :0.5607   Max.   :0.5676   Max.   :0.5743   Max.   :0.5807  \n       45               46               47               48        \n Min.   :0.4289   Min.   :0.4336   Min.   :0.4379   Min.   :0.4418  \n 1st Qu.:0.4542   1st Qu.:0.4590   1st Qu.:0.4636   1st Qu.:0.4675  \n Median :0.4738   Median :0.4787   Median :0.4833   Median :0.4872  \n Mean   :0.4769   Mean   :0.4819   Mean   :0.4864   Mean   :0.4906  \n 3rd Qu.:0.4930   3rd Qu.:0.4983   3rd Qu.:0.5031   3rd Qu.:0.5074  \n Max.   :0.5866   Max.   :0.5921   Max.   :0.5974   Max.   :0.6024  \n       49               50               51               52        \n Min.   :0.4451   Min.   :0.4480   Min.   :0.4506   Min.   :0.4527  \n 1st Qu.:0.4712   1st Qu.:0.4745   1st Qu.:0.4773   1st Qu.:0.4795  \n Median :0.4908   Median :0.4940   Median :0.4968   Median :0.4989  \n Mean   :0.4943   Mean   :0.4976   Mean   :0.5005   Mean   :0.5028  \n 3rd Qu.:0.5116   3rd Qu.:0.5154   3rd Qu.:0.5187   3rd Qu.:0.5213  \n Max.   :0.6069   Max.   :0.6110   Max.   :0.6144   Max.   :0.6173  \n       53               54               55               56        \n Min.   :0.4542   Min.   :0.4548   Min.   :0.4545   Min.   :0.4529  \n 1st Qu.:0.4812   1st Qu.:0.4821   1st Qu.:0.4818   1st Qu.:0.4801  \n Median :0.5003   Median :0.5009   Median :0.5003   Median :0.4987  \n Mean   :0.5043   Mean   :0.5051   Mean   :0.5047   Mean   :0.5029  \n 3rd Qu.:0.5232   3rd Qu.:0.5241   3rd Qu.:0.5237   3rd Qu.:0.5218  \n Max.   :0.6193   Max.   :0.6203   Max.   :0.6198   Max.   :0.6177  \n       57               58               59               60        \n Min.   :0.4500   Min.   :0.4460   Min.   :0.4406   Min.   :0.4343  \n 1st Qu.:0.4771   1st Qu.:0.4726   1st Qu.:0.4668   1st Qu.:0.4599  \n Median :0.4959   Median :0.4916   Median :0.4856   Median :0.4789  \n Mean   :0.4997   Mean   :0.4953   Mean   :0.4894   Mean   :0.4826  \n 3rd Qu.:0.5185   3rd Qu.:0.5137   3rd Qu.:0.5073   3rd Qu.:0.4997  \n Max.   :0.6137   Max.   :0.6083   Max.   :0.6013   Max.   :0.5930  \n       61               62               63               64        \n Min.   :0.4272   Min.   :0.4198   Min.   :0.4126   Min.   :0.4052  \n 1st Qu.:0.4528   1st Qu.:0.4452   1st Qu.:0.4370   1st Qu.:0.4291  \n Median :0.4717   Median :0.4640   Median :0.4563   Median :0.4489  \n Mean   :0.4751   Mean   :0.4674   Mean   :0.4596   Mean   :0.4520  \n 3rd Qu.:0.4921   3rd Qu.:0.4843   3rd Qu.:0.4763   3rd Qu.:0.4686  \n Max.   :0.5839   Max.   :0.5747   Max.   :0.5652   Max.   :0.5563  \n       65               66               67               68        \n Min.   :0.3980   Min.   :0.3913   Min.   :0.3854   Min.   :0.3801  \n 1st Qu.:0.4218   1st Qu.:0.4150   1st Qu.:0.4091   1st Qu.:0.4039  \n Median :0.4418   Median :0.4353   Median :0.4295   Median :0.4241  \n Mean   :0.4447   Mean   :0.4379   Mean   :0.4319   Mean   :0.4264  \n 3rd Qu.:0.4613   3rd Qu.:0.4545   3rd Qu.:0.4485   3rd Qu.:0.4429  \n Max.   :0.5476   Max.   :0.5399   Max.   :0.5328   Max.   :0.5264  \n       69               70               71               72        \n Min.   :0.3756   Min.   :0.3715   Min.   :0.3679   Min.   :0.3646  \n 1st Qu.:0.3992   1st Qu.:0.3951   1st Qu.:0.3915   1st Qu.:0.3882  \n Median :0.4193   Median :0.4151   Median :0.4113   Median :0.4080  \n Mean   :0.4216   Mean   :0.4173   Mean   :0.4136   Mean   :0.4103  \n 3rd Qu.:0.4380   3rd Qu.:0.4334   3rd Qu.:0.4296   3rd Qu.:0.4262  \n Max.   :0.5208   Max.   :0.5157   Max.   :0.5110   Max.   :0.5072  \n       73               74               75               76        \n Min.   :0.3616   Min.   :0.3589   Min.   :0.3564   Min.   :0.3541  \n 1st Qu.:0.3852   1st Qu.:0.3824   1st Qu.:0.3800   1st Qu.:0.3777  \n Median :0.4050   Median :0.4022   Median :0.3998   Median :0.3976  \n Mean   :0.4073   Mean   :0.4045   Mean   :0.4020   Mean   :0.3997  \n 3rd Qu.:0.4231   3rd Qu.:0.4203   3rd Qu.:0.4178   3rd Qu.:0.4154  \n Max.   :0.5036   Max.   :0.5005   Max.   :0.4977   Max.   :0.4951  \n       77               78               79               80        \n Min.   :0.3522   Min.   :0.3503   Min.   :0.3485   Min.   :0.3470  \n 1st Qu.:0.3756   1st Qu.:0.3738   1st Qu.:0.3722   1st Qu.:0.3707  \n Median :0.3957   Median :0.3938   Median :0.3922   Median :0.3906  \n Mean   :0.3977   Mean   :0.3959   Mean   :0.3942   Mean   :0.3927  \n 3rd Qu.:0.4131   3rd Qu.:0.4112   3rd Qu.:0.4094   3rd Qu.:0.4078  \n Max.   :0.4929   Max.   :0.4908   Max.   :0.4890   Max.   :0.4872  \n       81               82               83               84        \n Min.   :0.3455   Min.   :0.3444   Min.   :0.3432   Min.   :0.3422  \n 1st Qu.:0.3693   1st Qu.:0.3680   1st Qu.:0.3669   1st Qu.:0.3659  \n Median :0.3890   Median :0.3878   Median :0.3867   Median :0.3856  \n Mean   :0.3912   Mean   :0.3899   Mean   :0.3887   Mean   :0.3876  \n 3rd Qu.:0.4063   3rd Qu.:0.4049   3rd Qu.:0.4038   3rd Qu.:0.4027  \n Max.   :0.4855   Max.   :0.4843   Max.   :0.4831   Max.   :0.4819  \n       85               86               87               88        \n Min.   :0.3413   Min.   :0.3404   Min.   :0.3394   Min.   :0.3382  \n 1st Qu.:0.3648   1st Qu.:0.3638   1st Qu.:0.3628   1st Qu.:0.3618  \n Median :0.3846   Median :0.3835   Median :0.3824   Median :0.3813  \n Mean   :0.3866   Mean   :0.3856   Mean   :0.3845   Mean   :0.3834  \n 3rd Qu.:0.4015   3rd Qu.:0.4005   3rd Qu.:0.3995   3rd Qu.:0.3983  \n Max.   :0.4808   Max.   :0.4796   Max.   :0.4784   Max.   :0.4773  \n       89               90               91               92        \n Min.   :0.3371   Min.   :0.3359   Min.   :0.3348   Min.   :0.3337  \n 1st Qu.:0.3606   1st Qu.:0.3595   1st Qu.:0.3583   1st Qu.:0.3572  \n Median :0.3801   Median :0.3790   Median :0.3780   Median :0.3770  \n Mean   :0.3822   Mean   :0.3811   Mean   :0.3800   Mean   :0.3790  \n 3rd Qu.:0.3972   3rd Qu.:0.3960   3rd Qu.:0.3949   3rd Qu.:0.3940  \n Max.   :0.4763   Max.   :0.4753   Max.   :0.4744   Max.   :0.4736  \n       93               94               95               96        \n Min.   :0.3327   Min.   :0.3315   Min.   :0.3305   Min.   :0.3295  \n 1st Qu.:0.3561   1st Qu.:0.3550   1st Qu.:0.3541   1st Qu.:0.3532  \n Median :0.3760   Median :0.3751   Median :0.3742   Median :0.3735  \n Mean   :0.3780   Mean   :0.3771   Mean   :0.3762   Mean   :0.3755  \n 3rd Qu.:0.3930   3rd Qu.:0.3921   3rd Qu.:0.3914   3rd Qu.:0.3908  \n Max.   :0.4729   Max.   :0.4722   Max.   :0.4718   Max.   :0.4714  \n       97               98               99              100        \n Min.   :0.3287   Min.   :0.3280   Min.   :0.3275   Min.   :0.3272  \n 1st Qu.:0.3524   1st Qu.:0.3519   1st Qu.:0.3516   1st Qu.:0.3516  \n Median :0.3730   Median :0.3727   Median :0.3726   Median :0.3726  \n Mean   :0.3749   Mean   :0.3745   Mean   :0.3743   Mean   :0.3743  \n 3rd Qu.:0.3903   3rd Qu.:0.3898   3rd Qu.:0.3898   3rd Qu.:0.3898  \n Max.   :0.4712   Max.   :0.4713   Max.   :0.4717   Max.   :0.4722  \n      101              102              103              104        \n Min.   :0.3270   Min.   :0.3270   Min.   :0.3272   Min.   :0.3276  \n 1st Qu.:0.3518   1st Qu.:0.3520   1st Qu.:0.3525   1st Qu.:0.3532  \n Median :0.3729   Median :0.3734   Median :0.3740   Median :0.3749  \n Mean   :0.3745   Mean   :0.3750   Mean   :0.3756   Mean   :0.3764  \n 3rd Qu.:0.3899   3rd Qu.:0.3906   3rd Qu.:0.3913   3rd Qu.:0.3923  \n Max.   :0.4729   Max.   :0.4738   Max.   :0.4750   Max.   :0.4766  \n      105              106              107              108        \n Min.   :0.3283   Min.   :0.3295   Min.   :0.3309   Min.   :0.3325  \n 1st Qu.:0.3541   1st Qu.:0.3555   1st Qu.:0.3572   1st Qu.:0.3591  \n Median :0.3760   Median :0.3775   Median :0.3792   Median :0.3811  \n Mean   :0.3776   Mean   :0.3792   Mean   :0.3811   Mean   :0.3832  \n 3rd Qu.:0.3935   3rd Qu.:0.3951   3rd Qu.:0.3970   3rd Qu.:0.3992  \n Max.   :0.4786   Max.   :0.4808   Max.   :0.4835   Max.   :0.4864  \n      109              110              111              112        \n Min.   :0.3344   Min.   :0.3364   Min.   :0.3387   Min.   :0.3414  \n 1st Qu.:0.3613   1st Qu.:0.3638   1st Qu.:0.3664   1st Qu.:0.3695  \n Median :0.3834   Median :0.3861   Median :0.3890   Median :0.3922  \n Mean   :0.3857   Mean   :0.3884   Mean   :0.3915   Mean   :0.3948  \n 3rd Qu.:0.4018   3rd Qu.:0.4045   3rd Qu.:0.4076   3rd Qu.:0.4111  \n Max.   :0.4898   Max.   :0.4934   Max.   :0.4976   Max.   :0.5019  \n      113              114              115              116        \n Min.   :0.3445   Min.   :0.3480   Min.   :0.3518   Min.   :0.3556  \n 1st Qu.:0.3728   1st Qu.:0.3768   1st Qu.:0.3808   1st Qu.:0.3851  \n Median :0.3958   Median :0.4000   Median :0.4043   Median :0.4090  \n Mean   :0.3986   Mean   :0.4028   Mean   :0.4073   Mean   :0.4121  \n 3rd Qu.:0.4150   3rd Qu.:0.4193   3rd Qu.:0.4243   3rd Qu.:0.4296  \n Max.   :0.5068   Max.   :0.5121   Max.   :0.5177   Max.   :0.5234  \n      117              118              119              120        \n Min.   :0.3599   Min.   :0.3644   Min.   :0.3690   Min.   :0.3741  \n 1st Qu.:0.3897   1st Qu.:0.3946   1st Qu.:0.3996   1st Qu.:0.4048  \n Median :0.4139   Median :0.4191   Median :0.4245   Median :0.4301  \n Mean   :0.4173   Mean   :0.4226   Mean   :0.4282   Mean   :0.4339  \n 3rd Qu.:0.4350   3rd Qu.:0.4405   3rd Qu.:0.4463   3rd Qu.:0.4524  \n Max.   :0.5297   Max.   :0.5363   Max.   :0.5430   Max.   :0.5500  \n      121              122              123              124        \n Min.   :0.3794   Min.   :0.3848   Min.   :0.3902   Min.   :0.3958  \n 1st Qu.:0.4104   1st Qu.:0.4162   1st Qu.:0.4224   1st Qu.:0.4286  \n Median :0.4359   Median :0.4420   Median :0.4483   Median :0.4550  \n Mean   :0.4399   Mean   :0.4461   Mean   :0.4524   Mean   :0.4589  \n 3rd Qu.:0.4586   3rd Qu.:0.4650   3rd Qu.:0.4715   3rd Qu.:0.4781  \n Max.   :0.5570   Max.   :0.5642   Max.   :0.5715   Max.   :0.5789  \n      125              126              127              128        \n Min.   :0.4016   Min.   :0.4074   Min.   :0.4134   Min.   :0.4195  \n 1st Qu.:0.4351   1st Qu.:0.4416   1st Qu.:0.4484   1st Qu.:0.4551  \n Median :0.4615   Median :0.4680   Median :0.4748   Median :0.4815  \n Mean   :0.4654   Mean   :0.4720   Mean   :0.4786   Mean   :0.4853  \n 3rd Qu.:0.4847   3rd Qu.:0.4914   3rd Qu.:0.4984   3rd Qu.:0.5054  \n Max.   :0.5862   Max.   :0.5937   Max.   :0.6013   Max.   :0.6091  \n      129              130              131              132        \n Min.   :0.4255   Min.   :0.4314   Min.   :0.4373   Min.   :0.4428  \n 1st Qu.:0.4616   1st Qu.:0.4680   1st Qu.:0.4743   1st Qu.:0.4803  \n Median :0.4882   Median :0.4950   Median :0.5016   Median :0.5081  \n Mean   :0.4920   Mean   :0.4986   Mean   :0.5051   Mean   :0.5115  \n 3rd Qu.:0.5125   3rd Qu.:0.5195   3rd Qu.:0.5265   3rd Qu.:0.5332  \n Max.   :0.6166   Max.   :0.6243   Max.   :0.6318   Max.   :0.6393  \n      133              134              135              136        \n Min.   :0.4484   Min.   :0.4542   Min.   :0.4602   Min.   :0.4662  \n 1st Qu.:0.4864   1st Qu.:0.4924   1st Qu.:0.4987   1st Qu.:0.5051  \n Median :0.5144   Median :0.5210   Median :0.5277   Median :0.5345  \n Mean   :0.5180   Mean   :0.5244   Mean   :0.5311   Mean   :0.5379  \n 3rd Qu.:0.5399   3rd Qu.:0.5465   3rd Qu.:0.5534   3rd Qu.:0.5604  \n Max.   :0.6468   Max.   :0.6545   Max.   :0.6626   Max.   :0.6712  \n      137              138              139              140        \n Min.   :0.4725   Min.   :0.4788   Min.   :0.4855   Min.   :0.4928  \n 1st Qu.:0.5119   1st Qu.:0.5189   1st Qu.:0.5264   1st Qu.:0.5348  \n Median :0.5416   Median :0.5491   Median :0.5567   Median :0.5650  \n Mean   :0.5450   Mean   :0.5525   Mean   :0.5604   Mean   :0.5693  \n 3rd Qu.:0.5678   3rd Qu.:0.5756   3rd Qu.:0.5837   3rd Qu.:0.5928  \n Max.   :0.6804   Max.   :0.6901   Max.   :0.7009   Max.   :0.7129  \n      141              142              143              144        \n Min.   :0.5009   Min.   :0.5097   Min.   :0.5194   Min.   :0.5295  \n 1st Qu.:0.5441   1st Qu.:0.5539   1st Qu.:0.5648   1st Qu.:0.5765  \n Median :0.5741   Median :0.5842   Median :0.5955   Median :0.6079  \n Mean   :0.5790   Mean   :0.5898   Mean   :0.6016   Mean   :0.6143  \n 3rd Qu.:0.6030   3rd Qu.:0.6144   3rd Qu.:0.6267   3rd Qu.:0.6400  \n Max.   :0.7258   Max.   :0.7402   Max.   :0.7560   Max.   :0.7728  \n      145              146              147              148        \n Min.   :0.5401   Min.   :0.5512   Min.   :0.5626   Min.   :0.5748  \n 1st Qu.:0.5888   1st Qu.:0.6019   1st Qu.:0.6156   1st Qu.:0.6303  \n Median :0.6208   Median :0.6347   Median :0.6495   Median :0.6652  \n Mean   :0.6275   Mean   :0.6414   Mean   :0.6562   Mean   :0.6719  \n 3rd Qu.:0.6541   3rd Qu.:0.6690   3rd Qu.:0.6846   3rd Qu.:0.7012  \n Max.   :0.7903   Max.   :0.8084   Max.   :0.8276   Max.   :0.8477  \n      149              150              151              152        \n Min.   :0.5880   Min.   :0.6019   Min.   :0.6171   Min.   :0.6334  \n 1st Qu.:0.6458   1st Qu.:0.6624   1st Qu.:0.6803   1st Qu.:0.6990  \n Median :0.6820   Median :0.6999   Median :0.7191   Median :0.7394  \n Mean   :0.6886   Mean   :0.7064   Mean   :0.7255   Mean   :0.7457  \n 3rd Qu.:0.7189   3rd Qu.:0.7378   3rd Qu.:0.7580   3rd Qu.:0.7794  \n Max.   :0.8689   Max.   :0.8907   Max.   :0.9140   Max.   :0.9380  \n      153              154              155              156        \n Min.   :0.6503   Min.   :0.6676   Min.   :0.6847   Min.   :0.7020  \n 1st Qu.:0.7186   1st Qu.:0.7385   1st Qu.:0.7584   1st Qu.:0.7779  \n Median :0.7602   Median :0.7813   Median :0.8022   Median :0.8229  \n Mean   :0.7664   Mean   :0.7874   Mean   :0.8084   Mean   :0.8291  \n 3rd Qu.:0.8013   3rd Qu.:0.8232   3rd Qu.:0.8452   3rd Qu.:0.8670  \n Max.   :0.9622   Max.   :0.9862   Max.   :1.0096   Max.   :1.0322  \n      157              158              159              160        \n Min.   :0.7193   Min.   :0.7363   Min.   :0.7530   Min.   :0.7693  \n 1st Qu.:0.7973   1st Qu.:0.8163   1st Qu.:0.8347   1st Qu.:0.8525  \n Median :0.8434   Median :0.8636   Median :0.8829   Median :0.9019  \n Mean   :0.8496   Mean   :0.8697   Mean   :0.8891   Mean   :0.9081  \n 3rd Qu.:0.8885   3rd Qu.:0.9095   3rd Qu.:0.9301   3rd Qu.:0.9503  \n Max.   :1.0538   Max.   :1.0743   Max.   :1.0939   Max.   :1.1123  \n      161              162              163              164        \n Min.   :0.7855   Min.   :0.8017   Min.   :0.8180   Min.   :0.8339  \n 1st Qu.:0.8699   1st Qu.:0.8869   1st Qu.:0.9033   1st Qu.:0.9187  \n Median :0.9203   Median :0.9380   Median :0.9559   Median :0.9740  \n Mean   :0.9265   Mean   :0.9443   Mean   :0.9615   Mean   :0.9779  \n 3rd Qu.:0.9700   3rd Qu.:0.9892   3rd Qu.:1.0079   3rd Qu.:1.0248  \n Max.   :1.1296   Max.   :1.1457   Max.   :1.1607   Max.   :1.1746  \n      165              166              167              168        \n Min.   :0.8496   Min.   :0.8643   Min.   :0.8771   Min.   :0.8878  \n 1st Qu.:0.9349   1st Qu.:0.9505   1st Qu.:0.9627   1st Qu.:0.9738  \n Median :0.9910   Median :1.0054   Median :1.0181   Median :1.0292  \n Mean   :0.9936   Mean   :1.0080   Mean   :1.0209   Mean   :1.0320  \n 3rd Qu.:1.0395   3rd Qu.:1.0531   3rd Qu.:1.0668   3rd Qu.:1.0792  \n Max.   :1.1873   Max.   :1.1982   Max.   :1.2081   Max.   :1.2171  \n      169              170             171              172        \n Min.   :0.8958   Min.   :0.901   Min.   :0.9042   Min.   :0.9057  \n 1st Qu.:0.9834   1st Qu.:0.990   1st Qu.:0.9949   1st Qu.:0.9988  \n Median :1.0385   Median :1.046   Median :1.0513   Median :1.0556  \n Mean   :1.0412   Mean   :1.048   Mean   :1.0536   Mean   :1.0577  \n 3rd Qu.:1.0891   3rd Qu.:1.096   3rd Qu.:1.1013   3rd Qu.:1.1047  \n Max.   :1.2249   Max.   :1.231   Max.   :1.2375   Max.   :1.2431  \n      173              174              175              176        \n Min.   :0.9065   Min.   :0.9068   Min.   :0.9073   Min.   :0.9077  \n 1st Qu.:1.0023   1st Qu.:1.0053   1st Qu.:1.0076   1st Qu.:1.0090  \n Median :1.0586   Median :1.0605   Median :1.0615   Median :1.0631  \n Mean   :1.0608   Mean   :1.0632   Mean   :1.0652   Mean   :1.0669  \n 3rd Qu.:1.1078   3rd Qu.:1.1103   3rd Qu.:1.1117   3rd Qu.:1.1129  \n Max.   :1.2476   Max.   :1.2512   Max.   :1.2541   Max.   :1.2564  \n      177              178              179              180        \n Min.   :0.9082   Min.   :0.9089   Min.   :0.9096   Min.   :0.9103  \n 1st Qu.:1.0103   1st Qu.:1.0114   1st Qu.:1.0125   1st Qu.:1.0135  \n Median :1.0649   Median :1.0663   Median :1.0676   Median :1.0686  \n Mean   :1.0684   Mean   :1.0696   Mean   :1.0707   Mean   :1.0715  \n 3rd Qu.:1.1142   3rd Qu.:1.1152   3rd Qu.:1.1162   3rd Qu.:1.1171  \n Max.   :1.2581   Max.   :1.2594   Max.   :1.2602   Max.   :1.2606  \n      181              182              183              184        \n Min.   :0.9111   Min.   :0.9115   Min.   :0.9116   Min.   :0.9114  \n 1st Qu.:1.0143   1st Qu.:1.0146   1st Qu.:1.0143   1st Qu.:1.0136  \n Median :1.0693   Median :1.0696   Median :1.0696   Median :1.0692  \n Mean   :1.0721   Mean   :1.0723   Mean   :1.0722   Mean   :1.0717  \n 3rd Qu.:1.1176   3rd Qu.:1.1179   3rd Qu.:1.1178   3rd Qu.:1.1174  \n Max.   :1.2606   Max.   :1.2603   Max.   :1.2595   Max.   :1.2582  \n      185              186              187             188        \n Min.   :0.9109   Min.   :0.9101   Min.   :0.909   Min.   :0.9074  \n 1st Qu.:1.0125   1st Qu.:1.0109   1st Qu.:1.009   1st Qu.:1.0066  \n Median :1.0685   Median :1.0671   Median :1.065   Median :1.0628  \n Mean   :1.0708   Mean   :1.0695   Mean   :1.068   Mean   :1.0656  \n 3rd Qu.:1.1164   3rd Qu.:1.1151   3rd Qu.:1.113   3rd Qu.:1.1112  \n Max.   :1.2565   Max.   :1.2542   Max.   :1.252   Max.   :1.2486  \n      189              190              191              192        \n Min.   :0.9057   Min.   :0.9035   Min.   :0.9012   Min.   :0.8987  \n 1st Qu.:1.0039   1st Qu.:1.0009   1st Qu.:0.9979   1st Qu.:0.9947  \n Median :1.0601   Median :1.0571   Median :1.0541   Median :1.0511  \n Mean   :1.0630   Mean   :1.0602   Mean   :1.0572   Mean   :1.0540  \n 3rd Qu.:1.1085   3rd Qu.:1.1060   3rd Qu.:1.1032   3rd Qu.:1.1002  \n Max.   :1.2449   Max.   :1.2412   Max.   :1.2371   Max.   :1.2329  \n      193              194              195              196        \n Min.   :0.8960   Min.   :0.8933   Min.   :0.8904   Min.   :0.8874  \n 1st Qu.:0.9914   1st Qu.:0.9880   1st Qu.:0.9844   1st Qu.:0.9806  \n Median :1.0479   Median :1.0446   Median :1.0411   Median :1.0373  \n Mean   :1.0506   Mean   :1.0471   Mean   :1.0434   Mean   :1.0396  \n 3rd Qu.:1.0970   3rd Qu.:1.0936   3rd Qu.:1.0898   3rd Qu.:1.0858  \n Max.   :1.2286   Max.   :1.2243   Max.   :1.2197   Max.   :1.2152  \n      197              198              199              200        \n Min.   :0.8842   Min.   :0.8810   Min.   :0.8778   Min.   :0.8746  \n 1st Qu.:0.9768   1st Qu.:0.9728   1st Qu.:0.9690   1st Qu.:0.9649  \n Median :1.0332   Median :1.0290   Median :1.0247   Median :1.0203  \n Mean   :1.0357   Mean   :1.0316   Mean   :1.0275   Mean   :1.0233  \n 3rd Qu.:1.0819   3rd Qu.:1.0778   3rd Qu.:1.0736   3rd Qu.:1.0692  \n Max.   :1.2109   Max.   :1.2064   Max.   :1.2019   Max.   :1.1974  \n      201              202              203              204        \n Min.   :0.8716   Min.   :0.8684   Min.   :0.8652   Min.   :0.8620  \n 1st Qu.:0.9609   1st Qu.:0.9568   1st Qu.:0.9527   1st Qu.:0.9486  \n Median :1.0159   Median :1.0114   Median :1.0070   Median :1.0029  \n Mean   :1.0192   Mean   :1.0150   Mean   :1.0109   Mean   :1.0068  \n 3rd Qu.:1.0649   3rd Qu.:1.0605   3rd Qu.:1.0561   3rd Qu.:1.0517  \n Max.   :1.1928   Max.   :1.1884   Max.   :1.1840   Max.   :1.1799  \n      205              206              207              208        \n Min.   :0.8589   Min.   :0.8558   Min.   :0.8528   Min.   :0.8498  \n 1st Qu.:0.9448   1st Qu.:0.9409   1st Qu.:0.9373   1st Qu.:0.9336  \n Median :0.9987   Median :0.9946   Median :0.9908   Median :0.9869  \n Mean   :1.0028   Mean   :0.9989   Mean   :0.9950   Mean   :0.9912  \n 3rd Qu.:1.0477   3rd Qu.:1.0435   3rd Qu.:1.0395   3rd Qu.:1.0354  \n Max.   :1.1756   Max.   :1.1713   Max.   :1.1671   Max.   :1.1628  \n      209              210              211              212        \n Min.   :0.8472   Min.   :0.8444   Min.   :0.8414   Min.   :0.8386  \n 1st Qu.:0.9302   1st Qu.:0.9267   1st Qu.:0.9234   1st Qu.:0.9202  \n Median :0.9831   Median :0.9796   Median :0.9764   Median :0.9731  \n Mean   :0.9877   Mean   :0.9842   Mean   :0.9809   Mean   :0.9777  \n 3rd Qu.:1.0316   3rd Qu.:1.0279   3rd Qu.:1.0244   3rd Qu.:1.0210  \n Max.   :1.1588   Max.   :1.1549   Max.   :1.1511   Max.   :1.1474  \n      213              214              215              216        \n Min.   :0.8358   Min.   :0.8332   Min.   :0.8307   Min.   :0.8280  \n 1st Qu.:0.9172   1st Qu.:0.9143   1st Qu.:0.9116   1st Qu.:0.9090  \n Median :0.9702   Median :0.9672   Median :0.9645   Median :0.9620  \n Mean   :0.9746   Mean   :0.9717   Mean   :0.9689   Mean   :0.9662  \n 3rd Qu.:1.0179   3rd Qu.:1.0149   3rd Qu.:1.0120   3rd Qu.:1.0092  \n Max.   :1.1439   Max.   :1.1406   Max.   :1.1374   Max.   :1.1344  \n      217              218              219              220        \n Min.   :0.8257   Min.   :0.8234   Min.   :0.8211   Min.   :0.8189  \n 1st Qu.:0.9066   1st Qu.:0.9042   1st Qu.:0.9020   1st Qu.:0.8999  \n Median :0.9597   Median :0.9573   Median :0.9550   Median :0.9528  \n Mean   :0.9637   Mean   :0.9613   Mean   :0.9589   Mean   :0.9568  \n 3rd Qu.:1.0066   3rd Qu.:1.0040   3rd Qu.:1.0014   3rd Qu.:0.9991  \n Max.   :1.1316   Max.   :1.1290   Max.   :1.1264   Max.   :1.1240  \n      221              222              223              224        \n Min.   :0.8169   Min.   :0.8151   Min.   :0.8133   Min.   :0.8117  \n 1st Qu.:0.8981   1st Qu.:0.8965   1st Qu.:0.8949   1st Qu.:0.8935  \n Median :0.9509   Median :0.9492   Median :0.9478   Median :0.9464  \n Mean   :0.9548   Mean   :0.9529   Mean   :0.9511   Mean   :0.9495  \n 3rd Qu.:0.9970   3rd Qu.:0.9949   3rd Qu.:0.9930   3rd Qu.:0.9911  \n Max.   :1.1217   Max.   :1.1198   Max.   :1.1177   Max.   :1.1157  \n      225              226              227              228        \n Min.   :0.8102   Min.   :0.8088   Min.   :0.8073   Min.   :0.8061  \n 1st Qu.:0.8921   1st Qu.:0.8908   1st Qu.:0.8895   1st Qu.:0.8882  \n Median :0.9449   Median :0.9437   Median :0.9427   Median :0.9416  \n Mean   :0.9479   Mean   :0.9464   Mean   :0.9451   Mean   :0.9437  \n 3rd Qu.:0.9895   3rd Qu.:0.9878   3rd Qu.:0.9863   3rd Qu.:0.9852  \n Max.   :1.1138   Max.   :1.1120   Max.   :1.1103   Max.   :1.1085  \n      229              230              231              232        \n Min.   :0.8052   Min.   :0.8044   Min.   :0.8035   Min.   :0.8028  \n 1st Qu.:0.8871   1st Qu.:0.8860   1st Qu.:0.8849   1st Qu.:0.8837  \n Median :0.9408   Median :0.9402   Median :0.9395   Median :0.9389  \n Mean   :0.9425   Mean   :0.9413   Mean   :0.9401   Mean   :0.9389  \n 3rd Qu.:0.9841   3rd Qu.:0.9829   3rd Qu.:0.9818   3rd Qu.:0.9806  \n Max.   :1.1068   Max.   :1.1049   Max.   :1.1031   Max.   :1.1014  \n      233              234              235              236        \n Min.   :0.8021   Min.   :0.8013   Min.   :0.8003   Min.   :0.7992  \n 1st Qu.:0.8824   1st Qu.:0.8811   1st Qu.:0.8796   1st Qu.:0.8779  \n Median :0.9383   Median :0.9376   Median :0.9363   Median :0.9347  \n Mean   :0.9377   Mean   :0.9363   Mean   :0.9348   Mean   :0.9332  \n 3rd Qu.:0.9794   3rd Qu.:0.9781   3rd Qu.:0.9766   3rd Qu.:0.9751  \n Max.   :1.0996   Max.   :1.0977   Max.   :1.0956   Max.   :1.0934  \n      237              238              239              240        \n Min.   :0.7979   Min.   :0.7965   Min.   :0.7947   Min.   :0.7928  \n 1st Qu.:0.8761   1st Qu.:0.8742   1st Qu.:0.8722   1st Qu.:0.8703  \n Median :0.9329   Median :0.9309   Median :0.9287   Median :0.9263  \n Mean   :0.9314   Mean   :0.9295   Mean   :0.9273   Mean   :0.9249  \n 3rd Qu.:0.9733   3rd Qu.:0.9712   3rd Qu.:0.9688   3rd Qu.:0.9664  \n Max.   :1.0911   Max.   :1.0886   Max.   :1.0860   Max.   :1.0831  \n      241              242              243              244        \n Min.   :0.7908   Min.   :0.7885   Min.   :0.7860   Min.   :0.7832  \n 1st Qu.:0.8681   1st Qu.:0.8654   1st Qu.:0.8624   1st Qu.:0.8592  \n Median :0.9237   Median :0.9209   Median :0.9178   Median :0.9145  \n Mean   :0.9223   Mean   :0.9196   Mean   :0.9166   Mean   :0.9134  \n 3rd Qu.:0.9638   3rd Qu.:0.9610   3rd Qu.:0.9580   3rd Qu.:0.9549  \n Max.   :1.0801   Max.   :1.0772   Max.   :1.0740   Max.   :1.0706  \n      245              246              247              248        \n Min.   :0.7801   Min.   :0.7768   Min.   :0.7732   Min.   :0.7694  \n 1st Qu.:0.8558   1st Qu.:0.8522   1st Qu.:0.8482   1st Qu.:0.8440  \n Median :0.9110   Median :0.9070   Median :0.9028   Median :0.8985  \n Mean   :0.9099   Mean   :0.9062   Mean   :0.9023   Mean   :0.8981  \n 3rd Qu.:0.9515   3rd Qu.:0.9476   3rd Qu.:0.9435   3rd Qu.:0.9390  \n Max.   :1.0671   Max.   :1.0633   Max.   :1.0593   Max.   :1.0553  \n      249              250              251              252        \n Min.   :0.7653   Min.   :0.7610   Min.   :0.7568   Min.   :0.7523  \n 1st Qu.:0.8397   1st Qu.:0.8354   1st Qu.:0.8310   1st Qu.:0.8268  \n Median :0.8939   Median :0.8895   Median :0.8851   Median :0.8804  \n Mean   :0.8938   Mean   :0.8894   Mean   :0.8849   Mean   :0.8803  \n 3rd Qu.:0.9345   3rd Qu.:0.9298   3rd Qu.:0.9252   3rd Qu.:0.9204  \n Max.   :1.0512   Max.   :1.0470   Max.   :1.0426   Max.   :1.0382  \n      253              254              255              256        \n Min.   :0.7477   Min.   :0.7435   Min.   :0.7390   Min.   :0.7346  \n 1st Qu.:0.8222   1st Qu.:0.8177   1st Qu.:0.8131   1st Qu.:0.8084  \n Median :0.8756   Median :0.8709   Median :0.8661   Median :0.8613  \n Mean   :0.8756   Mean   :0.8709   Mean   :0.8662   Mean   :0.8614  \n 3rd Qu.:0.9155   3rd Qu.:0.9106   3rd Qu.:0.9057   3rd Qu.:0.9009  \n Max.   :1.0337   Max.   :1.0293   Max.   :1.0249   Max.   :1.0203  \n      257              258              259              260        \n Min.   :0.7303   Min.   :0.7261   Min.   :0.7219   Min.   :0.7177  \n 1st Qu.:0.8038   1st Qu.:0.7993   1st Qu.:0.7949   1st Qu.:0.7905  \n Median :0.8566   Median :0.8516   Median :0.8467   Median :0.8418  \n Mean   :0.8567   Mean   :0.8520   Mean   :0.8474   Mean   :0.8429  \n 3rd Qu.:0.8960   3rd Qu.:0.8911   3rd Qu.:0.8864   3rd Qu.:0.8816  \n Max.   :1.0156   Max.   :1.0110   Max.   :1.0065   Max.   :1.0020  \n      261              262              263              264        \n Min.   :0.7138   Min.   :0.7099   Min.   :0.7059   Min.   :0.7021  \n 1st Qu.:0.7863   1st Qu.:0.7820   1st Qu.:0.7777   1st Qu.:0.7737  \n Median :0.8371   Median :0.8326   Median :0.8281   Median :0.8237  \n Mean   :0.8384   Mean   :0.8340   Mean   :0.8296   Mean   :0.8254  \n 3rd Qu.:0.8769   3rd Qu.:0.8722   3rd Qu.:0.8676   3rd Qu.:0.8632  \n Max.   :0.9976   Max.   :0.9936   Max.   :0.9895   Max.   :0.9853  \n      265              266              267              268        \n Min.   :0.6984   Min.   :0.6949   Min.   :0.6913   Min.   :0.6878  \n 1st Qu.:0.7697   1st Qu.:0.7659   1st Qu.:0.7623   1st Qu.:0.7588  \n Median :0.8194   Median :0.8152   Median :0.8111   Median :0.8071  \n Mean   :0.8212   Mean   :0.8171   Mean   :0.8132   Mean   :0.8093  \n 3rd Qu.:0.8590   3rd Qu.:0.8548   3rd Qu.:0.8507   3rd Qu.:0.8466  \n Max.   :0.9811   Max.   :0.9771   Max.   :0.9732   Max.   :0.9693  \n      269              270              271              272        \n Min.   :0.6845   Min.   :0.6814   Min.   :0.6786   Min.   :0.6759  \n 1st Qu.:0.7553   1st Qu.:0.7521   1st Qu.:0.7489   1st Qu.:0.7459  \n Median :0.8033   Median :0.7998   Median :0.7964   Median :0.7930  \n Mean   :0.8057   Mean   :0.8023   Mean   :0.7990   Mean   :0.7958  \n 3rd Qu.:0.8427   3rd Qu.:0.8390   3rd Qu.:0.8356   3rd Qu.:0.8324  \n Max.   :0.9658   Max.   :0.9624   Max.   :0.9592   Max.   :0.9561  \n      273              274              275              276        \n Min.   :0.6735   Min.   :0.6711   Min.   :0.6688   Min.   :0.6667  \n 1st Qu.:0.7431   1st Qu.:0.7405   1st Qu.:0.7381   1st Qu.:0.7360  \n Median :0.7899   Median :0.7869   Median :0.7841   Median :0.7815  \n Mean   :0.7928   Mean   :0.7900   Mean   :0.7873   Mean   :0.7848  \n 3rd Qu.:0.8292   3rd Qu.:0.8262   3rd Qu.:0.8233   3rd Qu.:0.8208  \n Max.   :0.9531   Max.   :0.9502   Max.   :0.9475   Max.   :0.9451  \n      277              278              279              280        \n Min.   :0.6648   Min.   :0.6633   Min.   :0.6621   Min.   :0.6612  \n 1st Qu.:0.7340   1st Qu.:0.7323   1st Qu.:0.7310   1st Qu.:0.7300  \n Median :0.7791   Median :0.7771   Median :0.7753   Median :0.7738  \n Mean   :0.7827   Mean   :0.7808   Mean   :0.7793   Mean   :0.7780  \n 3rd Qu.:0.8185   3rd Qu.:0.8164   3rd Qu.:0.8145   3rd Qu.:0.8131  \n Max.   :0.9431   Max.   :0.9414   Max.   :0.9400   Max.   :0.9389  \n      281              282              283              284        \n Min.   :0.6608   Min.   :0.6608   Min.   :0.6608   Min.   :0.6614  \n 1st Qu.:0.7294   1st Qu.:0.7292   1st Qu.:0.7295   1st Qu.:0.7302  \n Median :0.7727   Median :0.7721   Median :0.7719   Median :0.7722  \n Mean   :0.7772   Mean   :0.7767   Mean   :0.7766   Mean   :0.7770  \n 3rd Qu.:0.8120   3rd Qu.:0.8113   3rd Qu.:0.8110   3rd Qu.:0.8112  \n Max.   :0.9383   Max.   :0.9379   Max.   :0.9383   Max.   :0.9389  \n      285              286              287              288        \n Min.   :0.6628   Min.   :0.6651   Min.   :0.6683   Min.   :0.6726  \n 1st Qu.:0.7315   1st Qu.:0.7336   1st Qu.:0.7364   1st Qu.:0.7404  \n Median :0.7732   Median :0.7752   Median :0.7781   Median :0.7820  \n Mean   :0.7782   Mean   :0.7801   Mean   :0.7829   Mean   :0.7867  \n 3rd Qu.:0.8120   3rd Qu.:0.8137   3rd Qu.:0.8165   3rd Qu.:0.8207  \n Max.   :0.9404   Max.   :0.9427   Max.   :0.9459   Max.   :0.9498  \n      289              290              291              292        \n Min.   :0.6778   Min.   :0.6842   Min.   :0.6917   Min.   :0.7001  \n 1st Qu.:0.7455   1st Qu.:0.7517   1st Qu.:0.7586   1st Qu.:0.7668  \n Median :0.7872   Median :0.7939   Median :0.8016   Median :0.8105  \n Mean   :0.7916   Mean   :0.7977   Mean   :0.8050   Mean   :0.8135  \n 3rd Qu.:0.8260   3rd Qu.:0.8326   3rd Qu.:0.8402   3rd Qu.:0.8492  \n Max.   :0.9549   Max.   :0.9610   Max.   :0.9684   Max.   :0.9769  \n      293              294              295              296        \n Min.   :0.7094   Min.   :0.7187   Min.   :0.7278   Min.   :0.7367  \n 1st Qu.:0.7759   1st Qu.:0.7853   1st Qu.:0.7946   1st Qu.:0.8039  \n Median :0.8203   Median :0.8302   Median :0.8400   Median :0.8497  \n Mean   :0.8227   Mean   :0.8323   Mean   :0.8418   Mean   :0.8513  \n 3rd Qu.:0.8592   3rd Qu.:0.8696   3rd Qu.:0.8794   3rd Qu.:0.8886  \n Max.   :0.9866   Max.   :0.9970   Max.   :1.0076   Max.   :1.0184  \n      297              298              299              300        \n Min.   :0.7452   Min.   :0.7537   Min.   :0.7621   Min.   :0.7712  \n 1st Qu.:0.8134   1st Qu.:0.8229   1st Qu.:0.8323   1st Qu.:0.8421  \n Median :0.8582   Median :0.8660   Median :0.8740   Median :0.8824  \n Mean   :0.8606   Mean   :0.8699   Mean   :0.8792   Mean   :0.8887  \n 3rd Qu.:0.8969   3rd Qu.:0.9051   3rd Qu.:0.9131   3rd Qu.:0.9218  \n Max.   :1.0291   Max.   :1.0399   Max.   :1.0505   Max.   :1.0612  \n      301              302              303              304        \n Min.   :0.7806   Min.   :0.7901   Min.   :0.7992   Min.   :0.8076  \n 1st Qu.:0.8521   1st Qu.:0.8621   1st Qu.:0.8715   1st Qu.:0.8802  \n Median :0.8918   Median :0.9012   Median :0.9094   Median :0.9169  \n Mean   :0.8982   Mean   :0.9075   Mean   :0.9163   Mean   :0.9243  \n 3rd Qu.:0.9325   3rd Qu.:0.9427   3rd Qu.:0.9525   3rd Qu.:0.9612  \n Max.   :1.0720   Max.   :1.0821   Max.   :1.0917   Max.   :1.1002  \n      305              306              307              308        \n Min.   :0.8155   Min.   :0.8230   Min.   :0.8304   Min.   :0.8378  \n 1st Qu.:0.8881   1st Qu.:0.8957   1st Qu.:0.9030   1st Qu.:0.9097  \n Median :0.9240   Median :0.9311   Median :0.9377   Median :0.9446  \n Mean   :0.9319   Mean   :0.9390   Mean   :0.9460   Mean   :0.9530  \n 3rd Qu.:0.9694   3rd Qu.:0.9767   3rd Qu.:0.9840   3rd Qu.:0.9912  \n Max.   :1.1082   Max.   :1.1155   Max.   :1.1225   Max.   :1.1296  \n      309              310              311              312        \n Min.   :0.8453   Min.   :0.8527   Min.   :0.8593   Min.   :0.8641  \n 1st Qu.:0.9161   1st Qu.:0.9225   1st Qu.:0.9286   1st Qu.:0.9338  \n Median :0.9517   Median :0.9586   Median :0.9647   Median :0.9696  \n Mean   :0.9599   Mean   :0.9666   Mean   :0.9729   Mean   :0.9777  \n 3rd Qu.:0.9983   3rd Qu.:1.0052   3rd Qu.:1.0115   3rd Qu.:1.0165  \n Max.   :1.1365   Max.   :1.1432   Max.   :1.1493   Max.   :1.1540  \n      313              314              315              316        \n Min.   :0.8669   Min.   :0.8672   Min.   :0.8650   Min.   :0.8602  \n 1st Qu.:0.9372   1st Qu.:0.9385   1st Qu.:0.9367   1st Qu.:0.9320  \n Median :0.9726   Median :0.9735   Median :0.9720   Median :0.9682  \n Mean   :0.9807   Mean   :0.9816   Mean   :0.9799   Mean   :0.9760  \n 3rd Qu.:1.0196   3rd Qu.:1.0203   3rd Qu.:1.0185   3rd Qu.:1.0139  \n Max.   :1.1569   Max.   :1.1576   Max.   :1.1556   Max.   :1.1514  \n      317              318              319              320        \n Min.   :0.8539   Min.   :0.8467   Min.   :0.8395   Min.   :0.8333  \n 1st Qu.:0.9262   1st Qu.:0.9201   1st Qu.:0.9142   1st Qu.:0.9092  \n Median :0.9632   Median :0.9575   Median :0.9518   Median :0.9466  \n Mean   :0.9705   Mean   :0.9643   Mean   :0.9582   Mean   :0.9531  \n 3rd Qu.:1.0076   3rd Qu.:1.0006   3rd Qu.:0.9938   3rd Qu.:0.9881  \n Max.   :1.1457   Max.   :1.1392   Max.   :1.1330   Max.   :1.1280  \n      321              322              323              324        \n Min.   :0.8285   Min.   :0.8255   Min.   :0.8245   Min.   :0.8252  \n 1st Qu.:0.9048   1st Qu.:0.9019   1st Qu.:0.9010   1st Qu.:0.9017  \n Median :0.9428   Median :0.9410   Median :0.9408   Median :0.9420  \n Mean   :0.9493   Mean   :0.9472   Mean   :0.9469   Mean   :0.9480  \n 3rd Qu.:0.9839   3rd Qu.:0.9817   3rd Qu.:0.9813   3rd Qu.:0.9826  \n Max.   :1.1247   Max.   :1.1229   Max.   :1.1231   Max.   :1.1247  \n      325              326              327              328        \n Min.   :0.8272   Min.   :0.8300   Min.   :0.8336   Min.   :0.8371  \n 1st Qu.:0.9036   1st Qu.:0.9067   1st Qu.:0.9102   1st Qu.:0.9137  \n Median :0.9445   Median :0.9475   Median :0.9511   Median :0.9545  \n Mean   :0.9503   Mean   :0.9535   Mean   :0.9572   Mean   :0.9610  \n 3rd Qu.:0.9852   3rd Qu.:0.9888   3rd Qu.:0.9929   3rd Qu.:0.9970  \n Max.   :1.1275   Max.   :1.1310   Max.   :1.1350   Max.   :1.1391  \n      329              330              331              332        \n Min.   :0.8401   Min.   :0.8423   Min.   :0.8430   Min.   :0.8423  \n 1st Qu.:0.9170   1st Qu.:0.9195   1st Qu.:0.9207   1st Qu.:0.9205  \n Median :0.9576   Median :0.9597   Median :0.9607   Median :0.9605  \n Mean   :0.9642   Mean   :0.9666   Mean   :0.9676   Mean   :0.9673  \n 3rd Qu.:1.0006   3rd Qu.:1.0031   3rd Qu.:1.0040   3rd Qu.:1.0036  \n Max.   :1.1426   Max.   :1.1451   Max.   :1.1461   Max.   :1.1458  \n      333              334              335              336        \n Min.   :0.8401   Min.   :0.8369   Min.   :0.8329   Min.   :0.8285  \n 1st Qu.:0.9190   1st Qu.:0.9164   1st Qu.:0.9130   1st Qu.:0.9092  \n Median :0.9591   Median :0.9567   Median :0.9531   Median :0.9495  \n Mean   :0.9657   Mean   :0.9632   Mean   :0.9598   Mean   :0.9562  \n 3rd Qu.:1.0019   3rd Qu.:0.9991   3rd Qu.:0.9955   3rd Qu.:0.9915  \n Max.   :1.1443   Max.   :1.1417   Max.   :1.1385   Max.   :1.1351  \n      337              338              339              340        \n Min.   :0.8242   Min.   :0.8200   Min.   :0.8162   Min.   :0.8124  \n 1st Qu.:0.9052   1st Qu.:0.9013   1st Qu.:0.8977   1st Qu.:0.8941  \n Median :0.9460   Median :0.9428   Median :0.9397   Median :0.9366  \n Mean   :0.9525   Mean   :0.9488   Mean   :0.9453   Mean   :0.9418  \n 3rd Qu.:0.9873   3rd Qu.:0.9834   3rd Qu.:0.9795   3rd Qu.:0.9757  \n Max.   :1.1317   Max.   :1.1282   Max.   :1.1248   Max.   :1.1215  \n      341              342              343              344        \n Min.   :0.8087   Min.   :0.8052   Min.   :0.8018   Min.   :0.7988  \n 1st Qu.:0.8905   1st Qu.:0.8871   1st Qu.:0.8837   1st Qu.:0.8804  \n Median :0.9334   Median :0.9301   Median :0.9269   Median :0.9237  \n Mean   :0.9384   Mean   :0.9351   Mean   :0.9318   Mean   :0.9285  \n 3rd Qu.:0.9721   3rd Qu.:0.9685   3rd Qu.:0.9650   3rd Qu.:0.9616  \n Max.   :1.1182   Max.   :1.1151   Max.   :1.1120   Max.   :1.1090  \n      345              346              347              348        \n Min.   :0.7960   Min.   :0.7932   Min.   :0.7903   Min.   :0.7873  \n 1st Qu.:0.8771   1st Qu.:0.8739   1st Qu.:0.8708   1st Qu.:0.8676  \n Median :0.9203   Median :0.9171   Median :0.9138   Median :0.9104  \n Mean   :0.9253   Mean   :0.9220   Mean   :0.9187   Mean   :0.9154  \n 3rd Qu.:0.9580   3rd Qu.:0.9549   3rd Qu.:0.9517   3rd Qu.:0.9484  \n Max.   :1.1059   Max.   :1.1027   Max.   :1.0996   Max.   :1.0963  \n      349              350              351              352        \n Min.   :0.7841   Min.   :0.7810   Min.   :0.7778   Min.   :0.7750  \n 1st Qu.:0.8644   1st Qu.:0.8609   1st Qu.:0.8577   1st Qu.:0.8545  \n Median :0.9070   Median :0.9036   Median :0.9002   Median :0.8970  \n Mean   :0.9120   Mean   :0.9086   Mean   :0.9053   Mean   :0.9021  \n 3rd Qu.:0.9449   3rd Qu.:0.9413   3rd Qu.:0.9378   3rd Qu.:0.9344  \n Max.   :1.0931   Max.   :1.0899   Max.   :1.0867   Max.   :1.0837  \n      353              354              355              356        \n Min.   :0.7722   Min.   :0.7696   Min.   :0.7673   Min.   :0.7650  \n 1st Qu.:0.8515   1st Qu.:0.8486   1st Qu.:0.8459   1st Qu.:0.8435  \n Median :0.8939   Median :0.8910   Median :0.8883   Median :0.8857  \n Mean   :0.8990   Mean   :0.8961   Mean   :0.8934   Mean   :0.8909  \n 3rd Qu.:0.9313   3rd Qu.:0.9283   3rd Qu.:0.9257   3rd Qu.:0.9232  \n Max.   :1.0808   Max.   :1.0778   Max.   :1.0752   Max.   :1.0726  \n      357              358              359              360        \n Min.   :0.7629   Min.   :0.7609   Min.   :0.7590   Min.   :0.7569  \n 1st Qu.:0.8410   1st Qu.:0.8388   1st Qu.:0.8365   1st Qu.:0.8340  \n Median :0.8832   Median :0.8807   Median :0.8782   Median :0.8755  \n Mean   :0.8885   Mean   :0.8862   Mean   :0.8838   Mean   :0.8813  \n 3rd Qu.:0.9207   3rd Qu.:0.9183   3rd Qu.:0.9159   3rd Qu.:0.9135  \n Max.   :1.0703   Max.   :1.0678   Max.   :1.0655   Max.   :1.0628  \n      361              362              363              364        \n Min.   :0.7548   Min.   :0.7522   Min.   :0.7493   Min.   :0.7462  \n 1st Qu.:0.8314   1st Qu.:0.8287   1st Qu.:0.8257   1st Qu.:0.8226  \n Median :0.8728   Median :0.8700   Median :0.8671   Median :0.8640  \n Mean   :0.8787   Mean   :0.8759   Mean   :0.8729   Mean   :0.8697  \n 3rd Qu.:0.9111   3rd Qu.:0.9083   3rd Qu.:0.9054   3rd Qu.:0.9022  \n Max.   :1.0601   Max.   :1.0573   Max.   :1.0546   Max.   :1.0517  \n      365              366              367              368        \n Min.   :0.7430   Min.   :0.7397   Min.   :0.7366   Min.   :0.7335  \n 1st Qu.:0.8195   1st Qu.:0.8163   1st Qu.:0.8130   1st Qu.:0.8098  \n Median :0.8610   Median :0.8578   Median :0.8546   Median :0.8516  \n Mean   :0.8665   Mean   :0.8632   Mean   :0.8600   Mean   :0.8568  \n 3rd Qu.:0.8989   3rd Qu.:0.8955   3rd Qu.:0.8921   3rd Qu.:0.8889  \n Max.   :1.0488   Max.   :1.0459   Max.   :1.0430   Max.   :1.0402  \n      369              370              371              372        \n Min.   :0.7304   Min.   :0.7274   Min.   :0.7246   Min.   :0.7221  \n 1st Qu.:0.8068   1st Qu.:0.8039   1st Qu.:0.8011   1st Qu.:0.7988  \n Median :0.8486   Median :0.8458   Median :0.8432   Median :0.8409  \n Mean   :0.8537   Mean   :0.8508   Mean   :0.8481   Mean   :0.8458  \n 3rd Qu.:0.8858   3rd Qu.:0.8829   3rd Qu.:0.8802   3rd Qu.:0.8779  \n Max.   :1.0373   Max.   :1.0348   Max.   :1.0325   Max.   :1.0308  \n      373              374              375              376        \n Min.   :0.7200   Min.   :0.7185   Min.   :0.7174   Min.   :0.7169  \n 1st Qu.:0.7967   1st Qu.:0.7952   1st Qu.:0.7942   1st Qu.:0.7938  \n Median :0.8390   Median :0.8377   Median :0.8369   Median :0.8368  \n Mean   :0.8439   Mean   :0.8425   Mean   :0.8417   Mean   :0.8416  \n 3rd Qu.:0.8761   3rd Qu.:0.8748   3rd Qu.:0.8741   3rd Qu.:0.8743  \n Max.   :1.0296   Max.   :1.0288   Max.   :1.0286   Max.   :1.0291  \n      377              378              379              380        \n Min.   :0.7170   Min.   :0.7177   Min.   :0.7192   Min.   :0.7213  \n 1st Qu.:0.7944   1st Qu.:0.7956   1st Qu.:0.7977   1st Qu.:0.8005  \n Median :0.8376   Median :0.8391   Median :0.8414   Median :0.8446  \n Mean   :0.8423   Mean   :0.8439   Mean   :0.8462   Mean   :0.8493  \n 3rd Qu.:0.8753   3rd Qu.:0.8770   3rd Qu.:0.8797   3rd Qu.:0.8831  \n Max.   :1.0308   Max.   :1.0333   Max.   :1.0367   Max.   :1.0412  \n      381              382              383              384        \n Min.   :0.7246   Min.   :0.7288   Min.   :0.7336   Min.   :0.7399  \n 1st Qu.:0.8043   1st Qu.:0.8092   1st Qu.:0.8155   1st Qu.:0.8233  \n Median :0.8487   Median :0.8540   Median :0.8605   Median :0.8686  \n Mean   :0.8535   Mean   :0.8588   Mean   :0.8654   Mean   :0.8735  \n 3rd Qu.:0.8875   3rd Qu.:0.8929   3rd Qu.:0.8996   3rd Qu.:0.9079  \n Max.   :1.0469   Max.   :1.0536   Max.   :1.0620   Max.   :1.0723  \n      385              386              387              388        \n Min.   :0.7477   Min.   :0.7569   Min.   :0.7673   Min.   :0.7793  \n 1st Qu.:0.8327   1st Qu.:0.8438   1st Qu.:0.8561   1st Qu.:0.8700  \n Median :0.8787   Median :0.8904   Median :0.9038   Median :0.9191  \n Mean   :0.8835   Mean   :0.8951   Mean   :0.9085   Mean   :0.9238  \n 3rd Qu.:0.9185   3rd Qu.:0.9313   3rd Qu.:0.9458   3rd Qu.:0.9622  \n Max.   :1.0848   Max.   :1.0993   Max.   :1.1157   Max.   :1.1335  \n      389              390              391              392        \n Min.   :0.7933   Min.   :0.8089   Min.   :0.8263   Min.   :0.8460  \n 1st Qu.:0.8859   1st Qu.:0.9045   1st Qu.:0.9249   1st Qu.:0.9479  \n Median :0.9364   Median :0.9556   Median :0.9768   Median :1.0005  \n Mean   :0.9411   Mean   :0.9604   Mean   :0.9817   Mean   :1.0054  \n 3rd Qu.:0.9807   3rd Qu.:1.0012   3rd Qu.:1.0239   3rd Qu.:1.0491  \n Max.   :1.1533   Max.   :1.1748   Max.   :1.1983   Max.   :1.2230  \n      393              394              395              396        \n Min.   :0.8673   Min.   :0.8904   Min.   :0.9152   Min.   :0.9412  \n 1st Qu.:0.9730   1st Qu.:1.0000   1st Qu.:1.0282   1st Qu.:1.0581  \n Median :1.0264   Median :1.0540   Median :1.0832   Median :1.1136  \n Mean   :1.0313   Mean   :1.0589   Mean   :1.0878   Mean   :1.1182  \n 3rd Qu.:1.0766   3rd Qu.:1.1059   3rd Qu.:1.1366   3rd Qu.:1.1683  \n Max.   :1.2496   Max.   :1.2769   Max.   :1.3043   Max.   :1.3338  \n      397              398              399             400       \n Min.   :0.9688   Min.   :0.9972   Min.   :1.027   Min.   :1.057  \n 1st Qu.:1.0893   1st Qu.:1.1216   1st Qu.:1.155   1st Qu.:1.188  \n Median :1.1453   Median :1.1779   Median :1.211   Median :1.243  \n Mean   :1.1498   Mean   :1.1822   Mean   :1.215   Mean   :1.248  \n 3rd Qu.:1.2007   3rd Qu.:1.2344   3rd Qu.:1.269   3rd Qu.:1.304  \n Max.   :1.3673   Max.   :1.4011   Max.   :1.433   Max.   :1.463  \n      401             402             403             404       \n Min.   :1.086   Min.   :1.113   Min.   :1.139   Min.   :1.163  \n 1st Qu.:1.220   1st Qu.:1.250   1st Qu.:1.278   1st Qu.:1.304  \n Median :1.275   Median :1.306   Median :1.334   Median :1.360  \n Mean   :1.280   Mean   :1.310   Mean   :1.338   Mean   :1.364  \n 3rd Qu.:1.338   3rd Qu.:1.370   3rd Qu.:1.399   3rd Qu.:1.425  \n Max.   :1.493   Max.   :1.520   Max.   :1.545   Max.   :1.566  \n      405             406             407             408       \n Min.   :1.186   Min.   :1.207   Min.   :1.227   Min.   :1.245  \n 1st Qu.:1.328   1st Qu.:1.350   1st Qu.:1.370   1st Qu.:1.389  \n Median :1.385   Median :1.406   Median :1.425   Median :1.442  \n Mean   :1.387   Mean   :1.409   Mean   :1.429   Mean   :1.447  \n 3rd Qu.:1.446   3rd Qu.:1.469   3rd Qu.:1.490   3rd Qu.:1.508  \n Max.   :1.585   Max.   :1.601   Max.   :1.616   Max.   :1.629  \n      409             410             411             412       \n Min.   :1.261   Min.   :1.276   Min.   :1.288   Min.   :1.299  \n 1st Qu.:1.405   1st Qu.:1.420   1st Qu.:1.433   1st Qu.:1.443  \n Median :1.458   Median :1.471   Median :1.482   Median :1.493  \n Mean   :1.462   Mean   :1.476   Mean   :1.488   Mean   :1.498  \n 3rd Qu.:1.525   3rd Qu.:1.539   3rd Qu.:1.550   3rd Qu.:1.560  \n Max.   :1.640   Max.   :1.650   Max.   :1.660   Max.   :1.669  \n      413             414             415             416       \n Min.   :1.308   Min.   :1.315   Min.   :1.319   Min.   :1.323  \n 1st Qu.:1.453   1st Qu.:1.460   1st Qu.:1.463   1st Qu.:1.466  \n Median :1.502   Median :1.509   Median :1.514   Median :1.517  \n Mean   :1.506   Mean   :1.513   Mean   :1.517   Mean   :1.520  \n 3rd Qu.:1.568   3rd Qu.:1.574   3rd Qu.:1.578   3rd Qu.:1.580  \n Max.   :1.677   Max.   :1.682   Max.   :1.686   Max.   :1.688  \n      417             418             419             420       \n Min.   :1.325   Min.   :1.325   Min.   :1.323   Min.   :1.321  \n 1st Qu.:1.468   1st Qu.:1.469   1st Qu.:1.469   1st Qu.:1.467  \n Median :1.519   Median :1.520   Median :1.519   Median :1.517  \n Mean   :1.522   Mean   :1.522   Mean   :1.521   Mean   :1.519  \n 3rd Qu.:1.581   3rd Qu.:1.582   3rd Qu.:1.581   3rd Qu.:1.578  \n Max.   :1.689   Max.   :1.689   Max.   :1.688   Max.   :1.686  \n      421             422             423             424       \n Min.   :1.318   Min.   :1.313   Min.   :1.308   Min.   :1.303  \n 1st Qu.:1.465   1st Qu.:1.462   1st Qu.:1.458   1st Qu.:1.453  \n Median :1.515   Median :1.511   Median :1.507   Median :1.502  \n Mean   :1.516   Mean   :1.513   Mean   :1.508   Mean   :1.503  \n 3rd Qu.:1.575   3rd Qu.:1.571   3rd Qu.:1.567   3rd Qu.:1.561  \n Max.   :1.683   Max.   :1.680   Max.   :1.675   Max.   :1.670  \n      425             426             427             428       \n Min.   :1.297   Min.   :1.291   Min.   :1.284   Min.   :1.277  \n 1st Qu.:1.448   1st Qu.:1.441   1st Qu.:1.434   1st Qu.:1.427  \n Median :1.496   Median :1.490   Median :1.483   Median :1.477  \n Mean   :1.498   Mean   :1.491   Mean   :1.485   Mean   :1.478  \n 3rd Qu.:1.555   3rd Qu.:1.549   3rd Qu.:1.542   3rd Qu.:1.535  \n Max.   :1.665   Max.   :1.659   Max.   :1.653   Max.   :1.648  \n      429             430             431             432       \n Min.   :1.270   Min.   :1.263   Min.   :1.256   Min.   :1.249  \n 1st Qu.:1.420   1st Qu.:1.413   1st Qu.:1.406   1st Qu.:1.398  \n Median :1.470   Median :1.463   Median :1.456   Median :1.449  \n Mean   :1.472   Mean   :1.465   Mean   :1.458   Mean   :1.451  \n 3rd Qu.:1.528   3rd Qu.:1.521   3rd Qu.:1.513   3rd Qu.:1.506  \n Max.   :1.642   Max.   :1.637   Max.   :1.631   Max.   :1.625  \n      433             434             435             436       \n Min.   :1.242   Min.   :1.235   Min.   :1.228   Min.   :1.220  \n 1st Qu.:1.390   1st Qu.:1.383   1st Qu.:1.375   1st Qu.:1.367  \n Median :1.443   Median :1.436   Median :1.430   Median :1.423  \n Mean   :1.444   Mean   :1.437   Mean   :1.430   Mean   :1.422  \n 3rd Qu.:1.499   3rd Qu.:1.491   3rd Qu.:1.483   3rd Qu.:1.474  \n Max.   :1.618   Max.   :1.612   Max.   :1.606   Max.   :1.600  \n      437             438             439             440       \n Min.   :1.213   Min.   :1.205   Min.   :1.198   Min.   :1.191  \n 1st Qu.:1.359   1st Qu.:1.351   1st Qu.:1.343   1st Qu.:1.334  \n Median :1.415   Median :1.407   Median :1.400   Median :1.392  \n Mean   :1.414   Mean   :1.407   Mean   :1.399   Mean   :1.391  \n 3rd Qu.:1.466   3rd Qu.:1.457   3rd Qu.:1.449   3rd Qu.:1.440  \n Max.   :1.593   Max.   :1.587   Max.   :1.581   Max.   :1.574  \n      441             442             443             444       \n Min.   :1.184   Min.   :1.177   Min.   :1.170   Min.   :1.163  \n 1st Qu.:1.327   1st Qu.:1.319   1st Qu.:1.311   1st Qu.:1.303  \n Median :1.384   Median :1.377   Median :1.369   Median :1.361  \n Mean   :1.384   Mean   :1.376   Mean   :1.368   Mean   :1.361  \n 3rd Qu.:1.433   3rd Qu.:1.426   3rd Qu.:1.419   3rd Qu.:1.411  \n Max.   :1.568   Max.   :1.561   Max.   :1.555   Max.   :1.548  \n      445             446             447             448       \n Min.   :1.156   Min.   :1.150   Min.   :1.144   Min.   :1.138  \n 1st Qu.:1.295   1st Qu.:1.288   1st Qu.:1.281   1st Qu.:1.274  \n Median :1.354   Median :1.347   Median :1.340   Median :1.334  \n Mean   :1.354   Mean   :1.346   Mean   :1.339   Mean   :1.333  \n 3rd Qu.:1.403   3rd Qu.:1.395   3rd Qu.:1.388   3rd Qu.:1.381  \n Max.   :1.542   Max.   :1.535   Max.   :1.529   Max.   :1.522  \n      449             450             451             452       \n Min.   :1.133   Min.   :1.129   Min.   :1.125   Min.   :1.122  \n 1st Qu.:1.268   1st Qu.:1.262   1st Qu.:1.257   1st Qu.:1.252  \n Median :1.329   Median :1.324   Median :1.320   Median :1.316  \n Mean   :1.327   Mean   :1.322   Mean   :1.317   Mean   :1.312  \n 3rd Qu.:1.375   3rd Qu.:1.369   3rd Qu.:1.364   3rd Qu.:1.360  \n Max.   :1.516   Max.   :1.511   Max.   :1.505   Max.   :1.500  \n      453             454             455             456       \n Min.   :1.119   Min.   :1.117   Min.   :1.115   Min.   :1.114  \n 1st Qu.:1.248   1st Qu.:1.244   1st Qu.:1.241   1st Qu.:1.238  \n Median :1.312   Median :1.308   Median :1.305   Median :1.302  \n Mean   :1.309   Mean   :1.305   Mean   :1.302   Mean   :1.300  \n 3rd Qu.:1.357   3rd Qu.:1.354   3rd Qu.:1.352   3rd Qu.:1.350  \n Max.   :1.496   Max.   :1.492   Max.   :1.488   Max.   :1.485  \n      457             458             459             460       \n Min.   :1.112   Min.   :1.111   Min.   :1.110   Min.   :1.110  \n 1st Qu.:1.236   1st Qu.:1.234   1st Qu.:1.233   1st Qu.:1.233  \n Median :1.300   Median :1.298   Median :1.296   Median :1.296  \n Mean   :1.298   Mean   :1.296   Mean   :1.295   Mean   :1.294  \n 3rd Qu.:1.349   3rd Qu.:1.347   3rd Qu.:1.346   3rd Qu.:1.346  \n Max.   :1.483   Max.   :1.481   Max.   :1.480   Max.   :1.479  \n      461             462             463             464       \n Min.   :1.110   Min.   :1.112   Min.   :1.114   Min.   :1.117  \n 1st Qu.:1.233   1st Qu.:1.234   1st Qu.:1.236   1st Qu.:1.238  \n Median :1.296   Median :1.296   Median :1.298   Median :1.300  \n Mean   :1.295   Mean   :1.296   Mean   :1.297   Mean   :1.300  \n 3rd Qu.:1.346   3rd Qu.:1.347   3rd Qu.:1.349   3rd Qu.:1.351  \n Max.   :1.478   Max.   :1.479   Max.   :1.479   Max.   :1.480  \n      465             466             467             468       \n Min.   :1.120   Min.   :1.125   Min.   :1.130   Min.   :1.135  \n 1st Qu.:1.241   1st Qu.:1.244   1st Qu.:1.248   1st Qu.:1.252  \n Median :1.302   Median :1.305   Median :1.308   Median :1.313  \n Mean   :1.302   Mean   :1.306   Mean   :1.310   Mean   :1.314  \n 3rd Qu.:1.354   3rd Qu.:1.358   3rd Qu.:1.362   3rd Qu.:1.366  \n Max.   :1.482   Max.   :1.484   Max.   :1.486   Max.   :1.489  \n      469             470             471             472       \n Min.   :1.142   Min.   :1.149   Min.   :1.157   Min.   :1.165  \n 1st Qu.:1.257   1st Qu.:1.262   1st Qu.:1.267   1st Qu.:1.273  \n Median :1.318   Median :1.323   Median :1.328   Median :1.333  \n Mean   :1.319   Mean   :1.325   Mean   :1.330   Mean   :1.336  \n 3rd Qu.:1.369   3rd Qu.:1.373   3rd Qu.:1.377   3rd Qu.:1.381  \n Max.   :1.492   Max.   :1.495   Max.   :1.498   Max.   :1.501  \n      473             474             475             476       \n Min.   :1.173   Min.   :1.180   Min.   :1.188   Min.   :1.195  \n 1st Qu.:1.280   1st Qu.:1.289   1st Qu.:1.296   1st Qu.:1.302  \n Median :1.340   Median :1.346   Median :1.353   Median :1.359  \n Mean   :1.342   Mean   :1.348   Mean   :1.354   Mean   :1.359  \n 3rd Qu.:1.386   3rd Qu.:1.390   3rd Qu.:1.395   3rd Qu.:1.400  \n Max.   :1.505   Max.   :1.508   Max.   :1.511   Max.   :1.514  \n      477             478             479             480       \n Min.   :1.202   Min.   :1.208   Min.   :1.214   Min.   :1.220  \n 1st Qu.:1.309   1st Qu.:1.316   1st Qu.:1.322   1st Qu.:1.328  \n Median :1.364   Median :1.368   Median :1.371   Median :1.375  \n Mean   :1.364   Mean   :1.368   Mean   :1.373   Mean   :1.377  \n 3rd Qu.:1.405   3rd Qu.:1.410   3rd Qu.:1.414   3rd Qu.:1.418  \n Max.   :1.516   Max.   :1.518   Max.   :1.520   Max.   :1.522  \n      481             482             483             484       \n Min.   :1.225   Min.   :1.231   Min.   :1.235   Min.   :1.240  \n 1st Qu.:1.334   1st Qu.:1.340   1st Qu.:1.346   1st Qu.:1.351  \n Median :1.378   Median :1.381   Median :1.384   Median :1.387  \n Mean   :1.381   Mean   :1.385   Mean   :1.388   Mean   :1.392  \n 3rd Qu.:1.422   3rd Qu.:1.426   3rd Qu.:1.430   3rd Qu.:1.433  \n Max.   :1.524   Max.   :1.526   Max.   :1.528   Max.   :1.531  \n      485             486             487             488       \n Min.   :1.245   Min.   :1.249   Min.   :1.252   Min.   :1.254  \n 1st Qu.:1.354   1st Qu.:1.357   1st Qu.:1.360   1st Qu.:1.362  \n Median :1.390   Median :1.392   Median :1.395   Median :1.397  \n Mean   :1.395   Mean   :1.398   Mean   :1.400   Mean   :1.402  \n 3rd Qu.:1.436   3rd Qu.:1.439   3rd Qu.:1.442   3rd Qu.:1.444  \n Max.   :1.534   Max.   :1.536   Max.   :1.538   Max.   :1.541  \n      489             490             491             492       \n Min.   :1.256   Min.   :1.258   Min.   :1.259   Min.   :1.259  \n 1st Qu.:1.365   1st Qu.:1.367   1st Qu.:1.368   1st Qu.:1.369  \n Median :1.400   Median :1.402   Median :1.403   Median :1.405  \n Mean   :1.404   Mean   :1.406   Mean   :1.407   Mean   :1.408  \n 3rd Qu.:1.447   3rd Qu.:1.448   3rd Qu.:1.449   3rd Qu.:1.450  \n Max.   :1.543   Max.   :1.546   Max.   :1.547   Max.   :1.549  \n      493             494             495             496       \n Min.   :1.258   Min.   :1.258   Min.   :1.257   Min.   :1.255  \n 1st Qu.:1.369   1st Qu.:1.369   1st Qu.:1.368   1st Qu.:1.366  \n Median :1.406   Median :1.405   Median :1.404   Median :1.403  \n Mean   :1.409   Mean   :1.409   Mean   :1.409   Mean   :1.408  \n 3rd Qu.:1.451   3rd Qu.:1.451   3rd Qu.:1.451   3rd Qu.:1.450  \n Max.   :1.550   Max.   :1.551   Max.   :1.551   Max.   :1.550  \n      497             498             499             500       \n Min.   :1.253   Min.   :1.251   Min.   :1.248   Min.   :1.246  \n 1st Qu.:1.364   1st Qu.:1.362   1st Qu.:1.359   1st Qu.:1.357  \n Median :1.402   Median :1.400   Median :1.399   Median :1.398  \n Mean   :1.406   Mean   :1.404   Mean   :1.403   Mean   :1.401  \n 3rd Qu.:1.449   3rd Qu.:1.448   3rd Qu.:1.446   3rd Qu.:1.445  \n Max.   :1.550   Max.   :1.548   Max.   :1.547   Max.   :1.547  \n      501             502             503             504       \n Min.   :1.244   Min.   :1.242   Min.   :1.240   Min.   :1.238  \n 1st Qu.:1.354   1st Qu.:1.351   1st Qu.:1.349   1st Qu.:1.347  \n Median :1.397   Median :1.396   Median :1.395   Median :1.393  \n Mean   :1.400   Mean   :1.399   Mean   :1.398   Mean   :1.397  \n 3rd Qu.:1.445   3rd Qu.:1.444   3rd Qu.:1.443   3rd Qu.:1.443  \n Max.   :1.546   Max.   :1.547   Max.   :1.547   Max.   :1.546  \n      505             506             507             508       \n Min.   :1.236   Min.   :1.233   Min.   :1.231   Min.   :1.228  \n 1st Qu.:1.345   1st Qu.:1.342   1st Qu.:1.340   1st Qu.:1.338  \n Median :1.391   Median :1.389   Median :1.388   Median :1.385  \n Mean   :1.395   Mean   :1.393   Mean   :1.392   Mean   :1.389  \n 3rd Qu.:1.442   3rd Qu.:1.440   3rd Qu.:1.439   3rd Qu.:1.436  \n Max.   :1.546   Max.   :1.544   Max.   :1.543   Max.   :1.541  \n      509             510             511             512       \n Min.   :1.225   Min.   :1.222   Min.   :1.218   Min.   :1.213  \n 1st Qu.:1.335   1st Qu.:1.332   1st Qu.:1.327   1st Qu.:1.323  \n Median :1.382   Median :1.378   Median :1.374   Median :1.368  \n Mean   :1.386   Mean   :1.383   Mean   :1.378   Mean   :1.372  \n 3rd Qu.:1.433   3rd Qu.:1.429   3rd Qu.:1.425   3rd Qu.:1.419  \n Max.   :1.538   Max.   :1.534   Max.   :1.529   Max.   :1.523  \n      513             514             515             516       \n Min.   :1.208   Min.   :1.204   Min.   :1.200   Min.   :1.196  \n 1st Qu.:1.317   1st Qu.:1.312   1st Qu.:1.308   1st Qu.:1.305  \n Median :1.363   Median :1.358   Median :1.354   Median :1.351  \n Mean   :1.367   Mean   :1.362   Mean   :1.358   Mean   :1.355  \n 3rd Qu.:1.413   3rd Qu.:1.408   3rd Qu.:1.404   3rd Qu.:1.401  \n Max.   :1.516   Max.   :1.510   Max.   :1.507   Max.   :1.503  \n      517             518             519             520       \n Min.   :1.193   Min.   :1.190   Min.   :1.187   Min.   :1.185  \n 1st Qu.:1.302   1st Qu.:1.299   1st Qu.:1.295   1st Qu.:1.292  \n Median :1.348   Median :1.346   Median :1.342   Median :1.340  \n Mean   :1.352   Mean   :1.349   Mean   :1.346   Mean   :1.343  \n 3rd Qu.:1.398   3rd Qu.:1.395   3rd Qu.:1.392   3rd Qu.:1.388  \n Max.   :1.501   Max.   :1.498   Max.   :1.494   Max.   :1.491  \n      521             522             523             524       \n Min.   :1.183   Min.   :1.181   Min.   :1.180   Min.   :1.179  \n 1st Qu.:1.289   1st Qu.:1.287   1st Qu.:1.285   1st Qu.:1.283  \n Median :1.338   Median :1.336   Median :1.336   Median :1.334  \n Mean   :1.341   Mean   :1.339   Mean   :1.338   Mean   :1.337  \n 3rd Qu.:1.386   3rd Qu.:1.384   3rd Qu.:1.383   3rd Qu.:1.382  \n Max.   :1.489   Max.   :1.486   Max.   :1.485   Max.   :1.484  \n      525             526             527             528       \n Min.   :1.176   Min.   :1.172   Min.   :1.168   Min.   :1.163  \n 1st Qu.:1.281   1st Qu.:1.278   1st Qu.:1.274   1st Qu.:1.270  \n Median :1.332   Median :1.329   Median :1.324   Median :1.319  \n Mean   :1.335   Mean   :1.332   Mean   :1.328   Mean   :1.323  \n 3rd Qu.:1.380   3rd Qu.:1.377   3rd Qu.:1.373   3rd Qu.:1.369  \n Max.   :1.482   Max.   :1.480   Max.   :1.477   Max.   :1.473  \n      529             530             531             532       \n Min.   :1.159   Min.   :1.155   Min.   :1.152   Min.   :1.148  \n 1st Qu.:1.266   1st Qu.:1.263   1st Qu.:1.260   1st Qu.:1.257  \n Median :1.315   Median :1.312   Median :1.309   Median :1.306  \n Mean   :1.319   Mean   :1.316   Mean   :1.314   Mean   :1.311  \n 3rd Qu.:1.365   3rd Qu.:1.362   3rd Qu.:1.360   3rd Qu.:1.357  \n Max.   :1.469   Max.   :1.467   Max.   :1.465   Max.   :1.463  \n      533             534             535             536       \n Min.   :1.145   Min.   :1.141   Min.   :1.137   Min.   :1.133  \n 1st Qu.:1.254   1st Qu.:1.250   1st Qu.:1.247   1st Qu.:1.242  \n Median :1.303   Median :1.300   Median :1.297   Median :1.293  \n Mean   :1.308   Mean   :1.305   Mean   :1.301   Mean   :1.297  \n 3rd Qu.:1.354   3rd Qu.:1.350   3rd Qu.:1.347   3rd Qu.:1.343  \n Max.   :1.461   Max.   :1.458   Max.   :1.455   Max.   :1.452  \n      537             538             539             540       \n Min.   :1.129   Min.   :1.125   Min.   :1.122   Min.   :1.119  \n 1st Qu.:1.238   1st Qu.:1.234   1st Qu.:1.230   1st Qu.:1.226  \n Median :1.289   Median :1.285   Median :1.281   Median :1.277  \n Mean   :1.293   Mean   :1.289   Mean   :1.285   Mean   :1.282  \n 3rd Qu.:1.338   3rd Qu.:1.334   3rd Qu.:1.330   3rd Qu.:1.327  \n Max.   :1.448   Max.   :1.443   Max.   :1.439   Max.   :1.436  \n      541             542             543             544       \n Min.   :1.117   Min.   :1.116   Min.   :1.115   Min.   :1.114  \n 1st Qu.:1.224   1st Qu.:1.222   1st Qu.:1.222   1st Qu.:1.221  \n Median :1.275   Median :1.274   Median :1.273   Median :1.272  \n Mean   :1.280   Mean   :1.279   Mean   :1.279   Mean   :1.278  \n 3rd Qu.:1.325   3rd Qu.:1.325   3rd Qu.:1.326   3rd Qu.:1.325  \n Max.   :1.435   Max.   :1.435   Max.   :1.435   Max.   :1.435  \n      545             546             547             548       \n Min.   :1.112   Min.   :1.109   Min.   :1.107   Min.   :1.103  \n 1st Qu.:1.220   1st Qu.:1.218   1st Qu.:1.215   1st Qu.:1.211  \n Median :1.271   Median :1.269   Median :1.267   Median :1.264  \n Mean   :1.277   Mean   :1.275   Mean   :1.272   Mean   :1.269  \n 3rd Qu.:1.325   3rd Qu.:1.323   3rd Qu.:1.321   3rd Qu.:1.319  \n Max.   :1.435   Max.   :1.434   Max.   :1.432   Max.   :1.429  \n      549             550             551             552       \n Min.   :1.099   Min.   :1.095   Min.   :1.091   Min.   :1.088  \n 1st Qu.:1.208   1st Qu.:1.204   1st Qu.:1.200   1st Qu.:1.196  \n Median :1.260   Median :1.256   Median :1.251   Median :1.247  \n Mean   :1.265   Mean   :1.261   Mean   :1.257   Mean   :1.253  \n 3rd Qu.:1.315   3rd Qu.:1.312   3rd Qu.:1.308   3rd Qu.:1.304  \n Max.   :1.425   Max.   :1.421   Max.   :1.417   Max.   :1.412  \n      553             554             555             556       \n Min.   :1.085   Min.   :1.083   Min.   :1.082   Min.   :1.081  \n 1st Qu.:1.193   1st Qu.:1.191   1st Qu.:1.189   1st Qu.:1.188  \n Median :1.244   Median :1.241   Median :1.239   Median :1.239  \n Mean   :1.249   Mean   :1.247   Mean   :1.246   Mean   :1.245  \n 3rd Qu.:1.301   3rd Qu.:1.300   3rd Qu.:1.299   3rd Qu.:1.298  \n Max.   :1.409   Max.   :1.406   Max.   :1.405   Max.   :1.405  \n      557             558             559             560       \n Min.   :1.080   Min.   :1.079   Min.   :1.079   Min.   :1.078  \n 1st Qu.:1.187   1st Qu.:1.187   1st Qu.:1.186   1st Qu.:1.185  \n Median :1.238   Median :1.237   Median :1.236   Median :1.235  \n Mean   :1.244   Mean   :1.244   Mean   :1.243   Mean   :1.242  \n 3rd Qu.:1.298   3rd Qu.:1.297   3rd Qu.:1.296   3rd Qu.:1.296  \n Max.   :1.404   Max.   :1.404   Max.   :1.403   Max.   :1.402  \n      561             562             563             564       \n Min.   :1.078   Min.   :1.079   Min.   :1.079   Min.   :1.079  \n 1st Qu.:1.185   1st Qu.:1.184   1st Qu.:1.185   1st Qu.:1.186  \n Median :1.234   Median :1.233   Median :1.233   Median :1.234  \n Mean   :1.241   Mean   :1.241   Mean   :1.241   Mean   :1.242  \n 3rd Qu.:1.295   3rd Qu.:1.294   3rd Qu.:1.294   3rd Qu.:1.294  \n Max.   :1.401   Max.   :1.401   Max.   :1.401   Max.   :1.401  \n      565             566             567             568       \n Min.   :1.079   Min.   :1.082   Min.   :1.086   Min.   :1.093  \n 1st Qu.:1.188   1st Qu.:1.192   1st Qu.:1.196   1st Qu.:1.202  \n Median :1.236   Median :1.240   Median :1.245   Median :1.251  \n Mean   :1.244   Mean   :1.247   Mean   :1.251   Mean   :1.257  \n 3rd Qu.:1.296   3rd Qu.:1.298   3rd Qu.:1.302   3rd Qu.:1.307  \n Max.   :1.403   Max.   :1.405   Max.   :1.409   Max.   :1.414  \n      569             570             571             572       \n Min.   :1.101   Min.   :1.110   Min.   :1.120   Min.   :1.131  \n 1st Qu.:1.210   1st Qu.:1.219   1st Qu.:1.229   1st Qu.:1.242  \n Median :1.258   Median :1.266   Median :1.275   Median :1.285  \n Mean   :1.264   Mean   :1.272   Mean   :1.281   Mean   :1.292  \n 3rd Qu.:1.313   3rd Qu.:1.320   3rd Qu.:1.330   3rd Qu.:1.341  \n Max.   :1.420   Max.   :1.428   Max.   :1.437   Max.   :1.446  \n      573             574             575             576       \n Min.   :1.143   Min.   :1.155   Min.   :1.167   Min.   :1.181  \n 1st Qu.:1.254   1st Qu.:1.267   1st Qu.:1.280   1st Qu.:1.294  \n Median :1.295   Median :1.306   Median :1.317   Median :1.329  \n Mean   :1.303   Mean   :1.315   Mean   :1.327   Mean   :1.339  \n 3rd Qu.:1.352   3rd Qu.:1.363   3rd Qu.:1.375   3rd Qu.:1.386  \n Max.   :1.458   Max.   :1.470   Max.   :1.483   Max.   :1.497  \n      577             578             579             580       \n Min.   :1.193   Min.   :1.206   Min.   :1.218   Min.   :1.231  \n 1st Qu.:1.307   1st Qu.:1.320   1st Qu.:1.334   1st Qu.:1.347  \n Median :1.342   Median :1.354   Median :1.367   Median :1.380  \n Mean   :1.353   Mean   :1.366   Mean   :1.379   Mean   :1.392  \n 3rd Qu.:1.398   3rd Qu.:1.410   3rd Qu.:1.423   3rd Qu.:1.438  \n Max.   :1.511   Max.   :1.525   Max.   :1.539   Max.   :1.552  \n      581             582             583             584       \n Min.   :1.244   Min.   :1.256   Min.   :1.269   Min.   :1.281  \n 1st Qu.:1.359   1st Qu.:1.372   1st Qu.:1.385   1st Qu.:1.397  \n Median :1.393   Median :1.406   Median :1.419   Median :1.432  \n Mean   :1.404   Mean   :1.417   Mean   :1.429   Mean   :1.441  \n 3rd Qu.:1.450   3rd Qu.:1.463   3rd Qu.:1.476   3rd Qu.:1.487  \n Max.   :1.567   Max.   :1.580   Max.   :1.595   Max.   :1.607  \n      585             586             587             588       \n Min.   :1.293   Min.   :1.305   Min.   :1.314   Min.   :1.323  \n 1st Qu.:1.408   1st Qu.:1.420   1st Qu.:1.433   1st Qu.:1.444  \n Median :1.445   Median :1.457   Median :1.467   Median :1.476  \n Mean   :1.453   Mean   :1.464   Mean   :1.475   Mean   :1.485  \n 3rd Qu.:1.498   3rd Qu.:1.508   3rd Qu.:1.520   3rd Qu.:1.531  \n Max.   :1.621   Max.   :1.632   Max.   :1.644   Max.   :1.655  \n      589             590             591             592       \n Min.   :1.331   Min.   :1.338   Min.   :1.345   Min.   :1.352  \n 1st Qu.:1.454   1st Qu.:1.461   1st Qu.:1.467   1st Qu.:1.473  \n Median :1.483   Median :1.490   Median :1.495   Median :1.500  \n Mean   :1.493   Mean   :1.501   Mean   :1.508   Mean   :1.513  \n 3rd Qu.:1.540   3rd Qu.:1.548   3rd Qu.:1.556   3rd Qu.:1.563  \n Max.   :1.665   Max.   :1.673   Max.   :1.680   Max.   :1.687  \n      593             594             595             596       \n Min.   :1.357   Min.   :1.363   Min.   :1.370   Min.   :1.379  \n 1st Qu.:1.479   1st Qu.:1.485   1st Qu.:1.491   1st Qu.:1.499  \n Median :1.505   Median :1.511   Median :1.515   Median :1.522  \n Mean   :1.519   Mean   :1.524   Mean   :1.530   Mean   :1.538  \n 3rd Qu.:1.567   3rd Qu.:1.571   3rd Qu.:1.577   3rd Qu.:1.582  \n Max.   :1.693   Max.   :1.699   Max.   :1.704   Max.   :1.711  \n      597             598             599             600       \n Min.   :1.389   Min.   :1.401   Min.   :1.413   Min.   :1.425  \n 1st Qu.:1.507   1st Qu.:1.515   1st Qu.:1.524   1st Qu.:1.533  \n Median :1.531   Median :1.542   Median :1.552   Median :1.561  \n Mean   :1.546   Mean   :1.556   Mean   :1.566   Mean   :1.576  \n 3rd Qu.:1.590   3rd Qu.:1.599   3rd Qu.:1.608   3rd Qu.:1.619  \n Max.   :1.718   Max.   :1.728   Max.   :1.736   Max.   :1.745  \n      601             602             603             604       \n Min.   :1.435   Min.   :1.444   Min.   :1.450   Min.   :1.451  \n 1st Qu.:1.540   1st Qu.:1.546   1st Qu.:1.550   1st Qu.:1.551  \n Median :1.570   Median :1.577   Median :1.582   Median :1.584  \n Mean   :1.584   Mean   :1.591   Mean   :1.596   Mean   :1.598  \n 3rd Qu.:1.629   3rd Qu.:1.638   3rd Qu.:1.643   3rd Qu.:1.645  \n Max.   :1.753   Max.   :1.760   Max.   :1.765   Max.   :1.767  \n      605             606             607             608       \n Min.   :1.450   Min.   :1.446   Min.   :1.440   Min.   :1.433  \n 1st Qu.:1.551   1st Qu.:1.549   1st Qu.:1.545   1st Qu.:1.541  \n Median :1.583   Median :1.579   Median :1.575   Median :1.570  \n Mean   :1.597   Mean   :1.594   Mean   :1.589   Mean   :1.584  \n 3rd Qu.:1.643   3rd Qu.:1.638   3rd Qu.:1.631   3rd Qu.:1.626  \n Max.   :1.768   Max.   :1.765   Max.   :1.761   Max.   :1.757  \n      609             610             611             612       \n Min.   :1.426   Min.   :1.420   Min.   :1.414   Min.   :1.410  \n 1st Qu.:1.535   1st Qu.:1.530   1st Qu.:1.527   1st Qu.:1.524  \n Median :1.565   Median :1.560   Median :1.554   Median :1.550  \n Mean   :1.580   Mean   :1.575   Mean   :1.571   Mean   :1.569  \n 3rd Qu.:1.622   3rd Qu.:1.618   3rd Qu.:1.615   3rd Qu.:1.612  \n Max.   :1.755   Max.   :1.752   Max.   :1.750   Max.   :1.747  \n      613             614             615             616       \n Min.   :1.407   Min.   :1.405   Min.   :1.404   Min.   :1.405  \n 1st Qu.:1.523   1st Qu.:1.522   1st Qu.:1.523   1st Qu.:1.525  \n Median :1.549   Median :1.548   Median :1.549   Median :1.551  \n Mean   :1.567   Mean   :1.566   Mean   :1.567   Mean   :1.569  \n 3rd Qu.:1.611   3rd Qu.:1.609   3rd Qu.:1.610   3rd Qu.:1.612  \n Max.   :1.747   Max.   :1.746   Max.   :1.747   Max.   :1.750  \n      617             618             619             620       \n Min.   :1.408   Min.   :1.412   Min.   :1.417   Min.   :1.423  \n 1st Qu.:1.528   1st Qu.:1.531   1st Qu.:1.534   1st Qu.:1.538  \n Median :1.555   Median :1.559   Median :1.564   Median :1.570  \n Mean   :1.572   Mean   :1.576   Mean   :1.581   Mean   :1.585  \n 3rd Qu.:1.614   3rd Qu.:1.618   3rd Qu.:1.625   3rd Qu.:1.631  \n Max.   :1.755   Max.   :1.757   Max.   :1.762   Max.   :1.765  \n      621             622             623             624       \n Min.   :1.428   Min.   :1.431   Min.   :1.432   Min.   :1.431  \n 1st Qu.:1.541   1st Qu.:1.543   1st Qu.:1.544   1st Qu.:1.543  \n Median :1.574   Median :1.576   Median :1.578   Median :1.577  \n Mean   :1.589   Mean   :1.592   Mean   :1.593   Mean   :1.592  \n 3rd Qu.:1.638   3rd Qu.:1.641   3rd Qu.:1.643   3rd Qu.:1.642  \n Max.   :1.770   Max.   :1.773   Max.   :1.774   Max.   :1.774  \n      625             626             627             628       \n Min.   :1.428   Min.   :1.423   Min.   :1.418   Min.   :1.413  \n 1st Qu.:1.541   1st Qu.:1.537   1st Qu.:1.533   1st Qu.:1.530  \n Median :1.574   Median :1.570   Median :1.565   Median :1.561  \n Mean   :1.590   Mean   :1.586   Mean   :1.582   Mean   :1.579  \n 3rd Qu.:1.639   3rd Qu.:1.635   3rd Qu.:1.630   3rd Qu.:1.627  \n Max.   :1.773   Max.   :1.771   Max.   :1.768   Max.   :1.767  \n      629             630             631             632       \n Min.   :1.409   Min.   :1.407   Min.   :1.406   Min.   :1.405  \n 1st Qu.:1.528   1st Qu.:1.527   1st Qu.:1.525   1st Qu.:1.526  \n Median :1.559   Median :1.557   Median :1.557   Median :1.558  \n Mean   :1.577   Mean   :1.575   Mean   :1.575   Mean   :1.576  \n 3rd Qu.:1.625   3rd Qu.:1.623   3rd Qu.:1.624   3rd Qu.:1.625  \n Max.   :1.766   Max.   :1.767   Max.   :1.768   Max.   :1.769  \n      633             634             635             636       \n Min.   :1.406   Min.   :1.407   Min.   :1.408   Min.   :1.408  \n 1st Qu.:1.526   1st Qu.:1.526   1st Qu.:1.527   1st Qu.:1.528  \n Median :1.559   Median :1.559   Median :1.561   Median :1.562  \n Mean   :1.577   Mean   :1.578   Mean   :1.579   Mean   :1.580  \n 3rd Qu.:1.627   3rd Qu.:1.628   3rd Qu.:1.629   3rd Qu.:1.632  \n Max.   :1.772   Max.   :1.773   Max.   :1.775   Max.   :1.778  \n      637             638             639             640       \n Min.   :1.408   Min.   :1.409   Min.   :1.410   Min.   :1.410  \n 1st Qu.:1.528   1st Qu.:1.529   1st Qu.:1.530   1st Qu.:1.532  \n Median :1.563   Median :1.564   Median :1.565   Median :1.567  \n Mean   :1.581   Mean   :1.583   Mean   :1.584   Mean   :1.586  \n 3rd Qu.:1.634   3rd Qu.:1.636   3rd Qu.:1.638   3rd Qu.:1.640  \n Max.   :1.781   Max.   :1.784   Max.   :1.786   Max.   :1.789  \n      641             642             643             644       \n Min.   :1.411   Min.   :1.412   Min.   :1.414   Min.   :1.415  \n 1st Qu.:1.533   1st Qu.:1.534   1st Qu.:1.535   1st Qu.:1.536  \n Median :1.569   Median :1.571   Median :1.574   Median :1.576  \n Mean   :1.588   Mean   :1.590   Mean   :1.592   Mean   :1.594  \n 3rd Qu.:1.643   3rd Qu.:1.645   3rd Qu.:1.648   3rd Qu.:1.650  \n Max.   :1.792   Max.   :1.795   Max.   :1.799   Max.   :1.802  \n      645             646             647             648       \n Min.   :1.417   Min.   :1.418   Min.   :1.421   Min.   :1.423  \n 1st Qu.:1.538   1st Qu.:1.539   1st Qu.:1.541   1st Qu.:1.544  \n Median :1.578   Median :1.581   Median :1.584   Median :1.587  \n Mean   :1.596   Mean   :1.598   Mean   :1.601   Mean   :1.605  \n 3rd Qu.:1.653   3rd Qu.:1.655   3rd Qu.:1.660   3rd Qu.:1.665  \n Max.   :1.805   Max.   :1.809   Max.   :1.813   Max.   :1.819  \n      649             650             651             652       \n Min.   :1.425   Min.   :1.428   Min.   :1.431   Min.   :1.435  \n 1st Qu.:1.547   1st Qu.:1.551   1st Qu.:1.555   1st Qu.:1.559  \n Median :1.591   Median :1.594   Median :1.599   Median :1.602  \n Mean   :1.608   Mean   :1.612   Mean   :1.617   Mean   :1.621  \n 3rd Qu.:1.671   3rd Qu.:1.675   3rd Qu.:1.680   3rd Qu.:1.683  \n Max.   :1.823   Max.   :1.827   Max.   :1.836   Max.   :1.840  \n      653             654             655             656       \n Min.   :1.440   Min.   :1.444   Min.   :1.449   Min.   :1.453  \n 1st Qu.:1.563   1st Qu.:1.567   1st Qu.:1.571   1st Qu.:1.575  \n Median :1.606   Median :1.609   Median :1.613   Median :1.616  \n Mean   :1.626   Mean   :1.630   Mean   :1.635   Mean   :1.640  \n 3rd Qu.:1.690   3rd Qu.:1.695   3rd Qu.:1.697   3rd Qu.:1.703  \n Max.   :1.848   Max.   :1.856   Max.   :1.864   Max.   :1.872  \n      657             658             659             660       \n Min.   :1.458   Min.   :1.460   Min.   :1.463   Min.   :1.466  \n 1st Qu.:1.580   1st Qu.:1.582   1st Qu.:1.586   1st Qu.:1.589  \n Median :1.619   Median :1.620   Median :1.626   Median :1.629  \n Mean   :1.644   Mean   :1.647   Mean   :1.653   Mean   :1.657  \n 3rd Qu.:1.707   3rd Qu.:1.710   3rd Qu.:1.719   3rd Qu.:1.723  \n Max.   :1.881   Max.   :1.889   Max.   :1.902   Max.   :1.915  \n      661             662             663             664       \n Min.   :1.468   Min.   :1.471   Min.   :1.474   Min.   :1.477  \n 1st Qu.:1.592   1st Qu.:1.594   1st Qu.:1.597   1st Qu.:1.600  \n Median :1.633   Median :1.636   Median :1.639   Median :1.643  \n Mean   :1.662   Mean   :1.667   Mean   :1.671   Mean   :1.676  \n 3rd Qu.:1.729   3rd Qu.:1.736   3rd Qu.:1.741   3rd Qu.:1.747  \n Max.   :1.926   Max.   :1.939   Max.   :1.952   Max.   :1.967  \n      665             666             667             668       \n Min.   :1.480   Min.   :1.483   Min.   :1.486   Min.   :1.488  \n 1st Qu.:1.603   1st Qu.:1.606   1st Qu.:1.610   1st Qu.:1.614  \n Median :1.646   Median :1.650   Median :1.654   Median :1.656  \n Mean   :1.680   Mean   :1.684   Mean   :1.689   Mean   :1.693  \n 3rd Qu.:1.753   3rd Qu.:1.758   3rd Qu.:1.765   3rd Qu.:1.771  \n Max.   :1.980   Max.   :1.992   Max.   :2.006   Max.   :2.020  \n      669             670             671             672       \n Min.   :1.491   Min.   :1.494   Min.   :1.496   Min.   :1.500  \n 1st Qu.:1.615   1st Qu.:1.618   1st Qu.:1.621   1st Qu.:1.624  \n Median :1.661   Median :1.665   Median :1.669   Median :1.673  \n Mean   :1.698   Mean   :1.703   Mean   :1.707   Mean   :1.713  \n 3rd Qu.:1.778   3rd Qu.:1.784   3rd Qu.:1.791   3rd Qu.:1.796  \n Max.   :2.033   Max.   :2.049   Max.   :2.065   Max.   :2.076  \n      673             674             675             676       \n Min.   :1.504   Min.   :1.508   Min.   :1.511   Min.   :1.515  \n 1st Qu.:1.629   1st Qu.:1.632   1st Qu.:1.636   1st Qu.:1.639  \n Median :1.677   Median :1.681   Median :1.685   Median :1.687  \n Mean   :1.718   Mean   :1.723   Mean   :1.729   Mean   :1.734  \n 3rd Qu.:1.801   3rd Qu.:1.806   3rd Qu.:1.812   3rd Qu.:1.820  \n Max.   :2.094   Max.   :2.108   Max.   :2.130   Max.   :2.146  \n      677             678             679             680       \n Min.   :1.518   Min.   :1.522   Min.   :1.526   Min.   :1.529  \n 1st Qu.:1.641   1st Qu.:1.644   1st Qu.:1.647   1st Qu.:1.650  \n Median :1.690   Median :1.694   Median :1.697   Median :1.700  \n Mean   :1.738   Mean   :1.744   Mean   :1.749   Mean   :1.754  \n 3rd Qu.:1.826   3rd Qu.:1.835   3rd Qu.:1.840   3rd Qu.:1.849  \n Max.   :2.164   Max.   :2.176   Max.   :2.184   Max.   :2.203  \n      681             682             683             684       \n Min.   :1.531   Min.   :1.534   Min.   :1.536   Min.   :1.538  \n 1st Qu.:1.654   1st Qu.:1.656   1st Qu.:1.659   1st Qu.:1.662  \n Median :1.701   Median :1.706   Median :1.710   Median :1.714  \n Mean   :1.759   Mean   :1.764   Mean   :1.769   Mean   :1.773  \n 3rd Qu.:1.856   3rd Qu.:1.866   3rd Qu.:1.877   3rd Qu.:1.885  \n Max.   :2.221   Max.   :2.243   Max.   :2.267   Max.   :2.284  \n      685             686             687             688       \n Min.   :1.539   Min.   :1.540   Min.   :1.540   Min.   :1.539  \n 1st Qu.:1.665   1st Qu.:1.670   1st Qu.:1.674   1st Qu.:1.676  \n Median :1.717   Median :1.719   Median :1.722   Median :1.725  \n Mean   :1.777   Mean   :1.782   Mean   :1.785   Mean   :1.788  \n 3rd Qu.:1.892   3rd Qu.:1.897   3rd Qu.:1.903   3rd Qu.:1.910  \n Max.   :2.302   Max.   :2.332   Max.   :2.336   Max.   :2.359  \n      689             690             691             692       \n Min.   :1.540   Min.   :1.539   Min.   :1.539   Min.   :1.539  \n 1st Qu.:1.678   1st Qu.:1.678   1st Qu.:1.680   1st Qu.:1.680  \n Median :1.728   Median :1.729   Median :1.731   Median :1.733  \n Mean   :1.792   Mean   :1.793   Mean   :1.797   Mean   :1.799  \n 3rd Qu.:1.913   3rd Qu.:1.914   3rd Qu.:1.918   3rd Qu.:1.920  \n Max.   :2.377   Max.   :2.386   Max.   :2.396   Max.   :2.412  \n      693             694             695             696       \n Min.   :1.539   Min.   :1.539   Min.   :1.538   Min.   :1.537  \n 1st Qu.:1.678   1st Qu.:1.678   1st Qu.:1.679   1st Qu.:1.677  \n Median :1.734   Median :1.734   Median :1.734   Median :1.732  \n Mean   :1.802   Mean   :1.804   Mean   :1.806   Mean   :1.806  \n 3rd Qu.:1.924   3rd Qu.:1.928   3rd Qu.:1.932   3rd Qu.:1.931  \n Max.   :2.427   Max.   :2.439   Max.   :2.451   Max.   :2.460  \n      697             698             699             700       \n Min.   :1.538   Min.   :1.536   Min.   :1.535   Min.   :1.535  \n 1st Qu.:1.676   1st Qu.:1.676   1st Qu.:1.676   1st Qu.:1.674  \n Median :1.734   Median :1.736   Median :1.738   Median :1.739  \n Mean   :1.808   Mean   :1.810   Mean   :1.813   Mean   :1.815  \n 3rd Qu.:1.937   3rd Qu.:1.940   3rd Qu.:1.945   3rd Qu.:1.950  \n Max.   :2.452   Max.   :2.476   Max.   :2.493   Max.   :2.507  \n       V1              V2              V3              V4       \n Min.   :14.84   Min.   : 9.95   Min.   :43.53   Min.   :11.03  \n 1st Qu.:16.59   1st Qu.:13.32   1st Qu.:46.36   1st Qu.:13.39  \n Median :18.38   Median :16.57   Median :49.38   Median :14.28  \n Mean   :18.31   Mean   :16.59   Mean   :48.98   Mean   :14.19  \n 3rd Qu.:19.94   3rd Qu.:19.88   3rd Qu.:50.77   3rd Qu.:15.07  \n Max.   :21.67   Max.   :23.19   Max.   :54.61   Max.   :17.69  \n\n\nCode\ncookies = na.omit(cookies)\ndim(cookies)\n\n\n[1]  72 704\n\n\nCode\n# effectivement pas de NA\n\ncookies.train = cookies[1:40,]\ncookies.test = cookies[41:72,]\n\n# Question 1 ------------------------------------------------------------\nmatplot(t(cookies.train),\n        type='l',\n        ylim=c(0.2, 1.85),\n        main = paste(\"Plot des spectres des\", dim(t(cookies.train))[2],\" cookies de l'échantillon train\"))\n\n\n\n\n\nCode\ndim(t(cookies.train)) \n\n\n[1] 704  40\n\n\nCode\n# 704  40\n\n# PCA\nFactoMineR::PCA(cookies, scale=T, quanti.sup=c(701:704))\n\n\n\n\n\n\n\n\n**Results for the Principal Component Analysis (PCA)**\nThe analysis was performed on 72 individuals, described by 704 variables\n*The results are available in the following objects:\n\n   name               \n1  \"$eig\"             \n2  \"$var\"             \n3  \"$var$coord\"       \n4  \"$var$cor\"         \n5  \"$var$cos2\"        \n6  \"$var$contrib\"     \n7  \"$ind\"             \n8  \"$ind$coord\"       \n9  \"$ind$cos2\"        \n10 \"$ind$contrib\"     \n11 \"$quanti.sup\"      \n12 \"$quanti.sup$coord\"\n13 \"$quanti.sup$cor\"  \n14 \"$call\"            \n15 \"$call$centre\"     \n16 \"$call$ecart.type\" \n17 \"$call$row.w\"      \n18 \"$call$col.w\"      \n   description                                              \n1  \"eigenvalues\"                                            \n2  \"results for the variables\"                              \n3  \"coord. for the variables\"                               \n4  \"correlations variables - dimensions\"                    \n5  \"cos2 for the variables\"                                 \n6  \"contributions of the variables\"                         \n7  \"results for the individuals\"                            \n8  \"coord. for the individuals\"                             \n9  \"cos2 for the individuals\"                               \n10 \"contributions of the individuals\"                       \n11 \"results for the supplementary quantitative variables\"   \n12 \"coord. for the supplementary quantitative variables\"    \n13 \"correlations suppl. quantitative variables - dimensions\"\n14 \"summary statistics\"                                     \n15 \"mean of the variables\"                                  \n16 \"standard error of the variables\"                        \n17 \"weights for the individuals\"                            \n18 \"weights for the variables\"                              \n\n\nCode\n# Question 2 ------------------------------------------------------------\n\n#### Data prep ####\nX.train = as.matrix(cookies.train[,1:700])\ny.train = as.matrix(cookies.train[,702])\n\nX.test = as.matrix(cookies.test[,1:700])\ny.test = as.matrix(cookies.test[,702])\n\ntab = data.frame(y = y.train, X = X.train)\n\n#### Step Forward (hybride) ####\nreg_full = lm(y~.,data = tab)\nreg_start = lm(y~1,data = tab)\nmodforw = step(reg_start,\n               scope = formula(reg_full),\n               direction = \"both\",\n               k = log(dim(tab)[1]),\n               trace = 0) \nsummary(modforw)\n\n\n\nCall:\nlm(formula = y ~ X.427 + X.489 + X.579 + X.1 + X.699 + X.492, \n    data = tab)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.61160 -0.62962 -0.02921  0.75900  3.07626 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    10.972      5.972   1.837  0.07517 .  \nX.427         -36.986     15.649  -2.364  0.02414 *  \nX.489        1326.348    422.106   3.142  0.00353 ** \nX.579        -181.024     27.102  -6.679 1.33e-07 ***\nX.1            58.077     17.975   3.231  0.00279 ** \nX.699          16.118      4.471   3.605  0.00102 ** \nX.492       -1134.433    440.535  -2.575  0.01469 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.264 on 33 degrees of freedom\nMultiple R-squared:  0.9116,    Adjusted R-squared:  0.8956 \nF-statistic: 56.75 on 6 and 33 DF,  p-value: 5.578e-16\n\n\nCode\n# Question 3 ------------------------------------------------------------\nprev_forw = sqrt(mean((predict(modforw,data.frame(X = X.test)) - y.test)^2))\nindexforw = as.numeric(substr(names(modforw$model)[-1],3,10))\n\nprev_forw # 1.669854\n\n\n[1] 1.669854\n\n\nCode\nindexforw # 427 489 579   1 699 492\n\n\n[1] 427 489 579   1 699 492\n\n\nCode\nmatplot(t(cookies.train), type='l', ylim=c(0.2, 2))\nabline(v = indexforw, lwd = 2, col =\"red\" )\nlegend(\"topleft\", legend = \"forward\", col=\"red\", lty = 1, lwd = 2)\n\n\n\n\n\nCode\n# Question 4 ------------------------------------------------------------\n\n#### PCR ####\nlibrary(pls)\n\n\n\nAttachement du package : 'pls'\n\n\nL'objet suivant est masqué depuis 'package:stats':\n\n    loadings\n\n\nCode\npcr.fit = pcr(y.train~X.train, \n              scale = TRUE,\n              validation = \"CV\",\n              segments = 4) \n# K = 4 dans CV car l'echantillon est petit (n=40) donc évitons les k=10\n\nvalidationplot(pcr.fit)\n\n\n\n\n\nCode\nM_pcr = which.min(pcr.fit$validation$PRESS)\nprev_pcr = sqrt(mean((predict(pcr.fit, cookies.test[,1:700], ncomp = M_pcr) - cookies.test[,702])^2))\n\n\nWarning: 'newdata' avait 32 lignes mais les variables trouvées ont 40 lignes\n\n\nWarning in predict(pcr.fit, cookies.test[, 1:700], ncomp = M_pcr) -\ncookies.test[, : la taille d'un objet plus long n'est pas multiple de la taille\nd'un objet plus court\n\n\nCode\nprev_pcr # 5.917207\n\n\n[1] 5.935859\n\n\nCode\n# Question 5 ------------------------------------------------------------\n\n#### PLS ####\npls.fit = plsr(y~X.train,\n               data = tab,\n               scale = TRUE,\n               validation = \"CV\",\n               segments = 4)\nvalidationplot(pls.fit)\n\n\n\n\n\nCode\nM_pls = which.min(pls.fit$validation$PRESS) \nprev_pls = sqrt(mean((predict(pls.fit, cookies.test[,1:700], ncomp = M_pls) - cookies.test[,702])^2))\n\n\nWarning: 'newdata' avait 32 lignes mais les variables trouvées ont 40 lignes\n\n\nWarning in predict(pls.fit, cookies.test[, 1:700], ncomp = M_pls) -\ncookies.test[, : la taille d'un objet plus long n'est pas multiple de la taille\nd'un objet plus court\n\n\nCode\nprev_pls # 5.923798\n\n\n[1] 6.032614\n\n\nCode\n# Question 6 ------------------------------------------------------------\n\n#### Ridge ####\nlibrary(glmnet)\n\n\nLoaded glmnet 4.1-8\n\n\nCode\ncvglm = cv.glmnet(X.train, y.train, \n                  alpha = 0, # alpha = 0 &lt;=&gt; Ridge \n                  nfolds = 4)\nplot(cvglm)\n\n\n\n\n\nCode\n# lambda n'a pas été choisi assez petit. \n# En effet, on est au bord en terme de min donc il faudrait regarder \n# plus petit et zoomer pour voir si on est pas en cas MCO\ncvglm$lambda\n\n\n  [1] 1912.13994 1825.23023 1742.27070 1663.08180 1587.49217 1515.33820\n  [7] 1446.46374 1380.71973 1317.96389 1258.06040 1200.87961 1146.29778\n [13] 1094.19677 1044.46384  996.99135  951.67655  908.42138  867.13223\n [19]  827.71973  790.09859  754.18739  719.90841  687.18746  655.95373\n [25]  626.13962  597.68061  570.51510  544.58431  519.83212  496.20494\n [31]  473.65166  452.12346  431.57375  411.95806  393.23393  375.36084\n [37]  358.30011  342.01482  326.46972  311.63117  297.46705  283.94671\n [43]  271.04090  258.72167  246.96237  235.73755  225.02291  214.79527\n [49]  205.03250  195.71345  186.81798  178.32681  170.22158  162.48475\n [55]  155.09957  148.05005  141.32095  134.89769  128.76638  122.91375\n [61]  117.32713  111.99443  106.90411  102.04516   97.40705   92.97975\n [67]   88.75367   84.71968   80.86904   77.19342   73.68486   70.33577\n [73]   67.13890   64.08733   61.17446   58.39399   55.73989   53.20643\n [79]   50.78812   48.47972   46.27624   44.17291   42.16519   40.24871\n [85]   38.41935   36.67313   35.00628   33.41519   31.89642   30.44668\n [91]   29.06283   27.74188   26.48097   25.27736   24.12847   23.03179\n [97]   21.98496   20.98571   20.03188   19.12140\n\n\nCode\ncvglm = cv.glmnet(X.train, y.train, \n                  alpha = 0,\n                  nfolds = 4,\n                  lambda = seq(0.1, 5, 0.01))\nplot(cvglm)\n\n\n\n\n\nCode\n# recommencer si choix aberrant\n\nlbda_min_ridge = cvglm$lambda.min\nprev_ridge = sqrt(mean((predict(cvglm, newx = X.test, s = lbda_min_ridge) - y.test)^2))\n# pas lambda.1se car ecart type trop grand avec n=40\n\nprev_ridge # 0.8743336 mais depend du nfold\n\n\n[1] 0.9352494\n\n\nCode\n# Question 7 ------------------------------------------------------------\n\n#### Lasso ####\ncvglm = cv.glmnet(X.train, y.train,\n                  alpha = 1, # alpha = 1 &lt;=&gt; Lasso\n                  nfolds = 4)\nplot(cvglm)\n\n\n\n\n\nCode\n# lambda n'a pas été choisi assez petit car \n# encore une fois on est proche du bord gauche et bord gauche = MCO\n# Attention, on est en log\n\ncvglm$lambda\n\n\n  [1] 1.91213994 1.82523023 1.74227070 1.66308180 1.58749217 1.51533820\n  [7] 1.44646374 1.38071973 1.31796389 1.25806040 1.20087961 1.14629778\n [13] 1.09419677 1.04446384 0.99699135 0.95167655 0.90842138 0.86713223\n [19] 0.82771973 0.79009859 0.75418739 0.71990841 0.68718746 0.65595373\n [25] 0.62613962 0.59768061 0.57051510 0.54458431 0.51983212 0.49620494\n [31] 0.47365166 0.45212346 0.43157375 0.41195806 0.39323393 0.37536084\n [37] 0.35830011 0.34201482 0.32646972 0.31163117 0.29746705 0.28394671\n [43] 0.27104090 0.25872167 0.24696237 0.23573755 0.22502291 0.21479527\n [49] 0.20503250 0.19571345 0.18681798 0.17832681 0.17022158 0.16248475\n [55] 0.15509957 0.14805005 0.14132095 0.13489769 0.12876638 0.12291375\n [61] 0.11732713 0.11199443 0.10690411 0.10204516 0.09740705 0.09297975\n [67] 0.08875367 0.08471968 0.08086904 0.07719342 0.07368486 0.07033577\n [73] 0.06713890 0.06408733 0.06117446 0.05839399 0.05573989 0.05320643\n [79] 0.05078812 0.04847972 0.04627624 0.04417291 0.04216519 0.04024871\n [85] 0.03841935 0.03667313 0.03500628 0.03341519 0.03189642 0.03044668\n [91] 0.02906283 0.02774188 0.02648097 0.02527736 0.02412847 0.02303179\n [97] 0.02198496 0.02098571 0.02003188 0.01912140\n\n\nCode\ncvglm = cv.glmnet(X.train, y.train,\n                  alpha = 1,\n                  nfolds = 4,\n                  lambda = seq(0.01, 1, 0.001))\nplot(cvglm)\n\n\n\n\n\nCode\n# recommencer si choix aberrant\n\nlbda_min_lasso = cvglm$lambda.min\nprev_lasso = sqrt(mean((predict(cvglm, newx = X.test, s = lbda_min_lasso) - y.test)^2))\n# pas lambda.1se car ecart type trop grand avec n=40\n\nprev_lasso # 1.846122 mais depend du nfold\n\n\n[1] 1.724018\n\n\nCode\n## Comme on a de la selection de variable, regardons les longeurs d'ondes gardée\n# pour comparer avec forward\ncoeff = coef(cvglm, s=lbda_min_lasso)\nindexlasso = which(coeff[-1] != 0)\nlength(indexlasso)\n\n\n[1] 26\n\n\nCode\nmatplot(t(cookies.train), type='l', ylim=c(0.2, 2))\nabline(v = indexforw, lwd = 2, col =\"red\" )\nabline(v = indexlasso, lwd = 2, col =\"green\" )\nlegend(\"topleft\", legend = c(\"forward\", \"lasso\"), col=c(\"red\", \"green\"), lty = c(1,1), lwd = c(2,2))\n\n\n\n\n\nCode\n# Pour comparer lasso et forward, on peut donc prendre en considération le rmsep \n# mais aussi prendre en compte le nombre de variables selectionnées\n\n\n# Question 8 ------------------------------------------------------------\n\n# Interessant de faire Gauss-Lasso car entre ridge et lasso \n# on a un écart de rmsep ou lasso pas forcément meilleur \n# donc interssant de tester un entre deux \n\n#### Gauss-Lasso ####\nw = coef(cvglm, s = cvglm$lambda.min)\nindexlasso = which(w[-1] != 0)\nlength(indexlasso)\n\n\n[1] 26\n\n\nCode\n# environ 30 : trop grand /à n=40 pour les MCO, ne marchera pas\n\nreg = lm(y~.,\n         data = tab[, c(1, indexlasso+1)])\n\n\nprev_gauss = sqrt(mean((predict(reg, data.frame(X = X.test)) - y.test)^2))\nprev_gauss # 1.825471\n\n\n[1] 5.091311\n\n\nCode\n# Question 9 ------------------------------------------------------------\n\n#### Lasso Adaptative ####\ncvglm = cv.glmnet(X.train, y.train, \n                  nfolds = 4,\n                  penalty.factor = 1/abs(w[-1]))\nplot(cvglm)\n\n\n\n\n\nCode\ncvglm$lambda\n\n\n  [1] 9673.65752 9233.97486 8814.27645 8413.65398 8031.24042 7666.20815\n  [7] 7317.76716 6985.16335 6667.67689 6364.62069 6075.33886 5799.20533\n [13] 5535.62250 5284.01991 5043.85305 4814.60214 4595.77105 4386.88616\n [19] 4187.49541 3997.16728 3815.48986 3642.06996 3476.53226 3318.51850\n [25] 3167.68671 3023.71046 2886.27816 2755.09236 2629.86916 2510.33755\n [31] 2396.23883 2287.32608 2183.36358 2084.12633 1989.39958 1898.97830\n [37] 1812.66680 1730.27830 1651.63448 1576.56514 1504.90782 1436.50744\n [43] 1371.21596 1308.89208 1249.40091 1192.61371 1138.40758 1086.66520\n [49] 1037.27459  990.12886  945.12597  902.16854  861.16359  822.02237\n [55]  784.66019  748.99617  714.95314  682.45741  651.43867  621.82978\n [61]  593.56666  566.58813  540.83583  516.25401  492.78947  470.39142\n [67]  449.01141  428.60315  409.12247  390.52722  372.77716  355.83386\n [73]  339.66066  324.22256  309.48615  295.41952  281.99225  269.17527\n [79]  256.94084  245.26248  234.11492  223.47404  213.31680  203.62122\n [85]  194.36632  185.53207  177.09935  169.04991  161.36633  154.03198\n [91]  147.03099  140.34821  133.96916  127.88006  122.06771  116.51955\n [97]  111.22355  106.16827  101.34276   96.73658\n\n\nCode\ncvglm = cv.glmnet(X.train, y.train,\n                  alpha = 1,\n                  nfolds = 4,\n                  penalty.factor = 1/abs(w[-1]),\n                  lambda = seq(1, 50, 0.1))\nplot(cvglm)\n\n\n\n\n\nCode\nlbda_min_alasso = cvglm$lambda.min\nprev_alasso = sqrt(mean((predict(cvglm, newx = X.test, s = lbda_min_alasso) - y.test)^2))\nprev_alasso # 1.901688 mais depend du nfold\n\n\n[1] 1.958676\n\n\nCode\n# + Gauss pour finir\nwal = coef(cvglm, s = lbda_min_alasso)\nindexadlasso = which(wal[-1] != 0)\nlength(indexadlasso) #9 : on peut essayer gauss\n\n\n[1] 7\n\n\nCode\nreg=lm(y~.,\n       data = tab[, c(1, indexadlasso + 1)])\n\nprev_algauss = sqrt(mean((predict(reg,data.frame(X = X.test)) - y.test)^2))\nprev_algauss # 2.003703\n\n\n[1] 2.137385\n\n\nCode\n# Question 10 ------------------------------------------------------------\n\n#### Elastic Net ####\ncvglm = cv.glmnet(X.train, y.train,\n                  alpha = 0.5,\n                  nfolds = 4)\nplot(cvglm)\n\n\n\n\n\nCode\n# encore une fois, lambda proche du bord gauche donc essayons d'affiner \ncvglm$lambda\n\n\n  [1] 3.82427988 3.65046045 3.48454139 3.32616361 3.17498434 3.03067640\n  [7] 2.89292748 2.76143946 2.63592778 2.51612079 2.40175922 2.29259555\n [13] 2.18839355 2.08892768 1.99398270 1.90335311 1.81684277 1.73426446\n [19] 1.65543947 1.58019719 1.50837479 1.43981682 1.37437493 1.31190747\n [25] 1.25227925 1.19536122 1.14103021 1.08916863 1.03966423 0.99240989\n [31] 0.94730333 0.90424693 0.86314751 0.82391612 0.78646786 0.75072169\n [37] 0.71660023 0.68402964 0.65293944 0.62326233 0.59493410 0.56789342\n [43] 0.54208179 0.51744334 0.49392474 0.47147510 0.45004583 0.42959055\n [49] 0.41006500 0.39142691 0.37363595 0.35665362 0.34044316 0.32496950\n [55] 0.31019913 0.29610010 0.28264190 0.26979539 0.25753277 0.24582751\n [61] 0.23465427 0.22398887 0.21380823 0.20409031 0.19481409 0.18595949\n [67] 0.17750735 0.16943936 0.16173808 0.15438684 0.14736972 0.14067154\n [73] 0.13427780 0.12817467 0.12234893 0.11678798 0.11147979 0.10641286\n [79] 0.10157623 0.09695943 0.09255248 0.08834583 0.08433037 0.08049743\n [85] 0.07683869 0.07334626 0.07001256 0.06683038 0.06379283 0.06089335\n [91] 0.05812565 0.05548375 0.05296193 0.05055473 0.04825694 0.04606359\n [97] 0.04396993 0.04197142 0.04006376 0.03824280\n\n\nCode\ncvglm = cv.glmnet(X.train, y.train,\n                  alpha = 0.5,\n                  nfolds = 4,\n                  lambda = seq(0.001, 1, 0.001))\nplot(cvglm)\n\n\n\n\n\nCode\nlbda_min_en = cvglm$lambda.min\nprev_en = sqrt(mean((predict(cvglm, newx = X.test, s = lbda_min_en) - y.test)^2))\nprev_en # 2.007716\n\n\n[1] 1.355662\n\n\nCode\nwen = coef(cvglm, s = lbda_min_en)\nindexen = which(wen[-1] != 0)\nlength(indexen)\n\n\n[1] 68\n\n\nCode\nprev_list = c(prev_forw, prev_ridge, prev_pcr, prev_pls, prev_lasso, prev_gauss, prev_alasso, prev_algauss, prev_en)\n\nmatplot(t(X.train), type='l', ylim = c(-0.2, 2))\npoints(indexforw, rep(0.2, length(indexforw)), pch = 3, col = \"red\")\npoints(indexadlasso, rep(0.1, length(indexadlasso)), pch = 3, col = \"cyan\")\npoints(indexlasso, rep(0, length(indexlasso)), pch = 3, col = \"deeppink\")\npoints(indexen, rep(-0.1, length(indexen)), pch = 3, col = \"purple\")\nlegend(\"topleft\", legend = c(\"Forward\",\"Adaptative Lasso\",\"Lasso\",\"Elastic Net\"), pch = 3, col = c(\"red\", \"cyan\", \"deeppink\", \"purple\"))\n\n\n\n\n\nCode\n## On met des points pour la lisibilité contrairement à une multitude de abline\n\n\n\n\n# Question 11 ------------------------------------------------------------\n\n# Analyser les résultats, à la fois en terme de qualité de prévision \n# et d’identification des longueurs d’onde importantes\n\n\nprev_list\n\n\n[1] 1.6698542 0.9352494 5.9358587 6.0326143 1.7240176 5.0913110 1.9586761\n[8] 2.1373851 1.3556622\n\n\nCode\n# en terme de rmsep (sans compter l'aléa du nfold), on a :\n# Ridge, forward, gauss, lasso, alasso, algauss, en\n# que l'on gardera (&lt;2.1)\n# par contre on pourrait rejeter pcr et pls (&gt;5.5)\n\n# en terme de selection de variables\nlength(indexforw) # 6\n\n\n[1] 6\n\n\nCode\nlength(indexlasso) # 19\n\n\n[1] 26\n\n\nCode\nlength(indexadlasso) # 8\n\n\n[1] 7\n\n\nCode\nlength(indexen) # 298\n\n\n[1] 68\n\n\nCode\n# Donc on pourrait enlever Elastic Net qui garde beaucoup de variables\n\n## Conclusion :\n# avec ces deux critères, on pourrait avoir une préférence \n# pour forward qui garde peut de variable et qui a un rmsep faible\n# mais le rmsep le plus faible reste pour ridge \n\n## Concl à revoir car normalement pcr et pls tournent également autour de 1.qqchose\n\n## IDEE : \n# faire un dataframe avec nom des methodes et rmsep associé et nb de variables gardées"
  },
  {
    "objectID": "posts/Exercice_09Bonus.html",
    "href": "posts/Exercice_09Bonus.html",
    "title": "Exercice 09 Bonus : régression logistique pénélisée",
    "section": "",
    "text": "Dans cette partie et avant de passer à l’exercice 10, nous allons faire la section 3.3 sur la régression logistique pénélisée du tutoriel de Laurent Rouvière\n\n\nCode\nlibrary(dplyr)\n\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(glmnet)\n\n\nLe chargement a nécessité le package : Matrix\n\n\nLoaded glmnet 4.1-8\n\n\nCode\nad.data &lt;- readr::read_delim(\n  \"~/Documents/1_Projet/Perso/Statistique_en_grande_dimension/data/internet+advertisements/ad.data\",\n  delim = \",\",\n  col_names = FALSE,\n  na = c(\"?\"),\n  trim_ws = TRUE,\n  col_types = readr::cols(X1559 = readr::col_factor())\n) |&gt;\n  rename(Y = X1559)  \n\n\n\n\nCode\nsummary(ad.data$Y)\n\n\n   ad. nonad. \n   459   2820 \n\n\n\n\nCode\nsum(is.na(ad.data))\n\n\n[1] 2729\n\n\n\n\nCode\nvar.na &lt;- apply(is.na(ad.data),2,any)\nnames(ad.data)[var.na]\n\n\n[1] \"X1\" \"X2\" \"X3\" \"X4\"\n\n\n\n\nCode\nind.na &lt;- apply(is.na(ad.data),1,any)\nsum(ind.na)\n\n\n[1] 920\n\n\n\n\nCode\nad.data1 &lt;- ad.data[,var.na==FALSE]\ndim(ad.data1)\n\n\n[1] 3279 1555\n\n\n\n\nCode\nsum(is.na(ad.data1))\n\n\n[1] 0\n\n\n\n\nCode\nX.ad &lt;- model.matrix(Y~.,data=ad.data1)[,-1]\nY.ad &lt;- ad.data1$Y\n\n\n\n\nCode\nset.seed(1234)\nlasso.cv &lt;- cv.glmnet(X.ad,Y.ad,family=\"binomial\",alpha=1)\nplot(lasso.cv)"
  },
  {
    "objectID": "posts/Exercice_11.html",
    "href": "posts/Exercice_11.html",
    "title": "Exercice 11",
    "section": "",
    "text": "Intervenant.e.s\n\nRédaction\n\nClément Poupelin, clementjc.poupelin@gmail.com\n\n\n\n\nRelecture\n\n\n\n\n\n\nRappels sur les tests multiples\n\n\nSetup\n\nPackagesFonctionsSeed\n\n\n\n\nCode\n# Données\nlibrary(dplyr)        # manipulation des données\n\n# Plots\n## ggplot\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\n\n\n\nSimulation des p-valuesFonction qui calcul les résultats\n\n\n\n\nCode\nsimulate_pvalues &lt;- function(n, m, m0, mu1) {\n  p_values &lt;- numeric(m)\n  \n  for (i in 1:m0) {\n    sample &lt;- rnorm(n, mean = 0, sd = 1)  \n    p_values[i] &lt;- t.test(sample)$p.value\n  }\n  \n  for (i in (m0+1):m) {\n    sample &lt;- rnorm(n, mean = mu1, sd = 1)  \n    p_values[i] &lt;- t.test(sample)$p.value\n  }\n  \n  return(p_values)\n}\n\n\n\n\n\n\nCode\nresults_fun &lt;- function(p_values, m0, alpha = 0.05) {\n  positives &lt;- p_values &lt; alpha\n  bonferroni_threshold &lt;- alpha / length(p_values)\n  bh_thresholds &lt;- p.adjust(p_values, method = \"BH\")\n  \n  bonferroni_positives &lt;- p_values &lt; bonferroni_threshold\n  bh_positives &lt;- bh_thresholds &lt; alpha\n  \n  count_results &lt;- function(positives) {\n    true_positives &lt;- sum(positives[(m0 + 1):length(p_values)])\n    false_positives &lt;- sum(positives[1:m0])\n    proportion_fp &lt;- false_positives / max(1, sum(positives))\n    return(c(\n      sum(positives),\n      true_positives,\n      false_positives,\n      proportion_fp\n    ))\n  }\n  \n  list(\n    \"Sans correction\" = count_results(positives),\n    \"Bonferroni\" = count_results(bonferroni_positives),\n    \"Benjamini-Hochberg\" = count_results(bh_positives)\n  )\n}\n\n\n\n\n\n\n\n\n\nCode\nset.seed(140400)\n\n\n\n\n\n\n\nDonnées\nDans cette exercice nous allons générer des p-values en différentes quantités afin d’illustrer l’importance des techniques de correction pour des tests mutliples.\n\nOn va alors utiliser la fonction simulate_pvalues qui prend en entrée \\(n\\), \\(m\\), \\(m_0 \\leq m\\) et \\(\\mu_1\\), et qui :\n\nsimule \\(m_0\\) échantillons contenant chacun \\(n\\) réalisations d’une loi Normale centrée réduite, et \\(m−m_0\\) échantillons de taille \\(n\\) d’une loi normale d’espérance \\(\\mu_1\\) et de variance \\(1\\)\neffectue pour chacun des \\(m\\) échantillons un test de Student de nullité de la moyenne\nretourne les \\(m\\) p-values associées à ces tests\n\nDans la suite, on considère que \\(n = 100\\) et \\(m = 1000\\). Pour différentes situations, on va alors relever le nombre de positifs, de vrais-positifs, de faux-positifs et la proportion de faux-positifs lorsqu’on applique chacun des \\(m\\) tests précédents au niveau \\(\\alpha = 0.05\\) sans correction, lorsqu’on applique une procédure de Bonferroni associée à FWER \\(\\leq 0.05\\) et lorsqu’on applique une procédure de Benjamini-Hochberg associée à FDR \\(\\leq 0.05\\).\n\nIl y a donc pour chaque situation, 3 méthodes et 4 scores à calculer par méthode\n\nLes différents scénarios testés seront :\n\nPas de positifs : \\(m_0\\) = 1000\nPeu de positifs, facilement identifiables : \\(m_0\\) = 950, \\(\\mu_1\\) = 1\nPeu de positifs, difficilement identifiables : \\(m_0\\) = 950, \\(\\mu_1\\) = 0.3\nPas mal de positifs, facilement identifiables : \\(m_0\\) = 800, \\(\\mu_1\\) = 1\nPas mal de positifs, difficilement identifiables : \\(m_0\\) = 800, \\(\\mu_1\\) = 0.3\nBeaucoup de positifs, facilement identifiables : \\(m_0\\) = 200, \\(\\mu_1\\) = 1\nBeaucoup de positifs, difficilement identifiables : \\(m_0\\) = 200, \\(\\mu_1\\) = 0.3\n\n\n\nCode\nscenarios &lt;- list(\n  c(1000, 0),\n  c(950, 1),\n  c(950, 0.3),\n  c(800, 1),\n  c(800, 0.3),\n  c(200, 1),\n  c(200, 0.3)\n)\n\n\n\n\nCode\nresults &lt;- list()\nn &lt;- 100\nm &lt;- 1000\n\nfor (i in 1:length(scenarios)) {\n  m0 &lt;- scenarios[[i]][1]\n  mu1 &lt;- scenarios[[i]][2]\n  p_values &lt;- simulate_pvalues(n, m, m0, mu1)\n  results[[paste(\"m0 =\", m0, \"mu1 =\", mu1)]] &lt;- results_fun(p_values, m0)\n}\n\n# Création du tableau des résultats\nresults_matrix &lt;- do.call(rbind, lapply(names(results), function(scenario) {\n  cbind(Scenario = scenario, do.call(rbind, results[[scenario]]))\n}))\nresults_df &lt;- as.data.frame(results_matrix)\nnames(results_df) &lt;- c(\"Scenario\", \"Total Positifs\", \"Vrais Positifs\", \"Faux Positifs\", \"Proportion Faux Positifs\")\n\n\n\n\nAnalyse des scénarios\n\nS0S1S2S3S4S5S6\n\n\nIci on a pas de positifs : \\(m_0\\) = 1000.\n\n\nCode\nresults_df[1:3,] %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\nIci on a peu de positifs, facilement identifiables : \\(m_0\\) = 950, \\(\\mu_1\\) = 1\n\n\nCode\nresults_df[4:6,] %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\nIci on a peu de positifs, difficilement identifiables : \\(m_0\\) = 950, \\(\\mu_1\\) = 0.3\n\n\nCode\nresults_df[7:9,] %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\nIci on a pas mal de positifs, facilement identifiables : \\(m_0\\) = 800, \\(\\mu_1\\) = 1\n\n\nCode\nresults_df[10:12,] %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\nIci on a pas mal de positifs, difficilement identifiables : \\(m_0\\) = 800, \\(\\mu_1\\) = 0.3\n\n\nCode\nresults_df[13:15,] %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\nIci on a beaucoup de positifs, facilement identifiables : \\(m_0\\) = 200, \\(\\mu_1\\) = 1\n\n\nCode\nresults_df[16:18,] %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\nIci on a beaucoup de positifs, difficilement identifiables : \\(m_0\\) = 200, \\(\\mu_1\\) = 0.3\n\n\nCode\nresults_df[19:21,] %&gt;% DT::datatable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRésultats\n\n\nLes avantages et inconvénients des méthodes sont :\n\nSans correction : Forte sensibilité mais haut risque de faux positifs\nBonferroni : Réduit considérablement les faux positifs mais manque de puissance\nBenjamini-Hochberg : Bon compromis entre puissance et contrôle des faux positifs\n\n\n\n\nConclusion\n\n\nSession info\n\n\nCode\nsessioninfo::session_info(pkgs = \"attached\")\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 24.04.1 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  fr_FR.UTF-8\n ctype    fr_FR.UTF-8\n tz       Europe/Paris\n date     2025-02-25\n pandoc   3.2 @ /usr/lib/rstudio/resources/app/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package   * version date (UTC) lib source\n dplyr     * 1.1.4   2023-11-17 [1] CRAN (R 4.4.2)\n ggplot2   * 3.5.1   2024-04-23 [1] CRAN (R 4.4.2)\n gridExtra * 2.3     2017-09-09 [1] CRAN (R 4.4.2)\n\n [1] /home/clement/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────"
  }
]
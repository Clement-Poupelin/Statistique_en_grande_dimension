---
title: "Exercice 07"
author: "Clément Poupelin"
date: "`r Sys.Date()`"
format: 
  html:
    embed-resources: false
    toc: true
    code-fold: true
    code-summary: "Show the code"
    code-tools: true
    toc-location: right
    page-layout: article
    code-overflow: wrap
toc: true
number-sections: false
editor: visual
categories: ["regression Ridge", "regression Lasso", "Validation croisée"]
image: "/img/baseball.png"
description: "Ici, on continu sur des données de baseball en mettant en pratique les techniques de PCR et PLS avec de la validation croisée"
---


# Intervenant.e.s

### Rédaction

-   **Clément Poupelin**, [clementjc.poupelin\@gmail.com](mailto:clementjc.poupelin@gmail.com){.email}\

### Relecture

-   



# Setup

:::: panel-tabset
## Packages

```{r, setup, warning=FALSE, message=FALSE}
# Données
library(ISLR)         # Hitters data 
library(dplyr)        # manipulation des données

# Infrence
library(pls) ## PCR et PLSR
library(glmnet) ## regression pénalisée

# Plots
## ggplot
library(ggplot2)
library(gridExtra)
```

## Fonctions

::: panel-tabset

:::

## Seed

```{r}
set.seed(140400)
```

::::


# Données

Cette exercice est la suite direct de l'[Exercice 6](../posts/Exercice_06.qmd) où l'on a pu utiliser les méthodes de regressions avec reduction de dimension **PCR** et **PLSR**.

On va donc reprdre les mêmes données avec le même découpage en *train* et *test*.

```{r}
Hitters_Without_NA <- Hitters %>% na.omit()
percent_to_draw <- 0.75
index_train <- sample(nrow(Hitters_Without_NA), size = floor(percent_to_draw * nrow(Hitters_Without_NA)))

Hitters_train <- Hitters_Without_NA[index_train, ]

Hitters_test <- Hitters_Without_NA[-index_train, ]
```




NOtre objectif ici sera donc de compléter l'analyse de l'exercice 6 (lien)en utilisant cette fois ci les méthodes de regression pénalisée Ridge et Lasso (méthodes détaillées dans le 6 bonus lien).

# Analyse inférentielle

Contrairement à la plupart des autres package *`R`* qui permettent de faire de l’apprentissage, le package *`glmnet`* n’autorise pas l’utilisation de formules.  Il faut donc spécifier explicitement la matrice $X$ et le vecteur $y$.\

On peut obtenir la matrice $X$ et notamment le codage des variables qualitatives avec la fonction *`model.matrix`*.

```{r}
X_train <- model.matrix(Salary ~ ., data = Hitters_train)[, -1]
Y_train <- Hitters_train$Salary

X_test <- model.matrix(Salary ~ ., data = Hitters_test)[, -1]
Y_test <- Hitters_test$Salary
```

Et ce n'est qu'après que l'on peut mettre en place la modélisation.

::::::::: panel-tabset
## Ridge

```{r}
mod.R <- glmnet(X_train, Y_train, alpha = 0) 
```


Puis on peut visualiser les chemins de régularisation des estimateurs **Ridge**.

```{r}
plot(mod.R, xvar = "lambda", label = TRUE)
```

:::: success-header
::: success-icon
:::

Résultats
::::

::: success
Ici on voit l'évolution de nos coefficients $\beta$ en fonction des diffrentes valeurs de $\lambda$. Ainsi, sur la gauche on se retrouve dans la situation où il n'y a pas de pénalisation et donc nos coefficients sont les $\beta$ de l'estimation par moindres carrés. Et donc plus $\lambda$ va augmenter, plus on se retrouvera dans une situation où les coefficients vont tendrent vers 0.
:::

## Lasso

```{r}
mod.L <- glmnet(X_train, Y_train, alpha = 1) 
```
Puis on peut visualiser les chemins de régularisation des estimateurs **Lasso**.

```{r}
plot(mod.L, xvar = "lambda", label = TRUE)
```

:::: success-header
::: success-icon
:::

Résultats
::::

::: success
Ici on voit l'évolution de nos coefficients $\beta$ en fonction des diffrentes valeurs de $\lambda$. Ainsi, sur la gauche on se retrouve dans la situation où il n'y a pas de pénalisation et donc nos coefficients sont les $\beta$ de l'estimation par moindres carrés. Et donc plus $\lambda$ va augmenter, plus on se retrouvera dans une situation où les coefficients vont tendrent vers 0.
:::
:::::::::

# Selection des paramètres de régularisation

Maintenant que les modèles sont estimés avec plusieurs valeurs de $\lambda$ possibles, il se pose la question du choix du bon paramètre.\
Pour cela, on utilise la fonction *`cv.glmnet`* qui, comme son nom le laisse suggérer, permet d'effectuer une validation croisée pour notre modèle avec par défaut *`nfolds=10`* (le nombre de pli pour le découpage de sous ensembles). Puis on peut faire un *`plot`* de l’objet.

::: panel-tabset
## Ridge

```{r}
ridgeCV <- cv.glmnet(X_train, Y_train, alpha = 0)
plot(ridgeCV)
```

On visualise ici les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de $\lambda$ (échelle logarithmique). Deux traits verticaux sont représentés :

-   celui de gauche correspond à la valeur de $\lambda$ qui minimise l’erreur quadratique

-   celui de droite correspond à la plus grande valeur de $\lambda$ telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur

D’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux.\
A savoir que si l'on veut diminuer la complexité du modèle on choisira la valeur de droite.

On peut obtenir ces deux valeurs assez facilement.

```{r}
cat(" Valeur minimale : ", ridgeCV$lambda.min, "\n", "Valeur maximale : ", ridgeCV$lambda.1se)
```

## Lasso

```{r}
lassoCV <- cv.glmnet(X_train, Y_train, alpha = 1)
plot(lassoCV)
```

On visualise ici les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de $\lambda$ (échelle logarithmique). Deux traits verticaux sont représentés :

-   celui de gauche correspond à la valeur de $\lambda$ qui minimise l’erreur quadratique

-   celui de droite correspond à la plus grande valeur de $\lambda$ telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur

D’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux.\
A savoir que si l'on veut diminuer la complexité du modèle on choisira la valeur de droite.

On peut obtenir ces deux valeurs assez facilement.

```{r}
cat(" Valeur minimale : ", lassoCV$lambda.min, "\n", "Valeur maximale : ", lassoCV$lambda.1se)
```
:::


# Prédiction


# Prédiction

On souhaite maintenant prédiction pour le jeu de données *test*.

Une première approche pourrait consister à réajuster le modèle sur toutes les données pour la valeur de $lambda$ sélectionnée.\
Cette étape est en réalité déjà effectuée par la fonction *`cv.glmnet`*. Il suffit par conséquent d’appliquer la fonction predict à l’objet obtenu avec *`cv.glmnet`* en spécifiant la valeur de $lambda$ souhaitée.

::: panel-tabset
## Ridge

```{r}
pred.ridge_min <- predict(ridgeCV, newx = X_test, s = "lambda.min")
err_ridge <- sqrt(mean((pred.ridge_min - Y_test)^2, na.rm=T))
```


## Lasso

```{r}
pred.lasso_min <- predict(lassoCV, newx = X_test, s = "lambda.min") 
err_lasso <- sqrt(mean((pred.lasso_min - Y_test)^2, na.rm=T))
```

:::

Ainsi on peut obtenir l'erreur de prédiction via le RMSEP pour les 2 modèles et les comparer avec les valeurs obtenues à l'exercice 6 (lien).
```{r, echo=FALSE}
library(pls)

## PCR
res.pcr = NULL
for(i in 1:100){
  # modele PCR
  pcr.fit = pcr(Salary~., data = Hitters_train, scale = TRUE, validation = "CV", segments = 10)
  # RMSEP
  RMSEP.cv = RMSEP(pcr.fit,'CV')$val[,,]
  
  # On stocke les resultats
  res.pcr = cbind(res.pcr, RMSEP.cv)
}
pcr.mean.cv = apply(res.pcr, MARGIN = 1, FUN = mean)

pcr.fit_final = pcr(Salary~., data = Hitters_train, ncomp = which.min(pcr.mean.cv)-1, scale = TRUE, validation = "CV", segments = 10 )


# PLS
res.pls = NULL
for(i in 1:100){
  pls.fit = plsr(Salary~., data = Hitters_train, scale = TRUE, validation = "CV", segments = 10)
  # RMSEP
  RMSEP.cv = RMSEP(pls.fit,'CV')$val[,,]
  # On stocke les resultats
  res.pls = cbind(res.pls, RMSEP.cv)
}

pls.mean.cv = apply(res.pls, MARGIN = 1, FUN = mean) 
pls.fit_final = plsr(Salary~., data = Hitters_train, ncomp = which.min(pls.mean.cv)-1, scale = TRUE, validation = "CV", segments = 10 )



# faire une selec forwise-stepwise avec critere BIC sur train puis rmsep sur test
mod0=lm(Salary~0, data=Hitters_train)
mod_full=lm(Salary~., data=Hitters_train)
mod.step = step(mod0, scope = formula(mod_full), trace = FALSE, direction = "both", k = log(nrow(Hitters_train)))

hat_df_test_mod.step = predict(mod.step, Hitters_test)
rmsep_mod.step = sqrt(mean((hat_df_test_mod.step - Hitters_test$Salary)**2))


```

```{r}
rmsep_pred_df <- data.frame("prediction Ridge" = err_ridge, "prediction Lasso" = err_lasso) 
rownames(rmsep_pred_df) <- "RMSEP"
rmsep_pred_df 
```


```{r}
err_vect = c(err_lasso, err_ridge, rmsep_mod.step, min(RMSEP(pls.fit_final,'CV')$val[,,]), min(RMSEP(pcr.fit_final,'CV')$val[,,]))
plot(err_vect, type = "h", main = "erreur pour les différentes méthodes")
points(err_vect, col=1:6)
legend("topright", 
       legend=c("Lasso_lars", "Lasso_glmnet", "Ridge", "mod.step", "pls", "pcr"),
       pch = 1,
       col = 1:6)
```




# Conclusion











# Session info

```{r}
sessioninfo::session_info(pkgs = "attached")
```








```{r, error=TRUE}


library(ISLR)

# Découpage des données ####
df = na.omit(Hitters)
dim(df)
set.seed(234)

pourcentage_a_tirer = 0.75
indices_train = sample(nrow(df), size = floor(pourcentage_a_tirer * nrow(df)))

df_train = df[indices_train, ]
dim(df_train)

df_test = df[-indices_train, ] 
dim(df_test)

####
# Création des modèles ####
X.train = model.matrix(Salary~.,data=df_train)[,-1]
Y.train = df_train$Salary

mod.R <- glmnet(X.train, Y.train, alpha=0) ## Ridge 
mod.R$beta[,1]

mod.L <- glmnet(X.train, Y.train, alpha=1) ## Lasso
mod.L$beta[,1]

par(mfrow=c(2,2))
plot(mod.R, label=TRUE, main = "Ridge")  
plot(mod.R, xvar="lambda",label=TRUE, main = "Ridge")

plot(mod.L, label=TRUE, main = "Lasso")  
plot(mod.L, xvar="lambda",label=TRUE, main = "Lasso")
par(mfrow=c(1,1))


####
# Sélectionn des paramètres de régularisation ####

ridgeCV <- cv.glmnet(X.train, Y.train, alpha=0)
lassoCV <- cv.glmnet(X.train, Y.train, alpha=1)

par(mfrow=c(1,2))
plot(ridgeCV, main = "Ridge")

plot(lassoCV, main = "Lasso")
par(mfrow=c(1,1))

ridgeCV$lambda.min
ridgeCV$lambda.1se

lassoCV$lambda.min
lassoCV$lambda.1se


####
# Prédiction de la variable cible pour de nouveaux individus ####
X.test = model.matrix(Salary~.,data=df_test)[,-1]
Y.test = df_test$Salary

pred.ridge_min = predict(ridgeCV, newx = X.test, s="lambda.min")
# predict(ridgeCV, newx = X.test, s="lambda.1se")
err_ridge = sqrt(mean((pred.ridge_min - Y.test)^2, na.rm=T))
err_ridge
# 345.5415

pred.lasso_min = predict(lassoCV, newx = X.test, s="lambda.min")
# predict(lassoCV, newx = X.test, s="lambda.1se")
err_lasso = sqrt(mean((pred.lasso_min - Y.test)^2, na.rm=T))
err_lasso
# 349.1404

####
# Lasso avec lars ####
library(lars)
lars.cv = cv.lars(X.train, Y.train)
choix = lars.cv$index[which.min(lars.cv$cv)]
temp = lars(X.train, Y.train)
plot(temp) # chemin lasso
coef(temp, s=choix, mode='fraction') 
#le resultat est diff qu'avec glmnet parce que le choix de lambda est diff
#attention: les lambda ne sont pas normalise pareil dans glmnet et dans lars
#cela n'a pas de sens d'utiliser le lambda choisi par glmnet dans lars et inversement
#cf le poly section 4.3.3 pour une explication


lars.pred = predict(temp, X.train, s=choix, mode='fraction')$fit
err_lars = sqrt(mean((lars.pred - Y.test)^2, na.rm=T))
err_lars
# 499.6627



####
# Reprenons les modèles sélectionnés à l'Exo 6 ####
library(pls)

## PCR
res.pcr = NULL
for(i in 1:100){
  # modele PCR
  pcr.fit = pcr(Salary~., data = df, scale = TRUE, subset = indices_train, validation = "CV", segments = 10)
  # RMSEP
  RMSEP.cv = RMSEP(pcr.fit,'CV')$val[,,]
  
  # On stocke les resultats
  res.pcr = cbind(res.pcr, RMSEP.cv)
}
pcr.mean.cv = apply(res.pcr, MARGIN = 1, FUN = mean)

pcr.fit_final = pcr(Salary~., data = df, ncomp = which.min(pcr.mean.cv)-1, scale = TRUE, subset = indices_train, validation = "CV", segments = 10 )

min(RMSEP(pcr.fit_final,'CV')$val[,,])
#  345.3995
which.min(RMSEP(pcr.fit_final,'CV')$val[,,])
# 16 comps

# PLS
res.pls = NULL
for(i in 1:100){
  pls.fit = plsr(Salary~., data = df, scale = TRUE, subset = indices_train, validation = "CV", segments = 10)
  # RMSEP
  RMSEP.cv = RMSEP(pls.fit,'CV')$val[,,]
  # On stocke les resultats
  res.pls = cbind(res.pls, RMSEP.cv)
}

pls.mean.cv = apply(res.pls, MARGIN = 1, FUN = mean) 
pls.fit_final = plsr(Salary~., data = df, ncomp = which.min(pls.mean.cv)-1, scale = TRUE, subset = indices_train, validation = "CV", segments = 10 )

min(RMSEP(pls.fit_final,'CV')$val[,,])
# 346.1336
which.min(RMSEP(pls.fit_final,'CV')$val[,,])
# 8 comps


# faire une selec forwise-stepwise avec critere BIC sur train puis rmsep sur test
mod0=lm(Salary~0, data=df_train)
mod_full=lm(Salary~., data=df_train)
mod.step = step(mod0, scope = formula(mod_full), trace = FALSE, direction = "both", k = log(nrow(Hitters_train)))

hat_df_test_mod.step = predict(mod.step, df_test)
rmsep_mod.step = sqrt(mean((hat_df_test_mod.step - df_test$Salary)**2))
rmsep_mod.step
# 370.2405


####




err_vect = c(err_lars, err_lasso, err_ridge, rmsep_mod.step, min(RMSEP(pls.fit_final,'CV')$val[,,]), min(RMSEP(pcr.fit_final,'CV')$val[,,]))
plot(err_vect, type = "h", main = "erreur pour les différentes méthodes")
points(err_vect, col=1:6)
legend("topright", 
       legend=c("Lasso_lars", "Lasso_glmnet", "Ridge", "mod.step", "pls", "pcr"),
       pch = 1,
       col = 1:6)





# Cette operation peut egalement etre repete pour avoir davantage que 10 decoupes.
K=10
n = nrow(df)
seg = cvsegments(n,K) #library(pls)
err_reg=NULL; err_pcr=NULL; err_pls=NULL; err_ridge=NULL; err_lasso=NULL; err_lars=NULL
for(i in 1:K){
  cat(i)
  
  test=seg[[i]]
  reg=lm(Salary~., data=df, subset=-test)
  reg0=lm(Salary~0,data=df, subset=-test)
  resfwd=step(reg0, scope=formula(reg), direction="forward", k=log(n), trace=0)
  reg.pred=predict(resfwd, df[test,])
  err_reg[i]=sqrt(mean((reg.pred - df[test,19])^2, na.rm=T))
  
  pcr.fit=pcr(Salary~., data=df,scale=TRUE,subset=-test,validation="CV",segments=10)
  nbcomp=which.min(RMSEP(pcr.fit,'CV')$val[,,1:10])
  pcr.pred=predict(pcr.fit,df[test,],ncomp=nbcomp)
  err_pcr[i]=sqrt(mean((pcr.pred-df[test,19])^2,na.rm=T))
  
  pls.fit=plsr(Salary~., data=df,subset=-test,scale=TRUE, validation="CV")
  nbcomp=which.min(RMSEP(pls.fit,'CV')$val[,,1:10])
  pls.pred=predict(pls.fit,df[test,],ncomp=nbcomp)
  err_pls[i]=sqrt(mean((pls.pred-df[test,19])^2,na.rm=T))
  
  train.mat=model.matrix(Salary~.,data=df[-test,])
  train.mat=train.mat[,-1]
  y=df[-test,19]
  test.mat=model.matrix(Salary~.,data=df[test,])
  test.mat=test.mat[,-1]
  ytest=df[test,19]
  
  
  ridge.cv=cv.glmnet(train.mat,y,alpha=0,lambda=seq(1,5000))
  lambdachoisi=ridge.cv$lambda.min
  ridge.pred=predict(ridge.cv,test.mat,s=lambdachoisi)
  err_ridge[i]=sqrt(mean((ridge.pred-df[test,19])^2,na.rm=T))
  
  
  lasso.cv=cv.glmnet(train.mat,y,alpha=1)
  lasso.pred=predict(lasso.cv,test.mat,s=lasso.cv$lambda.min)
  err_lasso[i]=sqrt(mean((lasso.pred-df[test,19])^2,na.rm=T))
  
  lars.cv = cv.lars(train.mat, y)
  choix = lars.cv$index[which.min(lars.cv$cv)]
  temp = lars(train.mat, y)
  lars.pred = predict(temp, train.mat, s=choix, mode='fraction')$fit
  err_lars[i] = sqrt(mean((lars.pred - df[test,19])^2, na.rm=T))
  
}


boxplot(err_reg,err_pcr,err_pls,err_ridge,err_lasso, err_lars,
        names=c('reg','pcr','pls','ridge','lasso', 'lasso_lars'), 
        col=2:8, 
        main="Erreurs des différentes méthodes")
#abline(h = median(err_pcr), col='red')

mean(err_reg)
mean(err_pcr)
mean(err_pls)
mean(err_ridge)
mean(err_lasso) # moyenne pas toujours la plus basse (change en relancant)
mean(err_lars)

# On ne constate pas de grandes differences.
# Finalement la reg avec selection forward (ici 6 var retenues) est pas mal.
# Ce n'est pas illogique :
#    - le jeu de donnees n'est pas de tres grande dimension
#    - les methodes de reduction de dimension et contraintes
#      n'apportent pas grand chose



```





